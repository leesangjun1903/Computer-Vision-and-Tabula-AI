# LightGBM: A Highly Efficient Gradient Boosting Decision Tree

## 1. 핵심 주장 및 주요 기여  
LightGBM은 대규모·고차원 데이터에서 기존 GBDT 구현(XGBoost 등)이 겪는 느린 학습 속도와 높은 메모리 소모 문제를 해결하기 위해 제안된 알고리즘이다.  
- **Gradient-based One-Side Sampling (GOSS)**: 기울기가 큰 인스턴스를 우선 보존하고, 기울기가 작은 인스턴스는 확률적으로 샘플링하며 보정 계수로 정보 이득 계산 시 왜곡을 최소화  
- **Exclusive Feature Bundling (EFB)**: 희소 공간에서 거의 동시에 0이 아닌 값을 갖지 않는(mutually exclusive) 다수의 특징을 하나의 번들로 묶어 피처 차원을 크게 축소  

이 둘을 결합하여 LightGBM은 학습 속도를 최대 20배 이상 가속하면서(대규모 실험에서 평균 6–13배), 기존 GBDT 수준의 예측 정확도를 유지한다.

***

## 2. 문제 정의 및 제안 기법 상세

### 2.1 해결하고자 하는 문제  
기존 GBDT 구현은
1. 피처 개수 * 인스턴스 수에 비례하는 히스토그램 구축 비용  
2. 모든 인스턴스에 대한 스캔  
3. 희소 피처 처리 비효율  
등으로 대규모·고차원 데이터에 부적합하며 학습 속도 및 메모리 사용량이 문제가 된다.

### 2.2 Gradient-based One-Side Sampling (GOSS)  
- **아이디어**: 기울기(gradient)의 절대값이 큰 인스턴스가 정보 이득 계산에 더 큰 기여를 하므로, 이를 모두 유지(a × 100%)하고 나머지를 b × 100%만큼 무작위 샘플링  
- **보정 수식**: 기울기 작은 인스턴스에 대해 가중치 $$\frac{1 - a}{b} $$를 곱하여 전체 분포 왜곡 보정  
- **정보 이득 추정**  

$$
    \tilde V_j(d)
    = \frac{1}{n}\Biggl[
    \frac{\bigl(\sum_{x_i \in A_l} g_i + \tfrac{1 - a}{b}\sum_{x_i \in B_l} g_i\bigr)^2}{n_{l}(d)}
    + \frac{\bigl(\sum_{x_i \in A_r} g_i + \tfrac{1 - a}{b}\sum_{x_i \in B_r} g_i\bigr)^2}{n_{r}(d)}
    \Biggr]
  $$  
  
  - $$A$$: 상위 $$a$$ 비율 인스턴스, $$B$$: 나머지에서 샘플링된 인스턴스  
  - $$n_l, n_r$$: 분할 후 좌·우 노드의 인스턴스 수  
- **이론적 결과**:  
  - 분할이 극단적으로 치우치지 않을 경우 오차가 $$O(1/\sqrt{n})$$로 감소  
  - 균일 샘플링(SGB) 대비 기울기 기반 선택으로 정보 이득 추정 정확도 우수  

### 2.3 Exclusive Feature Bundling (EFB)  
- **아이디어**: 희소 피처 공간에서 동시에 값이 0이 아닌 경우가 드문(mutually exclusive) 피처들을 하나로 묶어 번들 생성  
- **NP-Hard성**: 최적 번들링은 그래프 컬러링 문제로 환원되어 NP-Hard  
- **근사 알고리즘 (Alg. 3)**:  
  1. 피처를 정점, 상호 배타성이 아닐 경우 간선이 있는 그래프 생성  
  2. 차수가 높은 순으로 정렬 후 그리디하게 번들에 할당 (허용 충돌 수 γ 조절 가능)  
- **번들 합병 (Alg. 4)**:  
  - 각 피처의 이산화된 bin 범위(offset)를 다르게 설정하여 서로 값이 겹치지 않도록 한 뒤 하나의 히스토그램으로 처리  
- **효과**: 번들 개수가 원래 피처 수에 비해 극소화되어 히스토그램 구축 비용이 $O(\text{data} \times \text{bundle})$ 로 감소

***

## 3. 모델 구조 및 학습 절차  
LightGBM은 기본적으로 histogram-based GBDT를 사용하며, 각 반복(iteration)에서 다음 단계를 거친다:  
1. 현재 예측값으로부터 기울기 계산  
2. GOSS로 샘플링 및 가중치 보정  
3. 모든 번들(또는 원본 피처)마다 히스토그램 구축  
4. 최적 분할점 탐색 및 트리 확장 (leaf-wise 성장 전략)  
5. 트리 앙상블에 추가  

***

## 4. 성능 향상 및 한계

### 4.1 학습 속도  
- 대규모 데이터(10M~100M 인스턴스, 수십만 피처)에서 평균 6–13배 가속, 최대 20배 이상 속도 개선  
- 메모리 사용량 및 I/O 오버헤드 현저히 감소  

### 4.2 예측 정확도  
- GOSS와 EFB 적용 시에도 기존 XGBoost 및 SGB 대비 **AUC**(이진 분류)와 **NDCG@10**(랭킹)에서 동일 수준 유지  
- 기울기 기반 샘플링이 균일 샘플링보다 분할 품질 저하 최소화  

### 4.3 한계  
- GOSS의 파라미터 $$a,b$$ 선택은 데이터 특성에 따라 민감하며, 최적 설정을 위한 별도 탐색 필요  
- EFB 번들링 시 허용 충돌률 γ를 높이면 속도는 개선되나, 다소의 정보 손실이 발생할 수 있음  
- 고도로 상관된(sparse하지 않은) 피처에는 EFB 효과 제한적  

***

## 5. 일반화 성능 향상 가능성  
- GOSS는 학습 데이터의 다양성을 증가시켜(샘플링으로 인한 모델 편향 저감) 오히려 일반화 성능을 개선할 잠재력 보유  
- 이론적으로 GOSS의 분할 품질 오차가 $$O(1/\sqrt{n})$$로 감소하며, 추가적인 샘플링 노이즈가 ensemble의 다양성 강화로 작용  
- 실제 실험에서 SGB 대비 동일 샘플링 비율 하에 GOSS가 더 높은 검증 성능을 달성  

***

## 6. 향후 연구에 미치는 영향 및 고려 사항  
- **자동 하이퍼파라미터 탐색**: GOSS의 $$a, b$$ 및 EFB의 γ 설정을 자동화·적응화하는 메커니즘이 필요  
- **비희소 피처 확장**: 희소가 아닌 피처에도 적용 가능한 번들링 혹은 차원 축소 기법 통합  
- **이론적 일반화 분석 심화**: GOSS가 ensemble 일반화에 미치는 영향을 보다 엄밀히 규명  
- **GPU·분산 학습 연계**: LightGBM의 구조적 장점을 GPU나 대규모 클러스터 상에 최적화하여 더욱 고속화  

LightGBM의 GOSS와 EFB는 대규모·고차원 GBDT 학습의 패러다임을 전환했으며, 향후 자동화된 샘플링·번들링 기법 및 분산 처리 시스템과 결합되어 머신러닝 실무 적용 범위를 크게 확장할 것으로 기대된다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/dc9f6ca6-3e96-4f70-a8ae-a1282065342b/NIPS-2017-lightgbm-a-highly-efficient-gradient-boosting-decision-tree-Paper.pdf
