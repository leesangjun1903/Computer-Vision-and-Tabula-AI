# Autoencoder
- Encoder-decoder architectures

오토인코더는 두 부분으로 구성된 심층 신경망 구조입니다. 첫째, 인코더 네트워크는 각 입력 데이터 포인트를 다른 잠재 공간의 포인트로 매핑합니다. 둘째, 디코더 네트워크는 잠재 공간의 포인트를 다시 데이터 공간으로 매핑합니다. 이 두 구성 요소는 주어진 훈련 데이터셋의 포인트를 약식으로 보존하도록 비지도 학습 방식으로 공동 훈련됩니다.

오토인코더는 1980년대부터 시작된 비지도 학습을 위한 매우 인기 있는 심층 구조입니다. 이는 주성분 분석과 같은 다른 비지도 학습 방법과 유사하게, 훈련 데이터셋의 포인트에서 정보는 보존하면서 데이터를 단순화할 수 있는 잠재 표현을 찾는 것이 목표입니다.

## 주요 구성 요소
Autoencoder는 1980년대부터 사용된 매우 인기 있는 비지도 학습용 딥러닝 구조입니다. 주 목적은 주어진 데이터의 정보를 최대한 보존하면서 데이터의 잠재(latent) 표현을 찾는 것입니다.

Autoencoder는 크게 두 부분으로 구성됩니다.

- 인코더(encoder): 데이터를 잠재 공간으로 압축하는 신경망
- 디코더(decoder): 압축된 잠재 표현을 원래 데이터로 복원하는 신경망
이 두 네트워크를 병렬로 학습시키면서 입력 데이터와 복원값 간의 오차를 최소화합니다. 이를 통해 잠재 표현이 원본 데이터의 정보를 최대한 담도록 보장합니다.

또한, PCA(주성분분석) 같은 전통적인 차원 축소 기법과 유사하면서도, 비선형 변환을 통해 더 복잡한 데이터 구조도 모델링할 수 있습니다. 학습된 잠재 표현은 이후 분류, 이상 탐지, 이미지 재구성 등 다양한 기계학습 작업의 특징으로 활용 가능하며, 변분 오토인코더(VAE)처럼 생성 모델에도 응용됩니다.

## 이론
자동 인코더(Autoencoder)는 특정 데이터 공간 X에서 무감독 방식으로 학습되는 심층 아키텍처입니다. 이는 데이터가 어떤 기저 분포 ( p_X )에서 샘플링된 것으로 볼 수 있으며, 데이터 매니폴드 M을 학습하는 방법으로 이해할 수 있습니다.

자동 인코더는 두 가지 주요 구성 요소로 이루어져 있습니다:

인코더 ( $$e_\phi$$ ): 입력 데이터 ( x )를 잠재 표현 ( $$z = e_\phi(x)$$ )으로 변환합니다. 이는 학습 가능한 매개변수 ( $$\phi$$ )를 포함합니다.

디코더 ( $$d_\theta$$ ): 잠재 공간 Z에서 포인트 ( z )를 데이터 공간 X로 매핑합니다. 이 또한 학습 가능한 매개변수 ( $$\theta$$ )를 포함합니다.

자동 인코더의 작동 방식은 다음과 같습니다:

손실 함수: 데이터셋에서의 아이덴티티 매핑을 근사하여 학습합니다. 손실 함수는 Dissimilarity ( $$\Delta$$ )의 형태로 정의됩니다. 예를 들어, 제곱 유클리드 거리 ( $$\Delta(x,y) = |x - y|_2^2$$ )를 사용할 수 있습니다.

훈련 과정: 손실 ( $$L(\phi, \theta)$$ )를 최소화하는 방향으로 진행되며, 잠재 표현이 데이터를 단순화하는 형태를 학습하게 됩니다.

자동 인코더의 성공을 평가하는 지표 중 하나는 잠재 공간에서의 보간 가능성입니다. 예를 들어, 두 데이터 샘플 ( x_1 )과 ( x_2 )의 잠재 표현 ( z_1 )과 ( z_2 ) 사이의 선분에서의 잠재 포인트가 데이터 매니폴드 M에 매핑되는지를 검증할 수 있습니다.

이와 같이, 자동 인코더는 복잡한 데이터 구조를 단순화하여 효과적으로 학습할 수 있는 유용한 도구가 됩니다.

### Types of Autoencoders
오토인코더는 충분한 네트워크 용량(e 및 d)과 큰 공간(Z)이 주어질 경우, 데이터 포인트를 잠재 공간에 복사하고 이를 원래 공간으로 되돌리는 방식으로 학습할 가능성이 높습니다. 이러한 경우, 최소 손실(제로 손실)을 달성함에 따라 잠재 표현은 원래 데이터 표현보다 간단하지 않게 되고, 데이터 매니폴드는 제대로 매개변수화되지 않게 됩니다. 이를 해결하기 위한 방법들을 논의합니다.

#### 오토인코더는 비선형 차원 축소를 위한 아키텍처로 활용됩니다. 
여기서 Z의 차원은 X의 차원보다 훨씬 작게 설정되어, 오토인코더 학습이 차원 축소와 같아집니다. 학습 과정에서는 데이터에서 가장 중요한 변동 요인을 식별하고, 이를 잠재 표현(latent representation) 안에 보존하려고 합니다. 이렇게 함으로써 데이터의 핵심 특성을 효과적으로 파악할 수 있습니다.

#### 정규화 오토인코더
소규모 차원 Z: 잠재 변수 Z의 차원을 작게 유지하여 데이터의 중요한 특성을 포착합니다.

정규화 기법:
- 잠재 벡터 ( $$z_i$$ )의 특정 노름을 페널티로 부과하여 모델의 복잡성을 줄입니다.
- 네트워크 매개변수 ( $$\phi$$ )와 ( $$\theta$$ )에 대해서도 노름을 페널티로 부과할 수 있습니다.

이러한 정규화는 오토인코더가 단순히 항등 함수(입력 그대로 출력)를 학습하는 것을 방지하고, 데이터의 단순화된 잠재 표현을 학습하도록 유도합니다. 이러한 접근법은 데이터의 구조를 보다 잘 이해하고, 과적합을 방지하는 데 도움을 줍니다.

#### Denoising Autoencoder
이 모델은 입력 데이터에 노이즈를 추가하거나 구조적인 손상을 가하여 훈련 과정을 정규화하는 데 사용됩니다. 그리고 훈련 시점에서 손상된 데이터의 복원과 원본 데이터의 재구성을 동시에 수행해야 합니다.

주요 포인트는 다음과 같습니다.

훈련 데이터 손상: 입력 데이터 ( $$x_i$$ )에 노이즈를 추가하여 손상된 데이터 ( $$\tilde{x_i}$$ )를 생성합니다.

손실 함수: 손실 함수는 손상된 데이터를 통해 복원된 출력과 원본 데이터 간의 차이를 계산합니다.

$$
[
L(\phi, \theta) = \frac{1}{N} \sum_{i=1}^{N} \Delta(x_i, a_{\phi,\theta}(\tilde{x_i}))
]
$$

복원과 재구성의 필요성: Denoising Autoencoder 는 단순히 입력을 출력으로 복사하는 방식으로는 낮은 훈련 손실을 얻을 수 없습니다. 오히려 손상된 예제들을 원본 데이터의 분포가 있는 매니폴드로 투영해야 하며, 이를 통해 원본 데이터를 재구성할 수 있는 잠재 표현을 발견해야 합니다.

Denoising Autoencoder는 이러한 방식으로 노이즈 제거 및 데이터 복원을 동시에 수행하며, 이는 실제 데이터 손상을 고려하는 데 매우 유용할 수 있습니다.

#### Variational autoencoders
변분 오토인코더(Variational Autoencoders, VAE)는 확률적 설정에서 여러 아이디어를 결합한 모델입니다. VAE에서 인코더와 디코더는 각각 확률 함수로 간주됩니다.

인코더는 각 데이터 포인트를 잠재 공간의 분포(주로 대각 공분산 행렬을 가진 가우시안)로 매핑합니다.  
디코더는 잠재 공간의 포인트를 데이터 공간의 (주로 가우시안) 분포로 변환합니다.  
학습 과정은 훈련 데이터 샘플을 잠재 공간의 분포로 매핑한 후, 샘플링하고 다시 데이터 공간으로 변환하는 것으로 이루어집니다. 미분 로그 우도는 결과 분포에 대해 최소화되고, 학습은 각 데이터 샘플의 인코딩과 단위 제로 평균 가우시안 간의 Kulback-Leibler 발산을 패널티하여 정규화됩니다. 정규화 계수가 1일 때, 전체 학습 목표는 데이터 포인트의 로그 우도의 증거 하한 증대 최대화로 해석됩니다.

#### 주성분 분석(PCA)
주성분 분석은 특수한 형태의 오토인코더로 간주될 수 있으며, 여기서 인코더와 디코더는 추가적인 제약 조건 하에 완전 연결 단일 레이어 아키텍처를 가집니다. PCA는 보다 효율적인 훈련 알고리즘인 특이값 분해 및 고유값 분해를 활용합니다.

아래에서 논의되는 사용 패턴은 오토인코더와 PCA 모두에 공통적입니다. 그러나 일반적으로 다수의 합성곱 레이어를 가진 오토인코더는 이미지 데이터셋에서 더 나은 성능을 달성합니다.

#### Data Compression
자동 인코더(Autoencoder)는 입력 데이터의 차원을 축소하여 의미 있는 표현(latent representation)을 학습하는 신경망 구조입니다. 이러한 방법은 다음과 같은 특징이 있습니다:

차원 축소: 잠재 공간(latent space)의 차원이 원본 공간보다 낮을 경우, 정보의 손실이 발생하는 경우도 있지만, 더 효율적인 데이터 표현을 제공합니다.

손실 압축: 자동 인코더는 학습한 후 데이터 압축에 사용될 수 있으며, 이는 주로 비손실 방식(lossy compression)입니다. 즉, 원본 데이터와 복원된 데이터 간의 차이가 있을 수 있습니다.

압축 알고리즘 호환성: 잠재 표현이 압축 알고리즘과 더 잘 맞을 경우, 데이터의 효율적인 저장 및 전송이 가능해집니다.

이러한 특성 덕분에 자동 인코더는 이미지, 텍스트, 오디오와 같은 다양한 데이터 유형의 압축에 널리 사용되고 있습니다. 

#### GAN
생성적 적대 신경망(Generative Adversarial Networks, GANs)은 데이터 매니폴드 또는 분포를 근사화하기 위해 학습된 또 다른 종류의 잠재 모델입니다. 원래 형태의 GAN은 잠재 공간의 포인트를 데이터 공간으로 매핑할 수 있지만, 그 반대는 불가능합니다. 자동 인코더와 GAN을 결합한 다양한 하이브리드 모델도 존재합니다. 이는 GAN의 기능을 확장하여 잠재 공간에서 데이터 공간으로의 맵핑을 가능하게 합니다.

#### Generative Latent Optimization (GLO)
GLO는 데이터 매니폴드를 매개변수화하기 위해 학습된 딥 러닝 잠재 모델입니다. GLO는 오토인코더 모델의 단순화된 형태로 볼 수 있으며, 훈련 후에는 디코더 네트워크만을 학습하고 재사용합니다.

GLO의 주요 특징은 다음과 같습니다.

학습 효율성: 오토인코더와 달리 인코더를 학습하지 않음으로써 효율적인 훈련이 가능합니다.

데이터 재구성: 훈련된 디코더를 통해 새로운 데이터를 생성하거나 기존 데이터를 재구성할 수 있습니다.

Generative Latent Optimization(GLO)는 주로 이미지 생성 및 복원 같은 생성 모델 분야에서 사용되는 기법입니다. 이는 잠재 공간(latent space)의 특성을 최적화하여 높은 품질의 데이터를 생성하기 위해 설계되었습니다. GLO는 주로 생성적 적대 신경망(Generative Adversarial Networks, GANs)을 기반으로 하며, 다음과 같은 특징이 있습니다:

- 잠재 변수 최적화: 모델은 직접적으로 데이터 생성을 위한 최적의 잠재 변수를 학습합니다.

- 비지도 학습: GLO는 비지도 학습 방식으로 작동하여 레이블 없는 데이터로부터 패턴을 학습합니다.

다양성: 다양한 형태의 데이터를 생성할 수 있어, 다양한 분야에 응용 가능합니다.

## Applications
#### Feature extraction
오토인코더가 훈련된 후, 인코더 부분 ( $$e_\phi$$ )는 다양한 기계 학습 작업에서 특성 추출기로 사용될 수 있습니다. 반감독 학습 시나리오에서는 많은 양의 레이블이 없는 데이터 ( $${x'i}$$ )를 사용하여 오토인코더를 훈련하고, 소량의 레이블이 있는 데이터 ( $${x_i, y_i}$$ )를 이용하여 잠재 공간에서 예측기 ( $$c\psi$$ )를 훈련합니다. 여기서 ( $$y_i \approx c_\psi(e_\phi(x_i))$$ )가 성립해야 하며, 인코더의 훈련이 성공적이라면, 잠재 공간의 분포는 데이터 공간보다 간단한 형태를 가집니다. 이로 인해 원본 데이터 공간 ( X )에서 예측기를 훈련하는 것보다 잠재 공간 ( Z )에서 훈련할 때 더 나은 일반화 성능을 기대할 수 있습니다. 이 경우 디코더 ( $$d_\theta$$ )는 효과적으로 무시됩니다.

#### Pretraining generator networks
사용자가 제시한 상황에서 사전학습된 오토인코더의 디코더를 조건부 샘플링 또는 합성에 활용하는 방법에 대해 설명드리겠습니다.

주어진 상황은 두 가지 데이터가 존재합니다.

(1) 서로 정렬된 소량의 데이터 $${(x_i, y_i)} ⊂ X ⊗ Y$$ (예: 이미지 X와 그에 대응하는 텍스트 설명 Y)
(2) 라벨 없는 대량의 이미지 데이터 $${x′_j} ⊂ X$$ (예: 이미지만 존재하고 설명이 없는 경우)
여기서 목표는 텍스트 설명 Y에 조건을 둔 이미지 합성 기능을 학습하는 것입니다.

우선, 라벨이 없는 이미지 데이터 $${x′_j}$$ 를 이용하여 오토인코더(Autoencoder)를 학습합니다.

오토인코더는 입력 이미지를 낮은 차원의 잠재 공간 Z로 인코딩하고 다시 디코딩하여 원본 이미지에 가깝게 재현합니다.
이 과정에서 학습된 디코더 $$d_θ$$ 는 $$(Z \to X)$$ (잠재 공간에서 이미지 공간으로) 변환을 담당합니다.
그다음에, 정렬된 데이터 $$(x_i, y_i)$$를 활용하여, 텍스트 설명 공간 Y에서 잠재 공간 Z로 매핑하는 함수 $$(f_{\tau}: Y \to Z)$$ 를 학습합니다.

구체적으로 $$(x_i \approx d_{\theta}(f_{\tau}(y_i)))$$가 되도록 $$(f_{\tau})$$의 파라미터 $$(\tau)$$ 를 최적화합니다.
이때 디코더 $$(d_{\theta})$$ 는 사전학습된 상태로 고정하거나 함께 미세조정할 수 있습니다.
이로써, 텍스트 (y)를 잠재 공간으로 변환하는 $$(f_{\tau})$$ 와, 잠재 공간을 이미지로 변환하는 디코더 $$(d_{\theta})$$ 의 합성 $$(d_{\theta} \circ f_{\tau})$$ 는 텍스트 조건부 이미지 생성기 역할을 하게 됩니다.

이 방법의 강점은 다음과 같습니다:

라벨 없는 이미지 데이터로 먼저 오토인코더를 학습하여 이미지 데이터의 일반적인 특성을 잘 포착할 수 있습니다.  
이어서 소량의 정렬 데이터로만 텍스트 → 잠재 공간 매핑을 학습하기 때문에, 적은 라벨 데이터로도 더 강력하고 일반화된 텍스트-기반 이미지 생성 모델을 만들 수 있습니다.  
순수하게 정렬 데이터에만 의존하여 학습하는 것보다 더 넓은 데이터 특성 학습과 일반화 능력이 향상됩니다.  
요약하면, 사전학습된 오토인코더 디코더와 텍스트→잠재공간 매핑을 결합하여 조건부 샘플링을 수행하는 기법은, 제한된 정렬 데이터와 대량의 비정렬 데이터가 혼합된 실용적 환경에서 효과적인 이미지 합성 모델 학습 방안입니다.

#### Disentangling of factors
자동인코더(Autoencoder)에서 잠재 공간(latent space)으로의 분해된 인코딩(disentangling)은 특정 변동 요인(factor of variation)이 원본 공간 X의 거의 모든 차원에 영향을 미치지만, 잠재 공간 Z에서는 그 영향을 받는 차원이 적은 경우를 뜻합니다. 예를 들어, 얼굴 이미지 학습 시 인코더는 '사람의 정체성'과 '표정'을 각각 다른 잠재 변수 차원에 분리하여 표현할 수 있습니다. 즉, '사람의 정체성' 정보는 몇 개의 잠재 변수에만 집중되고, '표정' 정보는 다른 잠재 변수에 집중되는 식입니다.

이런 분해 특성은 변분 오토인코더(Variational Autoencoder, VAE)에서 특히 두드러지는데, 그 이유는 VAE가 잠재 변수의 분포에 대각 공분산(diagonal covariance) 구조를 강제로 부과하기 때문입니다. 즉, 잠재 변수들 간의 상호 상관관계를 최소화하도록 설계되어 각 잠재 변수는 서로 독립적인 요인을 학습하려고 합니다. 이로 인해 각 잠재 변수는 개별적인 생성 요인(generative factor)을 나타내게 되어 분해된 표현(disentangled representation)이 가능합니다.

또한, VAE의 손실 함수는 인코더가 학습한 잠재 분포가 미리 정해진 단순한 사전 분포(prior distribution), 보통은 표준 정규분포를 따르도록 강제하기 때문에 잠재 공간에서의 변수들이 서로 독립적이고 분리된 특성을 갖추게 돼 분해를 촉진합니다. 이 과정에서 β-VAE와 같은 변형 모델은 잠재 변수들 간 겹침(overlap)을 제어하여 분해 성능을 더욱 높일 수 있습니다.

정리하자면, VAE는

- 잠재 변수들 간 독립성 유도(대각 공분산 가정)
- 단순 사전 분포 강제화
- 손실 함수에서의 분포 규제
를 통해 원본 공간의 복잡한 변동 요인을 적은 수의 독립된 잠재 변수들로 분리하는 기능, 즉 분해된 잠재 표현을 효과적으로 학습할 수 있게 만듭니다.

#### Data manipulation in latent space 
데이터의 잠재 공간(latent space) 내 변형은, 비록 요인들이 잠재 공간에서 완전히 분리되지 않더라도, 실제 데이터 공간(data space)에서보다 더 쉽게 고수준(semantic) 편집이 가능하다는 점이 자주 관찰됩니다. 예를 들어, 얼굴 이미지 데이터셋에서 중립적인 표정을 웃는 표정으로 바꾸는 것 같은 변형은 복잡한 데이터 공간에서는 어려우나, 잠재 공간에서는 단순한 벡터의 이동(translation)으로 근사될 수 있습니다. 이러한 변환 벡터는 소량의 주석(annotation) 데이터를 통해 학습할 수 있는데, 예를 들어 여러 웃는 얼굴의 잠재 표현 평균과 중립 얼굴의 잠재 표현 평균 간 차이로 정의할 수 있습니다.

이 과정은 높은 차원의 이미지 데이터를 잠재 공간에 저차원으로 압축하여 핵심 특성만 보존함으로써, 복잡한 변형을 효과적이고 계산 효율적으로 수행할 수 있게 합니다. 특히, 생성 모델(GAN, VAE 등)은 잠재 공간을 활용해 새로운 데이터 샘플을 생성하고, 이 공간에서 선형 또는 비선형 변환을 학습해 의미 있는 편집을 수행합니다. 다만, 얼굴 이미지에 대해서는 선형 변환이 충분히 작동하는 반면, 다른 도메인에서는 비선형 변환이 필요할 수 있음을 연구들이 지적합니다.

이처럼 잠재 공간 내 변형은 데이터 공간에서 복잡한 변환을 간단한 벡터 연산으로 근사하며, 소량의 레이블 정보로도 의미 있는 고수준 편집을 가능하게 하는 중요한 기법입니다.

#### Unsupervised restoration and anomaly detection
Autoencoder는 데이터를 manifold에 프로젝션할 수 있는 능력을 활용하여 데이터 복구와 이상치 탐지에 사용될 수 있습니다.

Autoencoder의 기능
데이터 복구: Autoencoder는 데이터의 noise를 제거하고 원래 데이터로 복구하는 데 사용됩니다.
이상치 탐지: 데이터가 manifold에 속하지 않는 경우, Autoencoder는 이 데이터를 재구성하는 데 어려움을 겪으며, reconstruction error를 통해 이상치를 탐지할 수 있습니다.
이러한 기능들로 인해, Autoencoder는 다양한 분야에서 데이터를 정리하고 이상치를 찾아내는 데 유용하게 사용됩니다. 

그러나 최근 연구에 따르면, Autoencoder가 항상 신뢰할 수 있는 이상치 탐지 도구는 아니라는 점이 지적되었습니다.
