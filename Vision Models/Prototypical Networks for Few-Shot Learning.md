# Prototypical Networks for Few-Shot Learning | Image classification, Few-shot Learning

**핵심 주장 및 주요 기여**  
“Prototypical Networks”는 **극소수의 학습 예제**(few-shot)로 새로운 클래스에 빠르게 적응할 수 있는 분류 모델로, 각 클래스의 **프로토타입(대표 벡터)을 지원 집합(support set)의 평균**으로 정의하고, 쿼리(query) 예제는 이 프로토타입과의 **유클리드 거리**에 기반해 분류한다. 이 단순한 설계가 메타러닝 기반 복잡한 모델들보다 더 **효율적**이고 **일관된 성능 향상**을 가져옴을 보였다.

***

## 1. 문제 정의  
Few-shot classification 문제는 학습 단계에서 보지 못한 **신규 클래스**를, 클래스당 극소수(예: 1~5장)의 레이블된 예제로 학습하여 분류하는 과제이다. 전통적 방법처럼 재학습(retraining)할 경우 과적합이 심각하므로, **작은 데이터**에 적합한 단순한 귀납적 편향(inductive bias)이 요구된다.

***

## 2. 제안 방법  
### 2.1 모델 개요  
- 지원 집합 S = {(xᵢ, yᵢ)}에서 클래스 k에 속한 예제들의 임베딩 fφ(x) 평균으로 프로토타입 cₖ 정의:  

$$
    c_k = \frac{1}{|S_k|} \sum_{(x_i,y_i)\in S_k} f_\phi(x_i)
  $$

- 쿼리 x의 클래스 분포는 **소프트맥스** over 거리:  

$$
    p_\phi(y=k \mid x) = \frac{\exp(-d(f_\phi(x), c_k))}{\sum_{k'} \exp(-d(f_\phi(x), c_{k'}))}
  $$

- 본 논문에서는 Bregman 발산의 특성을 이용하여, **제곱 유클리드 거리**를 사용했을 때 프로토타입이 지원 집합의 최적 군집 중심임을 보이고, 학습은 음의 로그 우도(negative log-likelihood)를 최소화한다.

### 2.2 모델 구조  
- **임베딩 네트워크**: Omniglot/micro-ImageNet에서는 4개의 합성곱 블록(3×3 Conv → BatchNorm → ReLU → 2×2 MaxPool), 출력 차원 64 또는 1,600  
- **제로샷 확장**: 클래스 메타데이터(vₖ, 예: 속성 벡터)를 별도 임베딩 gϑ(vₖ)으로 매핑하여 프로토타입으로 사용

***

## 3. 성능 향상  
- Omniglot 5-way 1-shot에서 **98.8%**, 5-shot에서 **99.7%**로 최고 성능  
- miniImageNet 5-way 1-shot에서 **49.42%**, 5-shot에서 **68.20%** 달성  
- 제로샷 CUB-200 50-way 분류에서 **54.6%**로 기존 기법 대비 큰 폭의 개선  
- **유클리드 거리**가 코사인 거리 대비 Bregman 발산 특성을 만족해 성능이 크게 향상됨  
- **에피소드 구성**: 테스트 환경과 유사하게 “way”와 “shot”을 맞추되, 오히려 높은 “way”(클래스 수)로 학습 시 일반화 성능이 더욱 개선됨

***

## 4. 한계 및 일반화 성능 향상 관점  
- **한계**: 각 클래스당 하나의 프로토타입만 사용하므로 복잡한 클래스 내 분포(다모달 분포)를 충분히 반영하지 못함  
- **일반화 향상**: 높은 “way” 설정으로 학습 에피소드를 더 어려운 과제로 만들어, 네트워크가 세밀한 임베딩 학습을 수행하도록 유도  
- **거리 함수 확장**: 제곱 유클리드 외 다른 Bregman 발산(예: 마할라노비스 거리)을 적용 가능성 제시하나, 추가 매개변수 학습이 성능 향상에 기여하지 않는 것으로 관찰됨

***

## 5. 향후 연구 방향 및 고려 사항  
본 논문은 **단순한 클래스 평균**만으로도 효과적인 few-shot 분류가 가능함을 입증했다.  
- **다중 프로토타입**: 클래스 내 다모달 분포를 캡처하기 위한 여러 프로토타입 학습  
- **비정형 메타데이터**: 텍스트 설명 등 다양한 형식의 클래스 정보 임베딩  
- **거리 학습**: Bregman 발산을 넘어 비대칭 거리, 학습 가능한 메트릭 도입  
- **메타러닝 결합**: prototypical 구조와 meta-learner 통합으로 학습 및 일반화 능력 향상  

이와 같은 방향에서, 더 풍부한 클래스 표현과 더욱 견고한 일반화 능력을 갖춘 few-shot 학습 모델이 발전할 것으로 기대된다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/3ccf80ea-722f-414a-b54b-e626916f418f/1703.05175v2.pdf
