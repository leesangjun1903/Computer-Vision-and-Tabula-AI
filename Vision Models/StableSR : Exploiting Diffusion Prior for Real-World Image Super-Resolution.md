# StableSR : Exploiting Diffusion Prior for Real-World Image Super-Resolution | Super resolution

## 1. 핵심 주장 및 주요 기여
**핵심 주장:**  
사전 학습된 텍스트-투-이미지 확산 모델(Stable Diffusion)의 생성적(prior) 능력을 보존하면서, 최소한의 추가 학습만으로 **실제(real-world) 저해상도 이미지 슈퍼해상도**(SR) 문제를 효과적으로 해결할 수 있다.

**주요 기여:**  
1. **Time-Aware Encoder**: 확산 과정의 각 timestep별 신호 대 잡음비(SNR)에 적응하여, 저해상도(LR) 입력의 구조적 제약을 동적으로 확산 모델에 주입.  
2. **Controllable Feature Wrapping (CFW) 모듈**: 사용자 조작 가능한 스칼라 $$w$$로 **품질(realism)**과 **원본 충실도(fidelity)** 사이를 균형 조절.  
3. **Progressive Aggregation Sampling**: 임의 해상도 이미지를, 학습 해상도(512×512) 패치로 나누고 겹치는 영역을 가우시안 가중합하여 경계 불연속을 제거.  
4. **무훈련(from-scratch) 대비 효율성**: 확산 모델을 동결(freeze)하고, 인코더·SFT·CFW만 미세조정(finetune)하여 GPU 메모리 사용량과 학습 시간을 크게 절감.

## 2. 문제 정의 및 제안 방법

### 2.1 해결하고자 하는 문제
- **실제 세계 이미지 SR**는
  - 저해상도 이미지의 **미세 구조(fine detail)**와 **고충실도(fidelity)**를 복원해야 하나,
  - 기존 확산 기반 SR은 모델을 처음부터 훈련하거나, 열악한 degradations 가정(blur/noise 모델링)을 전제로 하여 일반화에 한계.

### 2.2 제안 모델 구조
![모델 구조]  
시간 인지 인코더(time-aware encoder)가 입력 LR을 인코딩→SFT(spatial feature transform) 레이어를 통해 UNet 중간 feature를 modulate→고정된 Stable Diffusion 모델→(옵션) CFW→디코더→HR 출력.

### 2.3 수식 요약
1. **Feature Modulation (SFT):**  

$$
   \hat F^{(n)}\_\text{dif} \;=\;(1+\alpha^{(n)}) \odot F^{(n)}\_\text{dif} \;+\;\beta^{(n)},\quad
   (\alpha^{(n)},\beta^{(n)}) = M_\theta^{(n)}\bigl(F^{(n)}\_\text{LR}\bigr)
$$

2. **Controllable Feature Wrapping (CFW):**  

$$
   F_m = F_d + w\;\bigl[C(F_e,\,F_d;\theta)\bigr],\quad w\in[1]
$$

3. **Aggregation Sampling:**

$$
   \epsilon_\theta(z_t, F, t) \;=\; \sum_{n=1}^M \frac{w_{\Omega_n}}{\sum w_{\Omega_n}}\;f\bigl(\epsilon_\theta(z_{t,\Omega_n},F_{\Omega_n},t)\bigr)
$$

## 3. 성능 향상 및 한계

### 3.1 성능 향상
- **Perceptual Metrics** (FID, CLIP-IQA, MUSIQ)에서 GAN·LDM 대비 일관된 우위.  
- **Validation Curve**: from-scratch 대비 빠른 수렴·더 낮은 LPIPS[Fig.10].  
- **User Study**: 실제 이미지 40장 대상, 35명 평가에서 80% 이상 선호.

### 3.2 한계
- **텍스트·작은 패턴·얼굴 SR** 성능 저하[Fig.18].  
- CFW 사용 시 메모리 소모 증가.  
- 확산 속도: 기본 DDPM(200 steps) → ≈15s/512².  

## 4. 일반화 성능 향상 관점
- **비가정(degradation-agnostic)**: 실제 degradations 사전 지식 없이도 강건.  
- **패치 기반 애그리게이션**으로 임의 해상도 대응.  
- **시간 인지** 제약으로 각 diffusion 단계에서 구조적 guidance 최적화.  
- “ControlNet-like” 복사 없이 경량 encoder만 fine-tune → 다양한 사전 학습 확산 모델 전이 가능.

## 5. 향후 연구에 미치는 영향 및 고려 사항
- **Impact**  
  - **확산 모델 재활용**: 거대 텍스트-투-이미지 모델을 SR·복원·편집 등 저수준 비전 과제에 손쉽게 전이.  
  - **다중 해상도 확장성**: arbitrary-size 영상 복원·초해상도에 적용 가능.  
- **고려 사항**  
  1. **더 강력한 사전학습 모델**(예: SDXL)과 **프롬프트 활용**으로 텍스트·얼굴 SR 성능 개선.  
  2. **속도 최적화**: 빠른 샘플러(DDIM, SD-Turbo) 또는 distillation 기법 도입.  
  3. **CFW 경량화**: 메모리·연산 절감 위한 네트워크 축소 및 sparse adaptation.  
  4. **데이터 다양성**: 현실세계 degradations 및 도메인별(의료·위성·문서) SR 평가.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d5463ed7-76f1-4e91-acba-98015345c462/2305.07015v4.pdf

# Abstract
논문에서는 Blind Super Resolution(SR)을 위해 Pre-trained Text to Image Diffusion Model의 사전 지식을 활용하는 새로운 방법론을 제시한다.  
본 논문에서 소개하는 방법론의 이점은 다음과 같다.

1. Pre-trained Synthetic Model을 변경하지 않고 Time-Aware Encoder를 사용함으로써 교육 비용 최소화 및 성능 향상
2. Realism과 Fidelity 사이의 Trade off를 조정할 수 있는 Feature Wrapping Module 제안
3. Pre-trained Diffusion Model의 고정 사이즈 제약을 극복하기 위해 progressive aggregation sampling 전략 사용

# Introduction
본 연구에서는 Super Resolution 분야에서 diffusion prior를 사용했을 때의 잠재적인 이점에 대해서 검토한다.  

기존 Diffusion model은 확률적 특성과 대조적으로 높은 이미지 fidelity를 요구하기때문에 어려움을 겪는다. 
("피델리티(Fidelity)"란, 모델의 예측 결과가 실제 결과와 얼마나 일치하는지를 나타내는 지표입니다. https://velog.io/@nomaday/Fidelity)
일반적인 해결 방법은 SR 모델을 처음부터 훈련시키는 것인데, 이 방법의 경우 성능은 어느정도 보장이 되지만, 모델을 훈련시키기 위한 리소스가 매우 많이 소요된다는 단점과 일반화 문제를 포함.

이러한 한계는 pre-trained synthesis model의 reverse diffusion processs에 제약 조건을 통합하는 것을 포함한 몇가지 대안적인 접근법에 영감을 주었지만, 이 방법 역시 image degradations에 대해서 사전에 알고 있다는 가정이 필요하므로 적용하기 어려운 문제가 존재.  

그래서 이 연구에서는 성능 저하에 대한 명시적인 가정을 하지 않고, pre-trained diffusion priors를 보존하는 접근법인 StableSR을 제시한다.  

StableSR의 핵심 아이디어는 다음과 같다.
- time-aware encoder와 일부 feature modulation layer에 대한 fine-tuning을 진행함으로써 training efficiency 향상  
→ encoder에서 time embedding layer를 통합하여 time-aware feature를 생성하여 diffusion model의 feature가 서로 다른 time step에서 적응적인 특성을 지님

- original diffusion model을 frozen 상태로 유지하여 generative prior를 보존  

- encoding process로 인한 정보 손실과 diffusion model의 확률성을 억제하기위해 조정가능한 상관계수를 가진 feature wrapping module을 적용  
임의의 해상도를 처리하기 위해 이미지를 중복 패치로 나누고 각 diffusion iteration에서 Gaussian kernel을 사용하여 합치는 방식인 progressive aggregation sampling strategy를 도입

# Related Work
## Image Super-Resolution
기존의 SR 접근법들은 보통 사전 정의된 degradation process를 가정한다. (e.g. bicubic downsampling, blurring) 
이러한 접근법들은 real-world 시나리오에서 제한된 일반화 능력으로 인해 성능 저하로 이어지게 된다.

최근 연구들은 synthetic setting에서 blind SR로 초점을 옮겼다.  
이와 관련해서 학습을 위한 real-world pair set을 구하기 어렵기 때문에 CycleGan과 contrastive learning과 같이 unsupervised 방법으로 해결하려는 접근 방식을 주로 사용했다.

unsupervised learning 이외에도 실제 데이터와 유사한 LR-HR image pair를 합성하는 접근 방식도 연구되었는데, 이와 관련해서 BSRGAN과 Real-ESRGAN은 real world에서 blind SR을 위한 효과적인 degradation pipeline을 제시했다.

이러한 degradation pipeline을 기반으로한 diffusion model과 같은 최근 연구들은 real-world image SR에서 경쟁력있는 퍼포먼스를 보이고 있다.  
본 연구에서는 SR을 위한 diffusion model에 대해서 fine-tuning을 orthogonal direction으로 고려함으로써 네트워크 학습 계산 비용을 줄이고 synthesis model의 generative prior을 활용하여 더 나은 성능을 얻도록 했다. → 모델 재사용성 강조

## Prior for Image Super-Resolution
복잡한 real-world SR 시나리오에서 성능을 높이기 위해 다양한 prior-based 접근법들이 제안되어 왔다.  
이러한 기법들은 texture 생성을 강화하기 위해 이미지를 추가로 배치한다.

최근에는 implicit prior에 기반한 접근법으로 성능을 향상시키는 연구가 진행됐다.  
이와 관련해서 Wang et al.은 semantic segmentation 확률맵을 feature space에서 SR을 guide하기 위해 최초로 제안했다.  
후속 작업에서는 LR input의 HR latent space을 탐색하는 pre-trained GAN을 채택했다.

이러한 방법론들은 효과적이지만 제한된 범주에 맞게 조정되는 경우가 많기 때문에 복잡한 real-world SR에 대한 일반화 성능이 부족한 문제가 있다.

논문에서는 기존 전략과 대조적으로 pre-trained diffusion model에서 robust하고 extensive generative prior을 탐색하는것을 목표로한다.  
최근 연구에서는 pre-trained diffusion model의 높은 generation 성능을 강조했지만, super-resolution에 내제된 high fidelity 요구 사항은 이러한 방법을 사용하기 어렵게 만든다.  
StableSR은 LDM과 달리 처음부터 학습시키는 대신, 소수의 학습가능한 매개 변수만을 fine-tuning함으로써 우수한 성능을 얻었다.

# Methodology
![](https://velog.velcdn.com/images/sckim0430/post/b39bafde-e63a-448c-9160-8ab235073e04/image.png)

StableSR의 주요 구성 요소는 time-aware encoder로, input image을 기반으로 조절가능한 frozen Stable Diffusion model과 함께 학습된다.  
그리고 realism과 fidelity 사이의 trade-off을 더 용이하게 하기위해 CodeFormer를 따라 Controllable feature wrapping module(CFW)을 도입했다.

## Guided Finetuning with Time Awareness
SR을 위한 Stable Diffusion의 prior knowledge를 활용하기 위해 논문 저자는 모델 설계할 때 다음과 같은 제약 조건을 설정했다.

1) 모델은 관측된 LR 입력에 따라 신뢰할 수 있는 HR 이미지를 생성할 수 있어야 한다.
2) 모델은 original Stable Diffusion 모델로부터 최소한의 변경만 도입해야한다.

### Feature Modulation
generation process를 보다 정확하게 유도하기 위해 LR image feature로부터 multi-scale feature( $(F^{n})^{N} _{n=1}$

를 추출하기 위한 추가 encoder를 사용했고,  

이를 사용해서 spatial feature transformation을 통해 Stable Diffusion에서 intermediate feature map( $(F^{n}_{dif})^{N} _{n=1}$ )
​
 )을 조정한다.  
 이 과정의 식은 다음과 같다.

 ![](https://velog.velcdn.com/images/sckim0430/post/90e64139-fa41-4b5c-b9d0-30c2daa5b1ae/image.png)

 여기서, $α^n$과 $β^n$은 SFT에서의 affine 파라미터를 의미하고, $M^θ_{n}$
​
 는 convolution network로 구성된 small network를 의미한다.  
 그리고 n은 Stable Diffusion의 Unet 구조의 spatial scale을 의미한다.  

fintuning 동안 Stable Diffusion의 weight는 freeze되고 오직 encoder와 SFT layer(https://mr-waguwagu.tistory.com/31)만 학습된다.

### Time-aware Guidance
제안하는 Encoder에 대해서 시간 정보, time-embedding layer(https://dlaiml.tistory.com/entry/Diffusion-Models-Beat-GANs-on-Image-Synthesis)를 통합함으로써 LR feature에서 파생된 condition strength를 적응적으로 조정하여 generation quality와 ground truth fidelity를 향상시켰다.  
generation process 동안 노이즈는 점차 제거됨에 따라 생성된 이미지의 SNR(Signal to Noise Ratio)이 점진적으로 증가한다.  
이와 관련해서 최근 연구에 따르면 SNR이 5e-2일때 이미지 컨텐츠가 빠르게 채워진다고 주장했고, 이러한 연구에 따라 제안된 encoder는 SNR이 5e-2를 만족하는 범위내에서 diffusion model이 비교적 강한 조건을 제공하도록 설계되었다.

논문에서는 SFT 전후의 Stable Diffusion의 feature간에 cosine similarity를 사용하여 encoder가 제공하는 조건 강도를 측정했다.  
서로 다른 timestep에서의 cosine similarity는 아래 Fig 3에 제시되어 있다.

![](https://velog.velcdn.com/images/sckim0430/post/88d47a46-6859-44c3-975e-9b5a7019f3bb/image.png)

측정 결과, timestep에 따라 SNR은 점차 감소했고, SNR이 5e-2인 구간에서 Cosine Similarity가 저점을 기록한 것을 확인할 수 있다.  
그리고 feature map을 시각화 했을때 이 구간에서 빠르게 이미지의 컨텐츠가 채워져 더 상세한 이미지 구조를 가지는 것을 확인할 수 있다.

### Color Correction
Diffusion Model은 color shift 문제(색 구성 문제, https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/p2weight/)를 가지고 있는데, 본 논문에서는 이러한 문제를 해결하기 위해 다음과 같이 Color Normalization을 수행한다.

![](https://velog.velcdn.com/images/sckim0430/post/6b9229e7-85e3-481c-9893-6aef13ff9192/image.png)

여기서 각 파라미터의 의미는 다음과 같다.
- $x$ : LR input image
- $\hat y$ : HR output image
- $c ∈ {r,g,b}$ : color channel  
- $μ, σ$ : 각 x와  $\hat y$ 에서의 c 채널의 평균 및 표준편차 값(이미지의 색상 값 분포)

## Fidelity-Realism Trade-off
제안된 접근의 출력은 시각적으로 설득력이 있지만, Diffusion Model의 고유한 확률성으로 인해 가끔 ground truth와 편차가 발생하게 된다.  
본 연구에서는 이러한 문제를 해결하기 위해 Controllable Feature Wrapping(CFW) module을 소개한다.

CFW의 수식은 다음과 같다.

![](https://velog.velcdn.com/images/sckim0430/post/672d6d3b-2774-433b-8460-73e980451bd6/image.png)

여기서, 각 파라미터 수식은 다음과 같다.
- $F_e$ : Encoder Feature
- $F_d$ : Decoder Feature
- $C$ : Convolution Layers
- $θ$ : Trainable Parameter
- $w∈[0,1]$ : adjustable coefficient, w는 값이 작으면 high realism, 반대로 높으면 fidelity를 높이는 역할을 한다.

## Aggregation Sampling
StableSR의 경우, 학습 설정과 다른 해상도에서 안좋은 성능을 보이는 경향이 있다.  
이러한 문제에 대한 일반적인 해결책은 여러 개의 중첩된 작은 패치로 분할하여 각각의 패치의 결과를 합치는 방식이다.  
그러나 이러한 방법들은 패치들 간의 불일치로 인해 문제가 발생된다. 이와 관련한 사례는 Fig 4에 제시되어 있다.

![](https://velog.velcdn.com/images/sckim0430/post/637adf5a-2362-4ff8-b2a4-b8d84bd16c84/image.png)

본 논문에서는 임의의 해상도의 이미지를 처리하기 위해 progressive patch aggregation sampling algorithm을 적용했다.

먼저, LR 이미지를 latent feature map으로 인코딩한 뒤, 64x64 resolution의 multiple overlapping small patch로 분할한다.  
reverse sampling에서의 각 timestep에서 각각 StableSR을 통해 처리되고, 처리된 patch들은 이후에 합쳐진다.  
그리고 중복되는 patch들을 합치기 위해 가우시안 커널을 통해 64x64 크기의 가중치 맵을 생성하고 중첩 픽셀은 각 가우시안 가중치 맵에 의해 가중치가 부여되어 합쳐진다.(https://ostin.tistory.com/174)

# Experiments
## Compared Methods.

![](https://velog.velcdn.com/images/sckim0430/post/75196ff8-3e40-4a74-8101-c33c2482e18f/image.png)

- PSNR(Peak Signal-to-Noise Ratio) : 영상 내 신호가 가질 수 있는 최대 신호에 대한 노이즈 비율
- SSIM(Structural Similarity Index Map) : 휘도, 대비, 구조 측면에서 품질 평가
- LPIPS(Learned Perceptual Image Patch Similarity) : Feature Map 기반 유사도 측정 값
- FID (Frechet Inception Distance) : Activation Map 기반 확률 분포 유사도 측정 값
- CLIP-IQA : Clip 모델을 활용한 Embedding 방식 기반 이미지 퀄리티 측정 방법
- MUSIQ(Multi-Scale Image Quality Transformer) : multi-scale 이미지 품질 평가 모델 기반 평가 방법

## Qualitative Comparisons.
![](https://velog.velcdn.com/images/sckim0430/post/21a5fc74-424b-44a3-a2f1-9bd671f52274/image.png)

## User Study.
![](https://velog.velcdn.com/images/sckim0430/post/dae5305d-80e8-48d9-bc38-ad72c011433e/image.png)

## Importance of Time-aware Guidance and Color Correction.
![](https://velog.velcdn.com/images/sckim0430/post/fa0c0c28-3907-42f2-b33a-b41e3c60f47c/image.png)

Time-aware encoder 와 Color-correction 을 사용하였을 때 이미지이다.
두 가지 전략을 사용한 StableSR 모델이 더 sharp, 날카로운 textures(질감)을 만들어내며 color shifts 문제를 피할 수 있었다.

## Flexibility of Fidelity-realism Trade-off.(Visual comparisons with different coefficients w for CFW module)
![](https://velog.velcdn.com/images/sckim0430/post/867940c1-b319-49ed-8860-285e4d1ff0c9/image.png)

작은 w 계수를 사용하면 realistic result, 실제적인 결과 이미지를 생성해내며 w가 커지면 fidelity를 높인다.

# Conclusion
StableSR은 SR에 대해서 높은 성능을 보이지만, time step 200을 기준으로 32G Tesla V100에서 512x512 이미지를 생성했을 때, 약 10초가 걸리는 한계가 있다.  
추후 연구에선 빠른 Sampling 전략과 Model Distillation 관점에서 이러한 제한점을 해결하고자 한다.

본 논문에서는 SR에 대해서 Diffusion Prior을 어떻게 적용할 수 있을지에 대한 문제를 집중적으로 다뤘고, Fine Tuning시에 전체적인 모델 학습을 피하면서 real-world SR을 위한 diffusion prior 활용하는 새로운 방법인 StableSR을 제안했다.

그리고 높은 계산 비용과 고정 해상도 문제를 해결할 수 있는 Time-Aware Encoder, CFW, Aggregation Sampling Scheme 등을 소개했다.

# Reference
- https://velog.io/@sckim0430/Exploiting-Diffusion-Prior-for-Real-World-Image-Super-Resolution
