# DiffuseIT : Diffusion-based Image Translation using Disentangled Style and Content Representation | 2022 · 215회 인용, Image-to-Image Translation

## 1. 핵심 주장과 주요 기여 요약

[1]

**DiffuseIT**라는 새로운 diffusion 기반 비지도 이미지 변환 방법을 제안하여, **스타일과 콘텐츠를 분리**하여 제어할 수 있는 이미지 변환을 실현했습니다. 주요 기여는 다음과 같습니다:

### 핵심 기여 요소
- **ViT 기반 콘텐츠 보존**: DINO ViT의 중간 레이어 키(keys)를 활용한 구조적 정보 보존
- **[CLS] 토큰을 통한 스타일 제어**: ViT의 [CLS] 토큰으로 의미적 스타일 정보 제어
- **텍스트/이미지 이중 조건부**: CLIP 손실과 이미지 기반 스타일 손실을 통한 유연한 제어
- **의미적 발산 손실**: 역확산 과정에서 의미 변화를 가속화하는 새로운 손실 함수
- **리샘플링 전략**: 초기화 개선을 통한 생성 과정 최적화

## 2. 해결하고자 하는 문제와 제안 방법

### 해결하려는 문제
기존 diffusion 모델의 **확률적 특성**으로 인해 역확산 과정에서 **원본 이미지의 콘텐츠를 유지하면서 스타일만 변경하는 것이 어려웠던 문제**입니다. 특히:[1]

- 조건부 diffusion 모델은 matched 타겟 이미지가 필요하여 실용적이지 않음
- 무조건부 diffusion 모델은 의미와 콘텐츠가 동시에 변화하는 얽힘 문제 발생
- 기존 방법들은 픽셀 단위 손실이나 지각적 손실을 사용하여 콘텐츠와 의미 요소를 명시적으로 구분하지 못함

### 제안하는 방법 (수식 포함)

#### **1. Manifold Constrained Gradient (MCG) 기반 접근**
역확산 과정을 역문제로 공식화하여 총 비용 함수를 최소화:[1]

$$ \ell_{total}(x; x_{trg}, x_{src}) \text{ 또는 } \ell_{total}(x; d_{trg}, x_{src}, d_{src}) $$

여기서 $$x_{src}$$와 $$x_{trg}$$는 소스/타겟 이미지, $$d_{src}$$와 $$d_{trg}$$는 소스/타겟 텍스트입니다.

#### **2. 구조 손실 (Structure Loss)**
**자기 유사성 손실**과 **대조 학습**을 결합:[1]

$$ \ell_{ssim}(x_{src}, x) = \|S^l(x_{src}) - S^l(x)\|_F $$

여기서 $$[S^l(x)]_{i,j} = \cos(k^l_i(x), k^l_j(x))$$

**InfoNCE 기반 대조 손실**:

$$ \ell_{cont}(x_{src}, x) = -\sum_i \log \frac{\exp(\text{sim}(k^l_i(x), k^l_i(x_{src}))/\tau)}{\exp(\text{sim}(k^l_i(x), k^l_i(x_{src}))/\tau) + \sum_{j \neq i} \exp(\text{sim}(k^l_i(x), k^l_j(x_{src}))/\tau)} $$

#### **3. 스타일 손실 (Style Loss)**
**텍스트 기반**: 입력 인식 방향성 CLIP 손실:[1]

$$ \ell_{CLIP}(x; d_{trg}, x_{src}, d_{src}) = -\text{sim}(v_{trg}, v_{src}) $$

여기서:

$$ v_{trg} = E_T(d_{trg}) + \lambda_i E_I(x_{src}) - \lambda_s E_T(d_{src}) $$

$$ v_{src} = E_I(\text{aug}(x)) $$

**이미지 기반**: [CLS] 토큰 거리 매칭:[1]

$$ \ell_{sty}(x_{trg}, x) = \|e^L_{[CLS]}(x_{trg}) - e^L_{[CLS]}(x)\|^2 + \lambda_{mse}\|x_{trg} - x\|^2 $$

#### **4. 의미적 발산 손실 (Semantic Divergence Loss)**
생성 과정 가속화를 위한 새로운 손실:[1]

$$ \ell_{sem}(x_t; x_{t+1}) = -\|e^L_{[CLS]}(\hat{x}_0(x_t)) - e^L_{[CLS]}(\hat{x}_0(x_{t+1}))\|^2 $$

#### **5. 최종 손실 함수**

$$ \ell_{total} = \lambda_1\ell_{cont} + \lambda_2\ell_{ssim} + \lambda_3\ell_{CLIP} + \lambda_4\ell_{sem} + \lambda_5\ell_{rng} $$

## 3. 모델 구조

### 핵심 구조 요소
1. **DINO ViT**: 콘텐츠(키)와 스타일([CLS] 토큰) 특징 추출
2. **CLIP 앙상블**: 5개 사전 훈련된 CLIP 모델 (RN50, RN50x4, ViT-B/32, RN50x16, ViT-B/16)
3. **DDPM 백본**: ImageNet 256×256에서 사전 훈련된 무조건부 스코어 모델
4. **리샘플링 모듈**: N=10회 반복으로 초기 $$x_T$$ 최적화

### 생성 과정
- **총 스텝**: 텍스트 기반 70회 (T=60 + N=10), 이미지 기반 130회 (T=120 + N=10)
- **추론 시간**: RTX 3090에서 약 40초/이미지
- **해상도**: 256×256 픽셀

## 4. 성능 향상 및 한계

### 성능 향상
**정량적 결과**:[1]
- Animals 데이터셋: SFID 9.98 (최고), CSFID 41.07 (최고)
- Landscapes 데이터셋: SFID 16.86 (최고), CSFID 54.48 (최고)
- 사용자 연구: 텍스트 매칭 3.68, 사실성 4.28, 콘텐츠 보존 4.11 (모두 최고)

**핵심 개선사항**:
- 기존 방법 대비 **구조적 일관성** 크게 향상
- **의미적 변화**와 **콘텐츠 보존**의 균형 달성
- **다양한 도메인**에서 안정적 성능 (동물, 풍경, 예술적 스타일)

### 한계점
1. **극단적 도메인 갭**: 소스와 타겟 간 의미적 거리가 너무 클 때 (예: 건물→호랑이) 변환 실패[1]
2. **백본 의존성**: 사전 훈련된 스코어 모델 성능에 크게 의존
3. **추론 시간**: 다중 역확산 스텝으로 인한 상대적으로 긴 처리 시간
4. **CLIP 임베딩 제약**: 텍스트-이미지 임베딩 공간의 정렬 한계

## 5. 일반화 성능 향상 가능성

### 강점
**도메인 독립성**: 무조건부 diffusion 모델 사용으로 **특정 도메인에 제약받지 않는** 범용적 적용 가능[1]

**다중 모달 제어**: 텍스트와 이미지 조건을 동시에 지원하여 **유연한 스타일 제어** 실현

**ViT 기반 분리**: DINO ViT의 사전 훈련된 특징을 활용하여 **다양한 이미지 유형**에서 안정적인 콘텐츠-스타일 분리

### 일반화 개선 방향
1. **더 강력한 백본**: 최신 diffusion 모델 (DDPM → DDIM, LDM 등) 활용
2. **고급 임베딩**: CLIP 이후의 더 정교한 텍스트-이미지 정렬 모델 사용
3. **적응적 가중치**: 도메인별 손실 함수 가중치 자동 조정
4. **계층적 제어**: 다중 스케일에서의 세밀한 스타일-콘텐츠 제어

## 6. 앞으로의 연구에 미치는 영향과 고려사항

### 연구 영향
**패러다임 전환**: GAN 기반에서 **diffusion 기반 이미지 변환**으로의 전환 가속화

**분리 표현 학습**: ViT 특징을 활용한 **콘텐츠-스타일 분리** 접근법의 새로운 표준 제시

**다중 모달 제어**: 텍스트와 이미지를 동시에 활용하는 **통합된 조건부 생성** 프레임워크 확립

### 향후 연구 고려사항

**기술적 개선**:
- **실시간 처리**: 추론 시간 단축을 위한 경량화 및 가속화 기법 연구
- **고해상도 확장**: 256×256을 넘어서는 고해상도 이미지 처리 능력 개발
- **3D 확장**: 2D 이미지에서 3D 객체/장면으로의 확장 가능성

**응용 분야**:
- **의료 영상**: 의료 이미지의 도메인 적응 및 데이터 증강
- **영상 제작**: 영화/게임 산업에서의 자동 스타일 변환
- **교육 콘텐츠**: 시각적 학습 자료의 스타일 다양화

**연구 방향**:
- **해석 가능성**: ViT 특징이 실제로 콘텐츠와 스타일을 얼마나 잘 분리하는지에 대한 정량적 분석
- **윤리적 고려**: 딥페이크 방지 및 악용 방지를 위한 검출/방어 메커니즘 연구
- **사용자 제어**: 더 직관적이고 세밀한 사용자 인터페이스 개발

이 연구는 diffusion 모델의 **제어 가능성**을 크게 향상시켰으며, 앞으로 **더욱 정교하고 실용적인 이미지 편집 도구**의 발전을 이끌 것으로 예상됩니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/6ae918e7-1dc3-4d5a-966c-db9a945876ae/2209.15264v2.pdf)
