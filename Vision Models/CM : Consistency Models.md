# CM : Consistency Models | 2023 · 1357회 인용, Image generation, Accelerate Sampling

# 핵심 요약 및 주요 기여

**주요 주장:** Consistency Models는 복잡한 반복 샘플링 없이도 단일 네트워크 평가로 고품질 샘플을 생성하는 **일관성(self-consistency)** 기반의 새로운 생성 모델 패밀리이다. 이 모델은 노이즈에서 데이터로 직접 매핑하며, 필요에 따라 멀티스텝 샘플링으로 연산량과 샘플 품질 간 트레이드오프가 가능하다. 또한, 사전 훈련된 확산 모델을 증류(distillation)하거나 완전한 독립 모델로 학습(isolation)할 수 있으며, 제로샷(Zero-Shot) 이미지 편집(인페인팅, 컬러라이제이션, 슈퍼해상도 등)을 지원한다.[1]

# 자세한 설명

## 1. 해결 문제  
전통적 확산 모델(Diffusion Models)은 SDE/ODE 수치 해석을 통해 반복적으로 노이즈를 제거하므로, 수백에서 수천 번의 네트워크 평가가 필요해 **추론 속도가 느림**과 **실시간 적용의 한계**가 있다.

## 2. 제안 방법  
- **일관성 함수 정의:**  
  ODE 궤적 $$x_t\to x_0$$의 모든 시점 $$(x_t,t)$$에 대해 동일한 초기점 $$x_0$$으로 복원하도록 학습할 함수 $$f(x_t,t)$$을 정의.  
  $$f(x_t,t)=x_0,\quad \forall\,t$$  
- **파라미터화:**  

$$
    f(x,t)=c_{\text{skip}}(t)\,x\;+\;c_{\text{out}}(t)\,F(x,t),
  $$
  
  여기서 $$c_{\text{skip}}(t)+c_{\text{out}}(t)=1$$로 경계 조건을 만족시킴.  
- **멀티스텝 샘플링 (Algorithm 1):**  
  단일 평가로 샘플 생성 후, 반복 평가와 노이즈 주입(noise injection)으로 품질 개선 및 편집 가능.  
- **훈련 방식:**  
  1) **Consistency Distillation (CD):**  
     사전 훈련된 확산 모델의 ODE 샘플링 궤적을 따라 인접 시점 쌍 $$(x_{t_{n+1}},x_{t_n})$$을 생성하고,  

$$
       L_{\mathrm{CD}}
       =\mathbb{E}_{t_n,x}\bigl\|f(x_{t_{n+1}},t_{n+1}) - f(x_{t_n},t_n)\bigr\|^2
     $$
     
  으로 최소화하여 확산 모델을 한 번의 평가로 증류.[1]
  2) **Consistency Training (CT):**  
     확산 모델 없이도, 무작위 데이터 점 $$x$$와 노이즈 $$z\sim\mathcal{N}(0,I)$$로 구성된 쌍 $$(x,z)$$만으로  

$$
       L_{\mathrm{CT}}
       =\mathbb{E}_{t,x,z}\bigl\|f(x_{t+1},t+1) - f(x_t,t)\bigr\|^2,\quad x_{t+1}=x + \sqrt{\Delta t}\,z
     $$
     
  을 최소화하여 독립적 생성 모델로 학습.[1]

## 3. 모델 구조  
- 기반 네트워크는 기존 확산 모델의 UNet 계열 아키텍처 차용  
- Skip connection 기반 파라미터화 활용  
- EMA(지수 가중 이동 평균) 및 LPIPS 유사도(metric)로 안정적 학습  

## 4. 성능 향상  
- **CIFAR-10 (32×32)**  
  - 1-스텝 샘플: FID 3.55 (종전 SOTA 8.34 대비 획기적 개선)  
  - 2-스텝 샘플: FID 2.93  
- **ImageNet 64×64**  
  - 1-스텝 샘플: FID 6.20 → 종전 15.39  
  - 2-스텝 샘플: FID 4.70  
- **LSUN 256×256**  
  - CD는 대부분 1-,2-스텝 샘플에서 Progressive Distillation 대비 우수.[1]

## 5. 한계  
- **싱글 스텝에서 데이터 복잡도 높을수록** 소폭 성능(예: LSUN Bedroom) 열세  
- **연속시간(continuous-time) 목표 함수**는 포워드 모드 자동 미분 필요, 구현 복잡도 존재  
- CT 학습 시 **편향-분산(bias-variance) 트레이드오프**로 적절한 이터레이션 수 스케줄링 필요  

# 일반화 성능 강화 관점

- **독립 학습(CT) 이론 보장:** 확산 모델 없이도 **편향 없는(score estimation) 추정**으로 $$L_{\mathrm{CT}}\to0$$ 시 모델이 실제 일관성 함수에 수렴함이 증명됨.[1]
- **구조적 일관성:** 동일 초기 노이즈로 생성된 샘플들이 **공통 구조**를 공유, 모드 붕괴(mode collapse) 위험 감소.  
- **제로샷 편집:** 학습 시 특정 태스크 미포함에도 **인페인팅, 컬러라이제이션, 슈퍼해상도, 스트로크 편집** 등 다양한 역문제 해결 능력에서 일반화 우수.[1]

# 향후 연구 및 고려 사항

- **연속시간 목표 함수 최적화:** 자동 미분 지원 없이도 효율적 구현 가능한 근사 방법 연구  
- **스케줄링 전략 개선:** 최적의 $$\Delta t$$ 및 EMA 감쇠율(schedule) 자동 탐색 기법 개발  
- **다양한 도메인 확장:** 음성, 텍스트, 3D 생성 등 **멀티모달** 환경으로의 적용  
- **아키텍처 혁신:** Transformer 기반 구조 등 **비대칭적 파라미터화** 연구  
- **교차 분야 시너지:** 강화학습, 대비학습(self-supervised) 기법과의 융합으로 **일관성 강화 및 효율성** 향상  

이로써 Consistency Models는 **실시간 생성**과 **다양한 편집**에서 차세대 생성 모델 패러다임을 제시하며, 향후 AI 생성 모델 연구에 중요한 초석이 될 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/42bf5649-eed8-4be1-bfa3-a7292014fa59/2303.01469v2.pdf)
