# Genetic CNN | Image classification

## 1. 핵심 주장 및 주요 기여  
**Genetic CNN** 논문은 *딥 CNN 구조를 수동 설계가 아닌 유전 알고리즘을 통해 자동으로 탐색*할 수 있음을 입증한다. 주요 기여는 다음과 같다.  
- **네트워크 구조 인코딩**: 각 CNN 구조를 고정 길이의 이진 문자열(binary string)로 표현하는 기법 제안.  
- **유전 알고리즘 적용**: 선택(selection), 돌연변이(mutation), 교차(crossover) 연산을 통해 대규모 구조 탐색 공간을 효율적으로 탐색.  
- **소규모 데이터셋 실험**: MNIST, CIFAR-10에서 자동 진화된 구조가 기존 수동 설계된 모델보다 동등하거나 개선된 성능을 보임.  
- **전이 학습 가능성**: CIFAR-10에서 학습된 구조를 ILSVRC2012에 적용하여 VGG 수준의 성능을 달성하며 대규모 과제에도 적용 가능함을 입증.

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 문제 정의  
- CNN 아키텍처 설계는 보통 수작업에 의존하며, 가능한 구조 조합은 레이어 수에 따라 기하급수적으로 증가.  
- 모든 조합을 일일이 평가하기에는 계산 비용이 과도함.

### 제안 방법  
1. **구조 인코딩**  
   - S 단계(stage), 각 단계에 $$K_s$$개의 노드(node)를 가정.  
   - 동일 단계 내 $$1 \le i < j \le K_s$$인 노드 쌍 $$(v_{s,i},v_{s,j})$$의 연결 유무를 이진 비트로 표현.  
   - 전체 비트 길이 $$L = \sum_{s=1}^S \frac{K_s(K_s-1)}{2}$$.  
2. **유전 알고리즘 흐름**  
   - 초기화: $$N$$개의 랜덤 이진 문자열 생성.  
   - 각 세대마다  
     - **선택**: 러시안 룰렛 방식으로 생존 개체 결정 (적합도는 검증 데이터에서의 정확도).  
     - **교차(crossover)**: 확률 $$p_C$$로 개체 쌍의 단계별 이진 서브스트링을 교환($$q_C$$ 비율).  
     - **돌연변이(mutation)**: 확률 $$p_M$$로 각 비트를 $$q_M$$ 확률로 뒤집음.  
     - **평가**: 새 개체를 처음부터 학습하여 정확도 측정.  
   - 총 $$T$$세대 반복.  

   수식 예시)  

$$
     L = \sum_{s=1}^S \frac{K_s(K_s - 1)}{2}, 
     \quad 
     \Pr[\text{선택}_n] \propto r_n - \min_i r_i
   $$
   
  여기서 $$r_n$$은 $$n$$번째 개체의 검증 정확도이다.

### 모델 구조  
- **단계별 블록**: 각 노드는 합연산 후 $$3\times3$$ 합성곱–배치정규화–ReLU로 구성.  
- **기본 모듈**: VGG, ResNet, DenseNet 등을 동일 인코딩으로 표현 가능.  
- **확장성**: MNIST에서는 $$(S,K)=(2,(3,5))$$, CIFAR-10에서는 $$(S,K)=(3,(3,4,5))$$ 구조로 실험.

### 성능 향상  
- **MNIST**: 세대가 진행될수록 평균·중앙 정확도 상승, 최고 정확도 99.66% 달성(기존 LeNet 대비 소폭 개선).  
- **CIFAR-10**: 최고 77.06% 정확도, DenseNet(76.84%)보다 우수한 성능 획득.  
- **전이 실험**: CIFAR-10 학습 구조를 ILSVRC2012에 적용 시 VGG-16 대비 Top-1 오류율 28.12%→27.87%로 개선.  

### 한계  
- **계산 비용**: 각 개체를 학습해야 하므로 전체 탐색에 수십 GPU-day 소요.  
- **모듈 제한**: 현재 합성곱·풀링만 인코딩, Inception 모듈 등 복합 모듈은 표현 불가.  
- **가중치 학습 분리**: 구조 탐색과 가중치 학습이 분리되어 있어 최적화 동시 수행 불가.

## 3. 일반화 성능 향상 가능성  
- **전이 학습**: 소규모 데이터에서 유전 탐색한 구조가 대규모 과제에도 효과적임을 입증.  
- **유전적 다양성 유지**: 돌연변이 및 교차 연산을 통해 과도한 과적합 없이 다양한 구조를 탐색함으로써 새로운 데이터셋에의 적응력 향상.  
- **적합도 기반 선택**: 적합도 분포 기반 선택으로 성능이 우수한 유전자를 보존하여 일반화 성능을 꾸준히 개선.

## 4. 향후 연구 영향 및 고려 사항  
- **연구 영향**:  
  - *자동 구조 탐색(AutoML)* 분야 초기 사례로, 이후 신경 구조 검색(NAS) 연구의 기초가 됨.  
  - 강화학습 기반 NAS, 진화 알고리즘 개선 연구로 확장.  
- **향후 고려 사항**:  
  - *검색 효율성* 개선: 다중 GPU 병렬화, 하이브리드 NAS 기법 도입.  
  - *모듈 확장* 및 *동적 인코딩*: Inception, Attention 모듈 포함.  
  - *동시 최적화*: 구조와 가중치를 통합 학습하는 공동 최적화 알고리즘 개발.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/23df305f-9a7b-4d7a-87c8-aeacb8ad8ec7/1703.01513v1.pdf
