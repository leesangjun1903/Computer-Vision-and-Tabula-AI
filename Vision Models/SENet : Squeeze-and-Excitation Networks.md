# SENet : Squeeze-and-Excitation Networks | Image classification, Object detection

**핵심 주장**  
Squeeze-and-Excitation (SE) 블록은 CNN의 채널 간 상호의존성을 명시적으로 모델링하여, 각 채널의 표현력(representational power)을 동적으로 재조정함으로써 전체 네트워크의 성능을 크게 향상시킨다.

**주요 기여**  
1. 채널 관계 집중: 채널별 전역 통계(“Squeeze”)를 추출한 후, 이를 기반으로 채널 중요도 가중치(“Excitation”)를 학습하여 특징 맵을 재보정.  
2. 모듈식 설계: SE 블록은 ResNet, Inception, ResNeXt, MobileNet, ShuffleNet 등 다양한 기존 아키텍처에 간단히 삽입 가능한 ‘drop-in’ 유닛으로 제안.  
3. 경량성 유지: 추가 GFLOPs 증가율 0.2% 내외, 파라미터 10% 이내 증분으로도 ResNet-50 대비 top-5 오류를 7.48%→6.62%로 0.86%p 절대 개선.  
4. 범용성 검증: ImageNet 외 CIFAR-10/100, Places365, COCO 검출 등 다수 과제에서 일관된 성능 향상 확인.  
5. 대회 성과: ILSVRC 2017 분류 챌린지 1위(2.251% top-5 오류) 달성.

***

# 상세 설명

## 해결하고자 하는 문제  
기존 CNN은 공간(spatial) 축소와 채널(channel) 축소를 모두 로컬 커널에 의존해 처리하므로, 채널 간 전역적 상호작용을 학습하기 어려움. 그 결과, 어떤 채널이 얼마나 중요한지 입력에 따라 동적으로 조정할 수 없으며, 표현력에 한계가 존재.

## 제안하는 방법  
SE 블록은 두 단계로 구성된다.

1. **Squeeze (전역 정보 집약)**  
   각 채널별 특징 맵 $$U \in \mathbb{R}^{H\times W \times C}$$에 대해 전역 평균 풀링을 적용하여 채널 디스크립터 $$z \in \mathbb{R}^C$$ 생성:  

$$
     z_c = \frac{1}{H W}\sum_{i=1}^H \sum_{j=1}^W U_c(i,j).
   $$

2. **Excitation (적응적 재조정)**  
   이 디스크립터에 두 개의 FC 레이어와 비선형(ReLU + sigmoid)을 적용해 채널별 스케일 벡터 $$s \in \mathbb{R}^C$$를 학습:  

$$
     s = \sigma\big(W_2\,\delta(W_1\,z)\big),\quad W_1\in\mathbb{R}^{\frac{C}{r}\times C},~W_2\in\mathbb{R}^{C\times\frac{C}{r}},
   $$  

여기서 $$r$$은 축소 비율. 최종 출력은 채널별 곱셈으로 재조정:  
   $$\widetilde{U}_c = s_c\cdot U_c.$$

## 모델 구조  
- 각 잔차(residual) 블록의 비정체(identity) 경로 끝에 SE 블록 삽입(ResNet-50 기준 총 16개).  
- Inception, ResNeXt, MobileNet 등에도 동일 원리로 모듈 단위로 삽입.  
- 기본 축소 비율 $$r=16$$ 채택하여 복잡도-성능 균형 최적화.

## 성능 향상  
- ImageNet: ResNet-50 top-5 오류 7.48%→6.62% (−0.86%p), ResNet-101 성능에 근접(6.52%).  
- CIFAR-10/100: ResNet-110 오류 6.37%→5.21% (CIFAR-10), 26.88%→23.85% (CIFAR-100).  
- Places365: ResNet-152 top-5 오류 11.61%→11.01%.  
- COCO 검출: Faster R-CNN AP 38.0→40.4 (+2.4).  
- ILSVRC 2017 1위: 최종 top-5 오류 2.251%.

## 한계  
- 마지막 블록(stage 5)에서는 활성화가 1에 수렴해 기여 미미; 제거 시 파라미터 약 −6% 절감 가능하지만, 세밀 튜닝 필요.  
- 축소 비율(r)·삽입 위치 등 하이퍼파라미터 최적화 과제 잔존.  
- 전역 풀링 기반 단순 집약, 복잡한 공간-채널 상호작용은 반영 미흡.

***

# 일반화 성능 향상 관점

SE 블록은 **채널별 전역 맥락**을 활용해 입력별로 특징 중요도를 동적으로 재조정하므로,  
- **다양한 데이터 분포**에 보다 유연하게 대응  
- **초기 층**에서는 클래스 무관 공통 표현 강화, **후기 층**에서는 클래스 특화 표현 강화  
- 소규모 데이터셋·태스크 전이 시에도 안정적 성능 향상  
등을 통해 **일반화 능력** 전반을 향상시킨다.

***

# 향후 연구 영향 및 고려사항

**영향**  
- 채널 주의(attention) 메커니즘의 대중화: SE는 간결·효율적인 채널 주의 모듈로 이후 CBAM, ECA 등 연구의 밑거름.  
- 구조 검색(NAS)·프루닝(Pruning)에서 SE 블록 활용 가능: 채널 중요도 지도 제공으로 자동 구조 최적화 지원.

**고려사항**  
1. **공간·채널 결합 설계**: 단순 전역 평균 대신 공간적 중요도까지 반영하는 복합 집약 전략 탐색.  
2. **경량화**: 모바일·임베디드 적용 위해 SE 블록 파라미터·연산량 추가 최적화.  
3. **동적 비율(r) 탐색**: 단계별·블록별로 최적 축소 비율 자동 학습.  
4. **이론적 해석**: 왜 전역 정보가 특정 계층에 더 크게 기여하는지 분석 강화.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/31b7fbce-1508-4d4e-8829-9eeee9170551/1709.01507v4.pdf
