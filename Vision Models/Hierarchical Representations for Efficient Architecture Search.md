# Hierarchical Representations for Efficient Architecture Search | NAS

## 핵심 주장 및 주요 기여
이 논문은 **효율적인 신경망 구조 탐색**을 위해 사람 전문가의 모듈화 설계 방식을 모방한 **계층적 표현(hierarchical representation)**을 제안한다. 주요 기여는 다음과 같다.  
1. **계층적 표현 도입**: 기본 연산(primitive operations)부터 최상위 네트워크 아키텍처까지, 여러 수준의 모티프(motif)로 구성된 계층적 그래프 구조를 정의  
2. **탐색 공간의 설계 중요성 강조**: 잘 설계된 계층적 탐색 공간만으로도 단순한 **랜덤 탐색**이 경쟁력 있는 아키텍처를 찾아냄  
3. **효율적 분산 진화 알고리즘**: 토너먼트 선택 기반의 비동기적 진화(evolutionary) 방식으로 36시간에서 1시간 수준으로 탐색 시간 대폭 단축  

## 해결하고자 하는 문제
- **수작업 설계 한계**: VGG, ResNet, Inception 등 수작업 구조 설계는 탐색 공간의 조합 폭이 기하급수적으로 증가  
- **탐색 효율성**: 강화학습 기반 NAS는 우수하지만 막대한 연산 자원(수백 GPU·일)이 필요  
- **단순 탐색 기법 부진**: 랜덤 또는 진화 기반 탐색의 성능 한계  

## 제안 방법
### 1. 아키텍처 표현  
- **평면 표현(flat representation)**: 노드 간 간단한 DAG, 연산 집합 $$o$$와 인접 행렬 $$G$$로 구성  

```math
 \text{arch} = \text{assemble}(G,\,o),\quad
x_i = \mathrm{merge} \bigl\{\,o_{G_{ij}}(x_j)\bigr\}_{j < i}
```

- **계층적 표현(hierarchical representation)**:  
  - L개 레벨, 각 레벨 $$\ell$$에 $$M_\ell$$개의 모티프.  
  - 레벨1은 primitive operations $$\{o^{(1)}\}$$.  
  - 상위 모티프는 하위 모티프로 구성된 DAG $$G^{(\ell)}_m$$을 재귀적으로 조립  

$$
    o_m^{(\ell)}
      = \text{assemble}\bigl(G_m^{(\ell)},\,\{o_n^{(\ell-1)}\}\bigr),
      \quad \ell=2,\dots,L
  $$

### 2. 검색 알고리즘  
- **비동기 분산 진화**  
  - **토너먼트 선택**: 현재 집단에서 임의로 크기 5%의 서브셋을 뽑아 최고 성능 개체 선택  
  - **변이(mutation)**: 선택된 genotype의 임의 모티프, 엣지(노드 쌍), 연산을 무작위 교체  
  - **무동기 평가**: 200개 GPU 워커가 연속적으로 훈련·평가  
- **랜덤 탐색(Random Search)**  
  - 1시간 내 200개 아키텍처 동시 탐색, 간단하지만 강력한 베이스라인  

## 모델 구조
1. **Cell Search**: CIFAR-10 데이터셋에서 작은 모델(cell) 구조 탐색  
2. **확장 평가**  
   - CIFAR-10 대형 모델: $$c_0=64$$, $$N=2$$  
   - ImageNet 모델: 입력 해상도 299×299, 초기 합성곱 2단, 4개 cell 그룹  

## 성능 향상
| 탐색 기법                  | CIFAR-10 오류율 (%)        | ImageNet Top-1/Top-5 오류율 (%) |
|---------------------------|---------------------------|---------------------------------|
| 랜덤 아키텍처              | 4.56                      | 21.4 / 5.8                      |
| 랜덤 탐색 (200샘플)        | 4.02 ± 0.11               | 20.8 / 5.7                      |
| 평면 표현 진화 (7000샘플)   | 3.92 ± 0.06               | 20.6 / 5.6                      |
| 계층적 표현 랜덤 탐색 (200) | 4.04 ± 0.20               | 20.4 / 5.3                      |
| **계층적 표현 진화 (7000)** | **3.75 ± 0.12**           | **20.3 / 5.2**                  |

- CIFAR-10에서 **3.75%**(±0.12)로 현존 진화 NAS 중 최저  
- ImageNet에서는 **20.3%/5.2%**로 Inception-v3, Xception 수준으로 경쟁력 확보  

## 한계 및 고려 사항
- **최종 모델 파라미터 수**: 64M로 NASNet-A(22.6M) 대비 크며 실서비스 적용 시 자원 부담  
- **변이 기반 탐색 한계**: 탐색 초기 다양성 유지 어렵고, 지역 최적 수렴 우려  
- **평가 비용**: 비록 200 GPU로 1.5일 소요지만, 대규모 자원이 필요  

## 일반화 성능 향상 가능성
- **계층적 모듈화**가 다양한 스케일의 표현을 포착하여 학습-추론 시 **풍부한 특성 학습**  
- **스킵 연결**의 자동 탐색으로 **기울기 소실** 완화 및 **훈련 안정성** 확보  
- 단순 랜덤 탐색만으로도 우수한 일반화 성능을 보여, **탐색 공간 설계**의 중요성 입증  

## 향후 연구 영향 및 고려점
- **탐색 공간 설계**: 표현력 높은 계층적·모듈화된 탐색 공간 연구 촉진  
- **경량화 기법**: 파라미터 효율적 모티프 설계, 지식 증류 등 경량화 전략 결합 필요  
- **탐색 효율성**: 변이 전략 개선, 메타학습이나 Bayesian 최적화와의 융합  
- **일반화 평가**: 다양한 도메인(자연어, 시계열) 및 소량 데이터 환경에서의 확장성 검증  

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/f51967b8-b3e0-406c-af47-853086ed4381/1711.00436v2.pdf
