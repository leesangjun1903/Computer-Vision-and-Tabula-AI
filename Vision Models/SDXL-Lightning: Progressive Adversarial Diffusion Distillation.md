# SDXL-Lightning: Progressive Adversarial Diffusion Distillation

**핵심 주장 및 주요 기여**  
SDXL-Lightning은 SDXL 텍스트-투-이미지 모델을 1024px 해상도에서 **1~8단계**만으로도 고품질 생성이 가능하도록 하는 **점진적(adaptive)·적대적(adversarial) 증류** 기법을 제안한다. 이 방법은  
- **진행적 증류(progressive distillation)**로 원모델의 확률 흐름(flow)과 모드 커버리지를 보존하며,  
- **적대적 증류(adversarial distillation)**로 블러 현상 및 운동 오류를 완화하고 선명도를 확보한다.  
또한, UNet 인코더를 디스크리미네이터 백본으로 사용해 잠재 공간(latent space)에서 효율적이고 안정적인 학습을 가능케 하며, LoRA 모듈 형태와 전체 UNet 형태 두 가지로 모델을 공개한다.[1]

***

## 1. 문제 정의  
기존 확산 모델(diffusion models)은 50단계 이상의 반복적 샘플링이 필요해 느리고 계산량이 크다.  
- 수치적 ODE 기반 샘플러(solving ODE) 개선이나  
- 사후(distillation) 기법 적용에도 불구하고 1~2단계 생성 시 **블러**나 **Janus 인공물(겹치는 형태)**가 발생해 실용성에 한계가 있다.

***

## 2. 제안 방법  
### 2.1. 진행적 증류 (Progressive Distillation)  
교사(teacher) 모델이 $$n$$단계 이동한 위치 $$x_{t+n s}$$를 학생(student) 모델이 한 번에 예측하도록 훈련한다:  

$$
x_{t+ns} = \mathrm{move}\bigl(x_t,\;u_{t},\;t,\;ns\bigr),\quad u_t = f_{\mathrm{teacher}}(x_t,t,c)
$$  

$$
\min_f \bigl\|\,x_{t+ns} - \mathrm{move}\bigl(x_t,\,f(x_t,t,c),\,t,\,ns\bigr)\bigr\|^2
$$  

MSE만 사용할 경우 블러를 야기하므로 후술할 적대적 손실을 병용한다.[1]

### 2.2. 적대적 증류 (Adversarial Distillation)  
학생 모델 예측 $$x_{t+ns}^\mathrm{S}$$와 교사 예측 $$x_{t+ns}^\mathrm{T}$$을 구분하는 디스크리미네이터 $$D$$를 도입:  

$$
D\bigl(x_t,\;x_{t+ns},\;t,\;t+ns,\;c\bigr)\in[1]
$$  

- **비포화(adversarial) 손실**로 학생 생성 샘플을 교사 쪽으로 가깝게 유도  

$$
L_G = -\log D\bigl(x_t,\;x_{t+ns}^\mathrm{S},\;t,\;t+ns,c\bigr)
$$

$$
L_D = -\bigl[\log D(x_t,x_{t+ns}^\mathrm{T},t,t+ns,c) + \log(1-D(x_t,x_{t+ns}^\mathrm{S},t,t+ns,c))\bigr]
$$  

- **조건(conditional)→비조건(unconditional)** 순으로 두 단계 학습을 거쳐 모드 커버리지와 선명도를 모두 만족시킨다.[1]

### 2.3. 디스크리미네이터 설계  
사전학습된 SDXL UNet 인코더를 백본으로 사용하여  
1) **잠재 공간**에서 작동  
2) **모든 시점(timestep)**에서 텍스트 조건을 지원  
3) LoRA 모듈 및 컨트롤 플러그인 호환성을 유지  
4) 메모리·연산 효율성을 확보  
전체 구조는 인코더 출력을 합친 후 소형 컨볼루션 헤드로 분류기를 구성한다.[1]

***

## 3. 모델 구조 및 학습 절차  
1. 128→32단계 MSE 증류(가이드 스케일 6)  
2. 32→8→4→2→1단계:  
   a. 조건부 적대적 증류  
   b. 비조건 무자극 증류  
   c. 단계별 LoRA 학습 → 합병 후 전체 UNet 미세조정  
3. **스케줄 보정**: 학습 시 순수 노이즈(t=T)를 직접 입력해 불일치 해소  
4. **안정화 기법**:  
   - 여러 시점에서 학생·디스크리미네이터 학습  
   - 0→x₀ 예측 전환(수치 안정성 개선)  
   - 노이즈 레벨 분포 균등화  
모두 64×A100 GPU에서 대규모 LAION·COYO 데이터로 수행된다.[1]

***

## 4. 성능 향상  
- **1단계 생성**: 1024px 지원 유일 모델  
- **FID-Patch (299px 센터 패치)**: 33.52 (4단계), 35.89 (32단계 대비 개선)  
- **CLIP 점수**: 26.48 (32단계 원본 SDXL 대비 동등)  
- **정성 평가**: SDXL-Turbo, LCM 대비 레이아웃·스타일 보존 및 세부 묘사 우수.[1]

***

## 5. 일반화 성능  
- **LoRA 모듈 범용성**: 만화, 애니메이션, 리얼리스틱 등 타 베이스 모델에 그대로 적용 가능.[1]
- **비정사각(aspect ratio) 추론**: 정사각 학습에도 3:2, 2:1 종횡비에 대해 양호한 결과 보임(1~2단계에서 일부 열화)​.[1]
- **잠재 공간 판별**: 픽셀 공간이 아닌 잠재 공간에서 학습하므로, 다양한 도메인(영상·음성 등) 확장성 우수​.[1]

***

## 6. 한계 및 고려 사항  
- **체크포인트 분리**: 단계별 별도 모델 제공(LoRA로 완화 가능)  
- **UNet 구조 제약**: 디코더에 대부분 생성 기능이 몰려 있어 1단계 최적화에는 한계  
- **다중 종횡비 증류 미지원**: 향후 개선 필요.[1]

***

## 7. 향후 연구에 미치는 영향 및 고려 사항  
SDXL-Lightning은 **초저단계** 확산 모델 가속화의 새로운 기준을 제시한다.  
- **모델 경량화·추론 속도** 최적화 연구 촉진  
- **잠재 공간 적대적 학습**을 타 도메인(3D, 동영상, 의료영상)으로 확장  
- UNet 이외 구조 탐색을 통한 **한계 극복**  
- **다중 종횡비·상세 편집** 지원 위한 증류 스케줄·데이터 다양화  
를 고려하면, 실시간 생성, 모바일 배포, 멀티모달 AI 등에 큰 파급력을 가질 것으로 예상된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/6a5ed98f-9ba4-444f-af53-50278bb671db/2402.13929v3.pdf)
