# Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models | 2023 · 155회 인용, Image generation

**주요 요약**  
확산 모델(DMs)의 풍부한 분포 정보를 데이터 없이도 다른 생성 모델(예: GAN, NeRF)에 전이하는 **Diff-Instruct**를 제안한다. 핵심 아이디어는 확산 과정 전반에 걸친 KL 발산을 통합한 **Integral KL(IKL) 발산**을 최소화하여 사전 학습된 DMs의 멀티 레벨 스코어 함수를 활용하는 것이다.[1]

***

## 1. 문제 정의 및 기여  
Diff-Instruct는 다음 두 가지 핵심 문제를 해결한다:[1]
-  Q1: 대규모 데이터 없이 사전 학습된 DMs로부터 다른 생성 모델(implicit generative models)으로 지식을 전이할 수 있는가?  
-  Q2: 생성 모델 구조나 입력·출력 차원이 달라도 범용적으로 적용 가능한가?  

이를 위해 아래와 같은 주요 기여를 제시한다:[1]
1. **IKL 발산 정의**: 확산 과정 $$t\in[0,T]$$ 전반의 KL 발산을 적분하여  

$$ D_{\mathrm{IKL}}(q,p) = \int_0^T w(t)\,D_{\mathrm{KL}}(q_t \,\|\, p_t)\,dt, $$  
   
   분포 서포트가 불일치해도 유한하며 그래디언트 계산에 멀티 레벨 스코어 함수만 필요하도록 설계.[1]

2. **Diff-Instruct 알고리즘**: (1) 생성 모델의 샘플로부터 보조 확산 모델 학습, (2) IKL 그래디언트를 활용한 생성기 파라미터 업데이트를 교대로 수행.[1]
3. **범용성**: single-step 확산 모델(distillation)과 GAN 개선 두 시나리오 모두에서 우수한 성능 입증.[1]

***

## 2. 제안 방법 상세  

### 2.1 Integral KL(IKL) 발산  
IKL 발산은 확산 과정의 각 시점 $$t$$ 에서의 KL 발산을 가중 적분한 것이다:[1]

$$
D_{\mathrm{IKL}}(q,p)
= \int_{0}^{T} w(t)\,\mathrm{KL}(q_t \,\|\, p_t)\,dt
= \int_{0}^{T} w(t)\,\mathbb{E}_{x\sim q_t}\!\bigl[\log q_t(x)-\log p_t(x)\bigr]\,dt.
$$  

여기서 $$q_t$$, $$p_t$$은 각각 생성 모델과 사전 학습 DMs의 확산 과정 분포이며, $$w(t)$$는 적절한 가중 함수이다.[1]

### 2.2 그래디언트 및 알고리즘  
Diff-Instruct는 IKL 발산의 그래디언트를 다음과 같이 유도하여 생성기 파라미터 $$\theta$$를 업데이트한다:[1]

$$
\nabla_\theta D_{\mathrm{IKL}}(q,p)
= \int_{0}^{T} w(t)\,\mathbb{E}_{z\sim p_z,\:\epsilon}\Bigl[s_q(x_t,t;\theta)-s_p(x_t,t)\Bigr]\;\nabla_\theta g(z;\theta)\,dt,
$$  

여기서 $$s_p$$·$$s_q$$는 각각 사전 학습·보조 확산 모델의 스코어 함수, $$g(z;\theta)$$는 생성기이다.  

**Diff-Instruct 절차**  
1. 생성 모델 샘플 $$\{x_0=g(z)\}$$로 보조 확산 모델 $$s_q$$ 학습 (Denoising Score Matching).  
2. IKL 그래디언트를 계산해 생성기 $$\theta$$ 업데이트.  
3. 수렴 시 $$s_q\approx s_p$$, 그래디언트 ≈ 0.  

### 2.3 모델 구조  
- **교사 모델**: 사전 학습된 U-Net 기반 DMs (예: EDM).  
- **학생 모델**:  
  - **확산 증류** 시나리오: 동일 구조 U-Net에 단일 스텝 생성기 사용.  
  - **GAN 개선** 시나리오: StyleGAN-2 등 임의의 GAN 생성기.  
- **보조 확산 모델**: 학생 모델로부터 생성된 샘플을 학습하는 U-Net.

***

## 3. 성능 개선 및 한계

### 3.1 확산 증류(Single-step)  
- CIFAR-10(64×64) 무조건 생성: FID 4.53, IS 9.89 (NFE=1)  
- ImageNet(64×64) 조건부 생성: FID 5.57 (NFE=1)  
– 대부분 단일/다중 스텝 증류 기법 대비 경쟁력 또는 우위 달성.[1]

### 3.2 GAN 생성기 개선  
- StyleGAN-2(CIFAR-10, ADA):  
  -  조건부 FID: 2.42 → 2.27  
  -  무조건 FID: 2.92 → 2.71  
– 사전 학습 DMs 지식을 주입하여 기존 GAN 성능 지속적 향상.[1]

### 3.3 한계  
- **다중 교사 모델 활용 미탐색**: 복수 확산 모델로부터 동시 전이 가능성 남음.  
- **실제 데이터 활용 미최적화**: 순수 데이터 무료 방식이나, 일부 실제 데이터 병용 효과 분석 필요.  
- **간접 학습 시나리오**: 데이터만 있을 때 확산 모델을 교사로 동시 학습시키는 효율성 검증 필요.[1]

***

## 4. 일반화 성능 관점  
Diff-Instruct는 IKL을 통해 확산 과정 전반의 정보(멀티 레벨 스코어)를 활용하고, 생성 모델이 보지 못한 데이터 포인트에도 안정적 그래디언트를 제공한다. 이로 인해 종래의 KL이나 JS 발산 기반 adversarial training에서 발생하는 서포트 불일치 및 모드 붕괴 문제를 완화하며, 다양한 생성기 구조에 **데이터 무료**로 지식을 전이함으로써 **일반화 가능성**을 크게 향상시킨다.[1]

***

## 5. 향후 연구 영향 및 고려 사항  
Diff-Instruct는 다음 연구 방향에 영향을 미칠 것으로 기대된다:  
- **멀티 교사 및 앙상블 전이**: 다양한 확산 모델 지식을 통합해 성능·다양성 극대화.  
- **하이브리드 학습**: 실제 데이터와 IKL 기반 지시를 결합해 더욱 견고한 생성기 학습.  
- **비확산 생성 모델에의 확대**: 변분 오토인코더, Normalizing Flows 등에도 확장 가능성 탐색.  

연구 시 고려 사항:  
1. 가중 함수 $$w(t)$$ 및 적분 구간 선택이 성능에 미치는 영향.  
2. 대규모 고해상도 데이터셋에서의 메모리·연산 효율성 최적화.  
3. 보조 확산 모델 학습 안정성 및 수렴 속도 개선.  

이처럼 Diff-Instruct는 데이터 부재 상황에서도 확산 모델의 지식을 폭넓게 활용하며, 생성 모델의 **일반화 성능**과 **유연성**을 한층 강화할 수 있는 유망한 범용 프레임워크이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/746f6fcd-592b-4359-bb55-a51bbf31bf5a/2305.18455v2.pdf)
