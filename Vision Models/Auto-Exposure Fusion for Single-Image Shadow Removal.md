# Auto-Exposure Fusion for Single-Image Shadow Removal | Shadow removal

## 1. 핵심 주장 및 주요 기여  
**핵심 주장**  
이 논문은 단일 이미지 그림자 제거를 **자동 노출 퓨전(auto-exposure fusion)** 문제로 재정의하고, 그림자 영역의 색상과 조명을 비그림자 영역과 일치시키기 위해 서로 다른 노출(오버익스포저) 이미지를 생성·융합함으로써 고품질의 그림자 제거 결과를 얻을 수 있음을 보인다.  

**주요 기여**  
- 그림자 제거를 위해 **첫번째로** 입력 그림자 이미지에 대해 서로 다른 노출 보정 파라미터(α, β)를 학습하여 다수의 오버익스포저 이미지를 생성.  
- **두번째로** 픽셀별로 “스마트”하게 적절한 노출 이미지를 선택·융합하는 **Shadow-Aware FusionNet** 제안.  
- **세번째로** 그림자 경계 부근의 페넘브라(penumbra) 잔여 흔적을 정교하게 제거하는 **Boundary-Aware RefineNet** 도입.  
- 세 가지 공개 데이터셋(ISTD, ISTD+, SRD)에서 그림자 영역 RMSE를 기존 기법 대비 최대 **20% 이상** 감소시키며 최고 성능 달성.  

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 문제 정의  
그림자 제거는 배경 의존적(background-dependent)이며 공간적으로 비균일(spatial-variant)한 조명 왜곡을 복원해야 한다. 특히 경계 부근의 페넘브라 영역은 잔여 흔적이 남기 쉬워 고품질 제거가 어렵다.

### 제안 방법

1. **오버익스포저 시퀀스 생성**  
   - 식 (3): 각 오버익스포저 이미지 $$I^o_i$$는  

$$
       I^o_i = \alpha_i\,I^s + \beta_i,\quad i=1,\dots,N
     $$
     
  여기서 $$I^s$$는 입력 그림자 이미지, $$\alpha_i, \beta_i\in\mathbb{R}^{3\times1}$$는 채널별 노출 조정 파라미터.  
   
   - 식 (4)-(5): 중앙 노출 파라미터 $$(\alpha_{N/2},\beta_{N/2})$$를 DNN으로 추정한 뒤, 선형 보간 계수 $$\{\gamma_i\}$$를 통해 나머지 $$(\alpha_i,\beta_i)$$를 생성.  

2. **Shadow-Aware FusionNet**  
   - 픽셀별 융합을 식 (7)처럼 이웃 픽셀 정보까지 고려하여 수행:  

$$
       \hat I[p] = \sum_{i=0}^N (K_i \ast I^o_i)[p]
               = \sum_{i=0}^N \sum_{q\in\mathcal N(p)} k^i_p[p-q]I^o_i[q].
     $$
   
   - ShadowNet은 입력 $$I^s$$, 그림자 마스크 $$M^s$$, 오버익스포저 이미지들 $$\{I^o_i\}$$를 받아 픽셀별 합성 커널 $$K=\{K_i\}$$를 출력.  
   - 손실은 L₁ 픽셀 오차:  

$$
       L_{\mathrm{pix}}=\|\hat I - I^*\|_1.
     $$

3. **Boundary-Aware RefineNet**  
   - 초기 결과 $$\hat I$$와 입력 $$I^s$$, 그림자 마스크 $$M^s$$, 페넘브라 마스크 $$M^b$$로부터 픽셀별 정제 커널 $$F$$를 추정하여  
     $$\hat I_r[p]=\sum_{q\in\mathcal N(p)} f_p[p-q]\hat I[q]$$.  
   - 경계 일관성을 위한 경계 손실(식 12):  

$$
       L_{bd} = \mathrm{MSE}(\nabla\hat I_r,\nabla I^s)\cdot(1-M^s)
               +\mathrm{MSE}(\nabla\hat I_r,\nabla I^*)\cdot M^s.
     $$
   
   - 총 손실: $$L_{\mathrm{pix}} + \lambda L_{bd}$$, $$\lambda=0.1$$.

### 모델 구조  
- **Exposure Estimation**: ResNeXt 기반, 그림자 이미지 + 마스크 → 중앙 노출 파라미터 추정.  
- **FusionNet & RefineNet**: U-Net256 백본, 픽셀별 커널 예측.  
- 오버익스포저 수 $$N=5$$, 입력 해상도 256×256, Adam 옵티마이저, 배치 크기 8, 에폭 400.

### 성능 향상  
- ISTD 데이터셋에서 그림자 영역 RMSE를 9.76→7.77, 전체 RMSE 6.67→5.92로 감소.  
- ISTD+ 데이터셋에서 그림자 RMSE 7.9→6.5(17.7%↓), 전체 3.9→4.2(비교 우위).  
- SRD 데이터셋에서도 그림자 RMSE 8.94→8.56으로 개선.  
- 페넘브라 영역 RMSE 7.06→5.96로 핵심 경계 부근 성능 크게 개선.  

### 한계  
- **계산 비용**: 픽셀별 커널 융합과 다중 노출 처리를 위한 메모리·연산 부하가 높음.  
- **실제 조명 변화**: 실세계 복잡한 조명(다중 광원, 반사) 사례에서 일반화 여부 불확실.  
- **노출 파라미터 보간 가정**: 선형 보간이 복잡한 배경·카메라 반응에 충분하지 않을 수 있음.

## 3. 모델 일반화 성능 향상 관점  
- **다중 노출 다양성**: $$N=5$$를 넘어 더 많은 노출 레벨을 사용하면 그림자 패턴 다양성 대응력 향상 가능.  
- **노출 파라미터 추정**: 중앙 노출만 추정하는 대신, 각 노출별 독립 추정 네트워크로 확장 시 더욱 세밀한 조정 가능.  
- **커널 예측의 공간적 확장성**: U-Net 대신 Transformer 기반 아키텍처를 도입하여 전역 문맥을 반영하면 이질적 조명 조건 일반화에 유리.  
- **Self-supervision 활용**: 페넘브라 경계 정보를 활용한 자체 지도 학습을 통해 마스크 품질 의존도 감소 및 미지 그림자 영역 일반화 강화.  
- **데이터 다양성**: 다양한 조명·재질·장치 조건에서 촬영된 방대한 실제 데이터로 학습 시 배경 의존성 완화 가능.

## 4. 향후 연구에 미치는 영향 및 고려 사항  
이 논문은 **그림자 제거**와 **다중 노출 융합** 분야 간 융합적 접근을 제시함으로써,  
- **비전 전처리**: 객체 인식·세그멘테이션 등 downstream 태스크에서 조명 변화 견고성 향상.  
- **고품질 영상 편집**: 자동 노출 보정, 고다이내믹레인지(HDR) 합성 등에 적용 가능.  
- **후속 과제**: 실시간 영상(비디오) 그림자 제거, 동적 조명 추적, 광원 추정 기반 물리적 모델 통합 등이 주요 연구 방향으로 떠오름.  
연구 시에는 **연산 효율화**, **글로벌 조명 모델링**, **셀프-슈퍼비전** 및 **다양한 현실 데이터 확보**를 중점적으로 고려해야 한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/013fab3f-9c79-4114-87a5-ac8c0bbd353f/2103.01255v2.pdf)
