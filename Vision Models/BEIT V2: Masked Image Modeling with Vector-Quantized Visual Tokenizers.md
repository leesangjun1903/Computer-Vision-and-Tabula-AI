# BEIT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers | 미세 조정(fine-tuning), 선형 탐색(linear probing), 의미론적 분할(semantic segmentation)

## 1. 핵심 주장과 주요 기여

BEIT V2는 기존 Masked Image Modeling(MIM) 방법들이 저수준 픽셀 단위에서 작동하는 한계를 극복하고자, **시맨틱 수준의 Visual Tokenizer**를 도입하여 MIM을 픽셀 수준에서 의미 수준으로 승격시킨 획기적인 연구입니다.[1]

**주요 기여:**
- **Vector-Quantized Knowledge Distillation (VQ-KD)** 알고리즘을 통한 의미론적 토크나이저 훈련[1]
- **Patch Aggregation 전략**으로 전역 시맨틱 표현 강화[1]
- ImageNet-1K에서 base-size 모델 85.5%, large-size 모델 87.3%의 최고 성능 달성[1]

## 2. 해결하고자 하는 문제와 제안 방법

### 문제 정의
기존 MIM 방법들은 세 가지 재구성 대상 중 하나를 사용했습니다:
1. 저수준 이미지 요소 (원시 픽셀)
2. 수작업 특징 (HOG 특징)
3. 시각적 토큰

하지만 이들은 모두 **고수준 의미론을 충분히 활용하지 못했습니다**.[1]

### 제안 방법

#### Vector-Quantized Knowledge Distillation (VQ-KD)

VQ-KD의 훈련 목표는 다음 수식으로 정의됩니다:[1]

$$
\max_{\theta, \Phi, D} \sum_{x \in D} \sum_{i=1}^{N} \cos(o_i, t_i) - \|sg[\ell_2(h_i)] - \ell_2(v_{z_i})\|_2 - \|\ell_2(h_i) - sg[\ell_2(v_{z_i})]\|_2
$$

여기서:
- $$o_i$$: 디코더 출력 벡터
- $$t_i$$: 교사 모델의 특징 벡터
- $$h_i$$: 인코더 출력
- $$v_{z_i}$$: 코드북 임베딩
- $$sg[\cdot]$$: 정지 기울기 연산자

**양자화 과정:**

각 이미지 패치에 대한 양자화된 코드는 다음과 같이 계산됩니다:[1]

$$
z_i = \arg\min_j \|\ell_2(h_i) - \ell_2(v_j)\|_2
$$

여기서 $$j \in \{1, 2, \ldots, K\}$$이고, ℓ2 정규화가 사용됩니다.

#### Patch Aggregation 전략

패치 수준 사전훈련과 이미지 수준 표현 집계 간의 불일치를 완화하기 위해, **정보 흐름 병목**을 구성합니다:[1]

1. L-layer Transformer에서 중간 l-layer의 패치 벡터들과 마지막 layer의 [CLS] 토큰을 연결
2. 얕은 Transformer 디코더에 입력하여 추가적인 마스크 예측 수행
3. 최종 손실은 두 항의 합: $$L_{MIM} + L'_{MIM}$$

## 3. 모델 구조

### 전체 아키텍처

**1단계: VQ-KD 토크나이저 훈련**
- 인코더: ViT-B/16
- 디코더: 3-layer Transformer
- 교사 모델: CLIP-B/16
- 코드북 크기: 8192 × 32[1]

**2단계: BEIT V2 사전훈련**
- 백본: ViT-B/16 또는 ViT-L/16
- 마스킹 비율: 40%
- Patch Aggregation: l=9 (Base), l=21 (Large)[1]

### 핵심 구성 요소

1. **시맨틱 비주얼 토크나이저**: 연속적 의미 공간을 압축 코드로 이산화
2. **패치 집계 메커니즘**: [CLS] 토큰이 모든 패치 정보를 집계하도록 유도
3. **교사 모델 지도**: CLIP 또는 DINO의 의미론적 특징 재구성

## 4. 성능 향상 및 일반화 능력

### 성능 향상

**ImageNet-1K 결과:**
- Base 모델 (300 epochs): 85.0% → BEIT 대비 2.1% 향상[1]
- Base 모델 (1600 epochs): 85.5%
- Large 모델 (1600 epochs): 87.3%[1]

**Linear Probing에서의 뛰어난 성능:**
- BEIT V2: 80.1%
- 기존 최고 성능 (MoCo v3): 76.7%
- **23.4%의 큰 격차로 BEIT 초월**[1]

### 일반화 성능 향상

**강건성 평가에서 탁월한 성능:**

| 데이터셋 | MAE (ViT-B/16) | BEIT V2 (ViT-B/16) | 개선 폭 |
|---------|---------------|-------------------|--------|
| ImageNet-Adversarial | 35.9% | 54.4% | +18.5% |
| ImageNet-Rendition | 48.3% | 61.0% | +12.7% |
| ImageNet-Sketch | 34.5% | 45.6% | +11.1% |

Large 모델에서는 더욱 뛰어난 일반화 성능을 보입니다:[1]
- ImageNet-Adversarial: 69.0% (MAE 57.1% 대비 +11.9%)
- ImageNet-Rendition: 69.9% (MAE 59.9% 대비 +10.0%)

**일반화 향상의 핵심 요인:**

1. **의미론적 코드북**: 색상, 조명, 회전, 스케일 같은 이미지 세부사항을 무시하고 명시적 의미론만 포착[1]
2. **차원의 저주 완화**: 이산적 의미 공간의 차원이 원래 연속 특징 공간보다 현저히 낮음[1]
3. **전역 표현 강화**: Patch Aggregation으로 이미지 수준 표현 능력 향상[1]

## 5. 한계점

논문에서 명시적으로 언급된 주요 한계점들:

1. **코드북 붕괴 문제**: 소수의 코드만 사용되는 현상이 여전히 발생 가능[1]
2. **깊은 디코더의 역효과**: VQ-KD에서 더 깊은 디코더가 더 나은 재구성을 얻지만, 코드북 사용률과 다운스트림 성능은 오히려 저하[1]
3. **단일 교사 모델 의존**: base-size와 large-size 모두에 동일한 base-size 교사 모델 사용[1]

## 5. 향후 연구에 미치는 영향과 고려사항

### 연구에 미치는 영향

**1. MIM 패러다임의 전환**
- 픽셀 수준에서 의미 수준으로의 패러다임 전환 제시
- 시각적 토크나이저의 중요성 부각
- 지식 증류와 벡터 양자화의 결합 방법론 확립

**2. 자기지도학습 발전**
- Linear Probing에서의 큰 성능 향상으로 표현 학습의 새로운 가능성 제시
- 전역 표현과 지역 표현의 균형 있는 학습 방법 제안

**3. 멀티모달 학습으로의 확장**
논문에서 향후 연구 방향으로 제시한 **범용 토크나이저**:[1]
> "단어와 이미지를 동일한 어휘로 투영하는 범용 토크나이저를 학습하여 비전-언어 사전훈련을 위한 마스크 예측을 수행하고자 합니다."

### 향후 연구 시 고려사항

**1. 코드북 효율성 개선**
- 코드북 활용도를 높이는 새로운 양자화 전략 필요
- 동적 코드북 크기 조정 방법 연구

**2. 다양한 교사 모델 활용**
- 모델 크기별 맞춤형 교사 모델 사용
- 여러 교사 모델의 앙상블 활용 방안

**3. 계산 효율성**
- VQ-KD 훈련과 BEIT 사전훈련의 이중 단계로 인한 계산 비용 증가
- 종단간 훈련 방법 모색

**4. 도메인 적응성**
- 의료, 위성 이미지 등 특수 도메인에서의 적용 가능성
- 도메인 특화 코드북 설계 방법

이 연구는 자기지도학습에서 의미론적 표현의 중요성을 입증하고, 향후 멀티모달 AI 발전의 기반을 제공하는 중요한 이정표가 될 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/85de496c-476e-446f-ae7b-65cbba170778/2208.06366v2.pdf)
