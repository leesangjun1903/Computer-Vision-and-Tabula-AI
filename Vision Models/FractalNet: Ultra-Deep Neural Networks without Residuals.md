# FractalNet: Ultra-Deep Neural Networks without Residuals | Image classification

## 핵심 주장과 주요 기여

FractalNet은 2017년 ICLR에서 발표된 논문으로, **잔차 연결(residual connections) 없이도 초심층 신경망을 성공적으로 훈련할 수 있다**는 혁신적인 주장을 제시합니다. 이 연구의 핵심은 **자기 유사성(self-similarity)에 기반한 프랙탈 구조**를 통해 ResNet과 동등한 성능을 달성하면서도, 잔차 학습이 극심층 네트워크의 성공에 필수적이지 않음을 증명한 것입니다.[1]

**주요 기여:**
- **FractalNet 도입**: ResNet의 첫 번째 단순한 대안으로, 명시적인 잔차 학습 없이도 초심층 신경망 구축이 가능함을 실증[1]
- **연결성 분석**: FractalNet과 기존 심층 네트워크 설계의 다양한 현상들 간의 연관성을 분석하고 설명[1]

## 해결하고자 하는 문제

### 기존 문제점
기존의 심층 신경망 훈련에서는 **기울기 소실(vanishing gradient) 문제**와 **깊이 증가에 따른 성능 저하** 문제가 존재했습니다. ResNet은 이를 잔차 연결을 통해 해결했지만, 저자들은 잔차 연결이 정말 필수적인지에 대한 의문을 제기했습니다.[1]

### 핵심 가설
연구진은 **경로 길이(path length)가 잔차 표현보다 더 중요**하며, 훈련 중 효과적으로 얕은 네트워크에서 깊은 네트워크로 전환할 수 있는 능력이 핵심이라고 가정했습니다.[1]

## 제안하는 방법 및 수식

### 수학적 정의

**기본 케이스:**

$$ f_1(z) = \text{conv}(z) $$

**재귀적 확장 규칙:**

$$ f_{C+1}(z) = [(f_C \circ f_C)(z)] \oplus [\text{conv}(z)] $$

여기서:
- $$\circ $$: 합성 연산(composition)
- $$\oplus $$: 조인 연산(element-wise mean)
- $$C $$: 컬럼 수 (네트워크 폭)
- 깊이는 $$2^{C-1} $$로 스케일링
- $$B $$개 블록 스택 시 총 깊이: $$B \times 2^{C-1} $$

### Drop-path 정규화

**두 가지 샘플링 전략:**

1. **Local 샘플링**: 각 조인에서 고정 확률로 입력을 드롭하되, 최소 하나는 보존
2. **Global 샘플링**: 전체 네트워크에서 단일 경로(단일 컬럼)만 선택

**훈련 프로토콜:**
- Local 50% + Global 50% 혼합 샘플링
- 각 미니배치마다 새로운 서브네트워크 샘플링
- 병렬 경로의 공동 적응(co-adaptation) 방지[1]

## 모델 구조

### 프랙탈 설계 원리
- **자기 유사성**: 간단한 확장 규칙의 반복 적용으로 생성
- **잔차 연결 없음**: 모든 내부 신호가 필터와 비선형성으로 변환
- **다양한 길이의 상호작용 서브경로** 포함[1]

### 구조적 특징
- $$C $$개 컬럼이 네트워크 폭에 대응
- 조인 레이어는 입력들의 원소별 평균 계산
- 풀링과 함께 $$B $$번 스택하여 총 깊이 결정
- ResNet과 달리 **특권적인 패스스루 신호 없음**[1]

## 성능 향상

### 실험 결과

**CIFAR-100 (데이터 증강 없음):**
- ResNet 기준: 44.76% 오류율
- FractalNet (정규화 없음): 35.34% 오류율
- FractalNet + Drop-path: 28.20% 오류율
- **16.56% 개선** 달성[1]

**CIFAR-10:**
- ResNet 기준: 13.63% 오류율
- FractalNet + Drop-path: 7.33% 오류율  
- **6.30% 개선** 달성[1]

**ImageNet:**
- ResNet-34와 동등한 성능 (24.12% vs 24.19% Top-1 오류율)[1]

### 일반화 성능 향상 메커니즘

1. **암시적 깊은 지도학습**: 프랙탈 구조가 자동으로 깊은 지도학습 효과 생성
2. **학생-교사 학습**: 다양한 깊이의 컬럼 간 양방향 정보 흐름
3. **Drop-path 정규화**: 매크로 스케일 정규화로 과적합 크게 감소
4. **Anytime 속성**: 얕은 서브네트워크는 빠른 답을, 깊은 서브네트워크는 정확한 답을 제공
5. **깊이 저항성**: 네트워크가 너무 깊어져도 성능 저하 없음[1]

## 한계

### 확인된 제약사항

1. **계산 복잡성**: 
   - 지수적 깊이 스케일링($$2^{C-1}$$)으로 매우 깊은 네트워크 생성
   - 프랙탈 복잡도에 따른 메모리 요구량 증가[1]

2. **구조적 제약**:
   - 조인 연산이 원소별 평균으로 제한
   - 고정된 프랙탈 구조로 모든 작업에 최적이 아닐 수 있음[1]

3. **이론적 이해 부족**:
   - 프랙탈 구조의 효과에 대한 완전한 이론적 설명 부재
   - 학생-교사 행동의 특성화 미완료[1]

## 미래 연구에의 영향

### 연구 방향성 변화

1. **잔차 학습의 대안**:
   - 경로 길이가 잔차 연결보다 중요함을 입증
   - 초심층 네트워크 설계의 새로운 방향 제시
   - 스킵 연결이 필수라는 가정에 도전[1]

2. **정규화 전략 발전**:
   - Drop-path를 다른 아키텍처에 적용 가능
   - 매크로 스케일 정규화 개념의 전이성
   - Anytime 컴퓨팅의 접근성 향상[1]

### 향후 고려사항

1. **아키텍처 탐색**: 프랙탈 원리를 신경망 구조 탐색에 활용하여 자기 유사성을 설계 제약으로 사용

2. **훈련 동역학 연구**: 단일 네트워크 내 학생-교사 학습과 훈련 중 점진적 깊이 증가에 대한 더 깊은 이해 필요

3. **이론적 기반 강화**: 프랙탈 구조의 효과와 최적화 경관과의 연결에 대한 이론적 분석 필요

4. **실용적 응용**: Drop-path 정규화와 Anytime 속성을 실제 애플리케이션에서 활용하는 방법 연구

FractalNet은 깊은 네트워크의 훈련 메커니즘에 대한 근본적인 이해를 재정립하고, 잔차 연결 없이도 효과적인 초심층 네트워크 구축이 가능함을 보여준 혁신적 연구입니다.[1]

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/91ff9700-6163-4d69-880f-0f8092d3d43d/1605.07648v4.pdf
