# Regularized Evolution for Image Classifier Architecture Search | NAS

## 1. 핵심 주장 및 주요 기여  
이 논문은 **신경망 구조 검색(Neural Architecture Search, NAS)** 분야에서 진화 알고리즘이 강화학습 기반 NAS에 비해 단순하면서도 경쟁력 있는 결과를 낼 수 있음을 처음으로 입증했다.  
- **Aging Evolution(정규화된 진화)**: 표준 토너먼트 선택 알고리즘에 “나이(age)” 개념을 도입해, 개체가 오래될수록 제거될 확률이 높도록 함으로써 과도한 조기 수렴을 방지하고 탐색 다양성을 유지.  
- **간단한 돌연변이(mutation) 설계**: NASNet 검색 공간에서 동작하는 최소한의 두 가지 돌연변이(은닉 상태 변경 및 연산 변경)를 정의하여, 전체 검색 공간을 효율적으로 탐색.  
- **경량·고효율성 비교 실험**: CIFAR-10에서 강화학습(RL) 및 무작위 탐색(Random Search, RS)과 동일 조건의 대규모 실험(20K 모델 평가) 수행.  
- **AmoebaNet-A 제안**: 진화로 얻은 모델이 동일한 크기의 NASNet-A 대비 CIFAR-10 및 ImageNet 정확도를 근소하게 상회하거나 모델 크기를 줄이면서 동일 성능을 달성.

## 2. 해결하려는 문제  
- **수작업 구조 설계 비용**: 인간이 이미지 분류기 아키텍처를 일일이 설계·튜닝하던 방식을 자동화하고, 기존 RL 기반 NAS보다 단순·효율적인 대안 제시.  
- **과도한 자원 소모**: 대규모 GPU/TPU 자원을 장기간 사용하는 RL 방식의 부담을 줄이고, 적은 예산·시간에도 유의미한 성능을 얻고자 함.

## 3. 제안하는 방법  
### 3.1 Aging Evolution 알고리즘  
- P개의 모델을 `population` 큐에 유지.  
- 매 사이클마다 S개의 후보를 랜덤 샘플링하여 **가장 높은 정확도** 모델(parent)을 선택.  
- parent에 돌연변이(mutation)를 적용해 child 생성, 훈련·평가 후 큐 오른쪽에 추가.  
- 큐 맨 왼쪽(가장 오래된 모델)을 제거 → **나이가 든 모델 우선 제거**.  
- 반복 후 `history` 내 최고 성능 모델 반환.

### 3.2 돌연변이 연산  
- **은닉 상태 변경(hidden state mutation)**: 임의의 셀(normal/reduction)·조합(pairwise)·요소(element) 선택 후, 피드포워드 순환을 피하며 은닉 상태를 다른 상태로 변경.  
- **연산 변경(op mutation)**: 동일한 방식으로 연산 종류(3×3 sep conv, 5×5 sep conv, avg pool 등) 교체.  
- **항등 변이(identity mutation)**: 변경 없이 후손 생성(확률 0.05).

### 3.3 수식  
- **돌연변이 선택 확률**: 각 변이(은닉 상태·연산·항등)는 균등 분포 또는 고정 확률(항등 0.05)로 선택.  
- **토너먼트 선택**: $\text{parent} = \text{argmax} m ∈ sample accuracy(m) $ 

## 4. 모델 구조  
- **NASNet 검색 공간**:  
  - 두 입력(hidden states 0,1)→5회 pairwise 조합→출력(concat)  
  - **Normal cell**과 **Reduction cell** 각각 5조합  
  - 최종 모델은 N×Normal + Reduction 스택으로 구성, 필터 수 F로 크기 조절  

- **AmoebaNet-A**  
  - Normal cell과 Reduction cell이 모두 진화된 구조  
  - 예: Normal cell 내 sep-3×3, avg-3×3, max-3×3, sep-5×5 조합 등  
  - N=6, F=190/448 규모에서 ImageNet top-1 82.8%→83.9%로 최상위 성능  

## 5. 성능 향상 및 비교  
| 방식        | CIFAR-10 Top-1 정확도 | ImageNet Top-1/Top-5 정확도 | FLOPs (G) / Params (M) |
|------------|----------------------|------------------------------|------------------------|
| NASNet-A⁽hand⁾ | 96.1%               | 82.7% / 96.2%               | 23.8B / 88.9M         |
| AmoebaNet-A  | 96.1%               | 82.8% / 96.1%               | 23.1B / 86.7M         |
| AmoebaNet-A  | –                   | **83.9% / 96.6%**           | 104B / 469M           |

- **리소스 제약 하 조기 탐색**: 20K 모델 평가 이전 단계(초기 5K 모델)에서 진화가 RL 대비 더 빠르게 성능 향상[Figure 3].  
- **모델 연산량 절감**: 동일 정확도 기준으로 FLOPs/RAM 절감[Figure 4].  

## 6. 한계 및 고려할 점  
- **검색 공간·데이터셋 한정**: NASNet 공간, CIFAR-10/ImageNet으로 제한. 다른 검색 공간·작업·데이터셋에 일반화 필요.  
- **노이즈 민감도**: 평가 잡음이 큰 경우 탐색 효율이 달라질 수 있어, 노이즈 완화 전략(regularization) 추가 연구 필요.  
- **메타파라미터 튜닝**: Population P, sample S 설정에 따라 결과 차이 발생. 적절한 기본값 제시에도 다양한 조건에서 재시험 권장.  

## 7. 일반화 성능 향상 가능성  
- **정규화 효과**: Aging Evolution은 우연히 높은 정확도를 얻은 모델을 오래 유지하지 못하게 해, 재훈련 반복을 통해 “재현 가능한” 구조만 선정. 이는 과적합 방지 및 검증세트 일반화 성능 향상으로 이어질 수 있음.  
- **탐색 다양성 유지**: 노후 개체 제거로 탐색 공간 전역을 광범위하게 커버, 지역 최적해에 매몰될 위험 완화.  

## 8. 향후 연구 방향 및 고려 사항  
- **검색 공간 확장**: SP-II, SP-III 등 더 큰 설계 공간에서도 AE 성능 검증 및 개선.  
- **다른 도메인 적용**: 객체 검출, 분할, 자연어 처리 등 다양한 과제에 AE 적용성 조사.  
- **이론적 분석**: Aging이 regularization으로 기능하는 메커니즘 분석 및 수학적 모델 제시.  
- **자동 메타파라미터 최적화**: 진화 과정 중 동적으로 P/S 조정, 자원 제약 시 적응적 탐색 전략 개발.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/cf7163fc-1fcb-4b28-ab3a-213b656c000f/1802.01548v7.pdf
