# GoogLeNet : Going Deeper with Convolutions | Image classification

**핵심 주장 및 주요 기여**  
“Going Deeper with Convolutions”에서는 연산 예산을 크게 늘리지 않으면서 네트워크의 폭(width)과 깊이(depth)를 동시에 확장할 수 있는 새로운 합성곱 신경망 구조인 **Inception 모듈**을 제안한다. 이를 통해 22층 깊이의 GoogLeNet를 구성하여 ILSVRC14 분류 및 탐지 대회에서 당시 최적 성능을 기록했다.  

## 1. 해결하고자 하는 문제  
기존 CNN은 모델 크기를 단순히 깊이 또는 폭을 확장하면  
- 파라미터 수 급증에 따른 **과적합**  
- 연산량의 **이차적 증가**  
라는 한계를 맞는다.  
Szegedy et al.는 **필터 레벨의 희소성(sparsity)을 조밀한 연산으로 근사**하면서 연산 효율을 유지하는 구조를 목표로 삼았다.  

## 2. 제안하는 방법  

### 2.1 Inception 모듈  
- **병렬 분기**로 1×1, 3×3, 5×5 합성곱과 3×3 풀링을 동시에 수행  
- **차원 축소용 1×1 합성곱**을 3×3·5×5 합성곱 앞과 풀링 뒤에 삽입해 연산량 제어  
- 네 분기의 출력 채널을 깊이 방향으로 **연결(concatenate)**  

```
입력 → ┌─1×1 conv──────────→  
      │  
      ├─1×1 conv→3×3 conv──→  
      │  
      ├─1×1 conv→5×5 conv──→  
      │  
      └─3×3 pool→1×1 conv──→ → 출력
```

### 2.2 수식적 표현  
- 차원 축소 전 3×3 분기의 연산량: O(N·3·3·M·K)  
- 1×1로 M→R 차원 축소 시:  
  - 축소 전후 연산량 비교:  

$$
      O(N·3·3·R·K) + O(N·1·1·M·R)
      \ll O(N·3·3·M·K)
    $$  
  
  (여기서 N=특성 맵 크기, M=입력 채널 수, K=출력 채널 수, R=축소 후 채널 수)  

## 3. 모델 구조  
- **GoogLeNet**: 22개 학습 층(파라미터 층)  
- 초기 레이어는 전통적 합성곱/풀링, 이후 여러 Inception 모듈을 9회 스택  
- 각 Inception 모듈 사이에 해상도 절반화(3×3 stride-2 max-pool)  
- **보조 분류기(auxiliary classifier)** 2개를 중간에 연결해 학습 시 그래디언트 소실 완화  
- 전체 파라미터 수 약 5백만, 단일 추론 시 1.5G multiply-add 연산 예산 유지  

## 4. 성능 향상 및 한계  

| 평가 과제            | Top-5 오류율 | 기존 대비 절대 개선 |
|-------------------|-----------|-----------------|
| ILSVRC14 분류 챌린지 | 6.67%     | −1.65pp (Clarifai 대비) |
| ILSVRC14 탐지 챌린지 | 43.9% mAP | +3.4pp (R-CNN 기반 대비) |

- **장점**  
  - 동일 예산 내 폭·깊이 확장  
  - 파라미터 수 12× 감소[1]
  - 분류·탐지 모두에서 SOTA 달성  

- **한계**  
  - 모듈 설계 수동 조정 필요  
  - 하드웨어 친화적 희소 구조 자동화 미흡  
  - 추가적인 메모리 오버헤드(보조 분류기, 복잡한 연결 구조)  

## 5. 일반화 성능 향상 관련 고찰  
- **다중 스케일 처리**: 1×1, 3×3, 5×5가 동시에 특징을 학습해 다양한 크기 패턴에 대응  
- **차원 축소→확장**을 통제된 방식으로 수행해 과적합 억제  
- **보조 분류기**로 중간층 특징에 직접 손실을 부여, 학습 시 표현력 제고  
- 결과적으로 작은 학습 데이터셋에도 **과적합 없이** 깊은 네트워크 학습이 가능  

## 6. 향후 연구에의 영향 및 고려점  
- **희소 구조 자동화**: Arora et al. 이론 기반으로 Inception 모듈을 자동 설계하는 연구  
- **모듈 경량화**: 모바일·임베디드 환경용 연산량 최적화  
- **다양한 도메인 적용**: 비전 외 시계열·음성 등에도 병렬 모듈 확장 여부 조사  
- **학습 안정화 기법**: 보조 분류기를 대체할 효율적 그래디언트 강화 방법 개발  

Szegedy et al.의 Inception 아키텍처는 **효율적 스케일 확장과 일반화의 균형**을 제시하며, 이후 자동화된 네트워크 검색(NAS)과 경량화 CNN 연구의 토대를 마련했다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b5d5be83-40fa-4250-9c6d-89734ac53cf9/1409.4842v1.pdf
