# Is Space-Time Attention All You Need for Video Understanding? | Action recognition

# 1. 핵심 주장 및 주요 기여 요약  
- **핵심 주장**  
  - 영상 이해에서 전통적인 3D 합성곱 네트워크(CNN) 대신 순수 **공간-시간(self-)어텐션**만으로도 최첨단 수준의 행동 인식 성능을 달성할 수 있다.  
- **주요 기여**  
  1. **TimeSformer**: ViT(Vision Transformer)를 영상에 확장하여, 프레임 패치를 일련의 토큰으로 보고 Transformer 인코더로 처리  
  2. **분리된 공간-시간 어텐션(divided attention, T+S)**: 한 블록 안에서 시간 어텐션과 공간 어텐션을 순차적으로 적용해 계산량(비교 연산 수)을 Joint ST 대비 극적으로 절감  
  3. **효율성과 성능**:  
     - Kinetics-400/600, Something-Something-V2 등 주요 벤치마크에서 SOTA 달성  
     - 3D CNN 대비 훈련·추론 비용 절감  
     - 한 클립으로 수십초 분량까지 처리 가능, 장기 시퀀스 모델링에 유리  

# 2. 문제 정의·제안 방법·모델 구조·성능 및 한계  
## 2.1 해결하고자 하는 문제  
- 3D CNN은  
  1) 고해상도·장시간 입력 시 계산·메모리 부담이 크고  
  2) 짧은 수용 영역으로 인해 장거리 종속성 포착에 제약이 있으며  
  3) 강한 귀납 편향(locality, equivariance)이 대규모 데이터 환경에서 표현력 제한을 초래  

## 2.2 제안 방법  
- **클립** $$X \in \mathbb{R}^{H\times W\times 3 \times F}$$을 $$P\times P$$ 크기 패치로 분할해 $$N=\frac{HW}{P^2}$$개의 패치를 생성  
- 각 패치 $$x^{(p,t)}\in\mathbb{R}^{3P^2}$$를 선형 임베딩 $$z^{(0)}\_{(p,t)}=E\,x^{(p,t)}+e^{\rm pos}_{(p,t)}$$로 토큰화  
- **분리된 공간-시간 어텐션** (블록 ℓ 내)  

1. 시간 어텐션:  

```math
       \alpha^{(\ell,a)}_{\rm time}(p,t)\;=\;\mathrm{softmax}\!\Bigl(\tfrac{q^{(\ell,a)}_{(p,t)}\,(k^{(\ell,a)}_{(p,1:F)} )^\top}{\sqrt{D_h}}\Bigr)
``` 
  
2. 공간 어텐션:  

```math
       \alpha^{(\ell,a)}_{\rm space}(p,t)\;=\;\mathrm{softmax}\!\Bigl(\tfrac{q^{(\ell,a)}_{(p,t)}\,(k^{(\ell,a)}_{(1:N,t)} )^\top}{\sqrt{D_h}}\Bigr)
```
  
  각각 잔차 연결·레이어 정규화·MLP 적용 후 출력  
- **모델 구조**: ViT-Base(12개 블록, 768-D, 12헤드) 기반. Divided 방식이 Joint ST 대비 비교 연산 수를 $$NF+1\to N+F+2$$로 절감  

## 2.3 성능 향상  
- **효율성**:  
  - 같은 해상도·프레임수에서 Joint ST 대비 3−10배 TFLOPs 감소  
  - 8×224×224 입력 기준 추론 0.59 TFLOPs(3크롭), 121M 파라미터  
- **정확도**:  
  - K400: 80.7% (Long-range variant)  
  - SSv2: 62.5% (High-res variant)  
  - HowTo100M(장기 모델링): 62.6% (96프레임 입력)  
- **학습 비용**: ImageNet-1K 사전학습 후 K400 훈련 416 GPU-시간, SlowFast(Res50) 4× 비용  

## 2.4 한계  
1. **큰 모델 크기**(121M+)와 사전학습 의존성  
2. **메모리 부담**: 분리형이라도 96프레임·고해상도에서 GPU 메모리 요구  
3. **시간 어텐션 순서 민감**: T→S만 최적, S→T나 병렬식은 성능 하락  
4. **사전학습 데이터**: SSv2처럼 복잡한 시공간 패턴은 충분한 데이터 필요  

# 3. 모델 일반화 성능 향상 가능성  
- **장기 시퀀스 처리**: 96프레임(102초) 이상까지 확장이 가능해, 드라마틱한 장기 종속 학습  
- **사전학습 확장**: ImageNet-21K→Kinetics→HowTo100M 순 사전학습으로 단계적 일반화 강화  
- **어텐션 스파스화**: 동적 플렉시블 스파스 패턴 학습으로 메모리 절감하며 문맥범위 확장  
- **클립 윈도우 융합**: 여러 클립 뷰를 효율적으로 통합하는 토큰 융합 기법으로 일반화  

# 4. 향후 연구에 미치는 영향 및 고려사항  
- **영향**  
  - 영상 분야에서도 **Transformer 기반 순수 어텐션** 아키텍처 가능성 제시  
  - 장기 행동 예측, 영상 언어 통합, 자율주행 등 장시간 문맥 모델링 응용 촉진  
- **고려사항**  
  1. **사전학습 규모·도메인**: 영상 특화 대규모 자가감독(contrastive, masked) 사전학습 필요  
  2. **연산·메모리 최적화**: 스파스·로컬·구조적 경량화로 대용량 시퀀스 처리  
  3. **멀티모달 확장**: 음성·텍스트 결합해 영상-언어 태스크 일반화  
  4. **해석 가능성**: 어텐션 패턴·특정 토큰 중요도 분석으로 모델 신뢰성 확보

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/899499bf-d6b6-44bf-aed0-bcbeaf48398a/2102.05095v4.pdf)
