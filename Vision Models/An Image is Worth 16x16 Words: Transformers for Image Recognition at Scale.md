이 논문은 "Vision Transformer (ViT)"라는 모델을 제안하는데, 이 모델은 기존의 합성곱 신경망(CNN) 대신 순수한 트랜스포머 아키텍처를 이미지 패치 시퀀스에 직접 적용합니다.  
ViT는 큰 데이터셋에서 사전학습(pre-training)된 후, 이미지넷(ImageNet), CIFAR-100 등 다양한 이미지 인식 작업에 활용할 수 있으며, 뛰어난 성능을 보여줍니다.  
또한, 전통적인 CNN과 비교했을 때 학습에 사용하는 계산 자원이 훨씬 적고 확장성도 높다는 것이 핵심 장점입니다.

1. 입력 처리: 이미지를 일정 크기의 격자(패치)로 나누고, 각 패치를 1D 시퀀스로 변환합니다. 각 패치는 (P, P) 크기의 작은 이미지 블록이고, 이들을 평평하게 펼쳐서 벡터로 만든 후, 선형 투영 계층을 통해 고정 크기(D)의 벡터(패치 임베딩)로 만듭니다.

2. 포지셔닝 임베딩: 이미지 내 위치 정보를 보존하기 위해, 위치 정보를 나타내는 포지셔닝 임베딩을 사용하며, 사전 학습된 임베딩을 고해상도 이미지를 위해 선형 보간을 통해 조정할 수 있습니다.

3. 트랜스포머 인코더: 이렇게 만들어진 시퀀스(패치 임베딩)를 표준 트랜스포머 인코더에 입력합니다. 이 인코더는 여러 층의 셀프 어텐션과 피드포워드 네트워크로 구성되어 있으며, 언어 모델에서 사용하는 것과 거의 동일합니다.

4. 클래스 토큰: 시퀀스의 처음에 특별한 "클래스 토큰"을 넣고, 최종 출력에서 이 토큰의 표현을 사용해 분류 결과를 얻습니다.

5. 학습 및 fine-tuning: 이 모델은 대규모 데이터셋에서 사전학습(pre-training, 예를 들어, 마스킹 없는 언어 모델처럼 다수의 이미지를 사용)된 후, 특정 분류 태스크에 맞춰 미세 조정(fine-tuning)됩니다.

이러한 구조는 CNN과 달리 이미지 내 지역적 구조를 별도로 명시하지 않고도 뛰어난 성능을 발휘하며, 전체적으로 매우 간단하고 확장 가능하다는 장점이 있습니다.
