# LSGAN : Least Squares Generative Adversarial Networks | Image generation

**핵심 요약**  
LSGAN은 기존 GAN의 시그모이드 교차 엔트로피 손실이자에 vanishing gradient 문제를 일으켜 학습이 불안정하고 생성물 품질이 제한된다는 점을 지적하고,[1] 이를 해결하기 위해 판별기에 최소제곱 손실(least squares loss)을 도입함으로써 판별 경계에서 멀리 떨어진 가짜 샘플에도 충분한 기울기를 제공하여(1) 더 안정적인 학습과 (2) 고품질 이미지 생성을 달성했다[1].

## 1. 해결하고자 하는 문제  
기존 GAN은 판별자 $$D$$가 실·가짜를 구분할 때 시그모이드 교차 엔트로피 손실을 사용한다. 이 경우 가짜 샘플이 판별 경계의 올바른 쪽에만 위치해도 손실이 거의 0이 되어, 생성자 $$G$$의 업데이트 시 충분한 기울기가 제공되지 않아 **vanishing gradient** 문제가 발생한다. 결과적으로  
- 생성된 이미지가 실제 데이터 분포의 경계를 충분히 따라가지 못해 품질이 떨어지고  
- 학습이 불안정해져 모드 붕괴(mode collapse)나 발산이 자주 발생  

## 2. 제안하는 방법  
LSGAN은 판별자의 손실을 최소제곱 오차로 대체하여, 판별 경계에서 멀리 떨어진 가짜 샘플에도 비례하는 손실을 부여함으로써 생성자가 경계 쪽으로 샘플을 끌어당기도록 유도한다[1].

### 2.1 손실 함수 정의  
가짜에 대한 레이블을 $$a$$, 진짜에 대한 레이블을 $$b$$, 생성자가 유도할 목표 레이블을 $$c$$로 놓고 다음과 같이 정의한다[1]:

$$
\begin{aligned}
\min_D \;V_{\text{LSGAN}}(D)&=\tfrac{1}{2}\,\mathbb{E}\_{x\sim p_{\text{data}}}\big[(D(x)-b)^2\big]
    +\tfrac{1}{2}\,\mathbb{E}\_{z\sim p_z}\big[(D(G(z))-a)^2\big],\\
\min_G \;V_{\text{LSGAN}}(G)&=\tfrac{1}{2}\,\mathbb{E}_{z\sim p_z}\big[(D(G(z))-c)^2\big].
\end{aligned}
$$  

일반적으로 $$a=0$$, $$b=1$$, $$c=1$$을 사용하며, 이 설정 시 LSGAN의 학습은 Pearson $$\chi^2$$ 발산을 최소화하는 것과 동치가 된다[1].

### 2.2 모델 구조  
두 가지 아키텍처를 설계하였다[1]:

1. **112×112 해상도 이미지 생성용**  
   - **생성자**: VGG 스타일의 일련의 3×3 Deconv + BatchNorm + ReLU  
   - **판별자**: DCGAN 기반의 5×5 Conv + BatchNorm + LeakyReLU  

2. **다중 클래스(예: 손글씨 한자 3,740종) 대응용**  
   - **조건부 LSGAN**: 라벨(one-hot 3,740차원)을 선형 매핑을 거쳐 낮은 차원 벡터로 변환 후, 각 층에 결합  
   - 이로써 대규모 클래스에도 메모리·연산 부담 없이 생성 품질을 보장  

## 3. 성능 향상 및 한계  

### 3.1 성능 향상  
- **이미지 질 개선**: LSUN-bedroom 등 5개 장면 데이터셋에서 DCGAN·EBGAN 대비 시각적 품질 우수[1].  
- **학습 안정성**:  
  - 배치노멀 제거 실험에서 Adam·RMSProp 모두 LSGAN이 모드 붕괴 없이 수렴[1].  
  - 8-가우시안 혼합 실험에서도 정규 GAN은 모드 붕괴가 시작되지만, LSGAN은 전체 분포를 학습[1].  
- **다중 클래스 생성**: 3,740개 한자 생성 시, 가독성 있는 문자 생성 가능[1].

### 3.2 한계 및 일반화 성능  
- **경계 유도 방식의 한계**: 판별 경계로 생성 샘플을 끌어당기는 간접적 방법이므로, 실제 데이터 분포 심층 구조를 완전 학습했다고 보장하기 어려움.  
- **고해상도·다양성**: 112×112보다 높은 해상도나 매우 복잡한 분포에서는 추가적 구조 개선 필요.  
- **일반화 성능**:  
  - Pearson $$\chi^2$$ 발산 최적화는 특정 분포 차이를 강조하나, 실제 고차원 데이터 일반화에는 다른 발산(예: Wasserstein)이 더 유리할 수 있음.  
  - 조건부 LSGAN에서 수백만 대 클래스 대응 시, 라벨 매핑의 정보 손실과 차원 축소가 오히려 일반화에 제약을 줄 가능성  

## 4. 향후 연구 영향 및 고려 사항  
- **다양한 발산 척도 비교**: $$\chi^2$$ 외 Jensen–Shannon·Wasserstein·f-발산을 통합 비교하여, 데이터 특성별 최적 발산 함수 설계  
- **직접 분포 매칭 기법**: 경계 유도가 아닌 **실제 데이터 포인트로 직접 수렴**시키는 손실(term) 도입 연구  
- **고해상도·텍스처 표현**: LSGAN을 StyleGAN·BigGAN과 결합하여 고해상도 텍스처·다양성 보장  
- **일반화 이론 분석**: LSGAN이 제공하는 기울기 특성이 이론적·경험적으로 모델 일반화에 미치는 영향을 심층 분석  

이 논문은 GAN 학습 안정성 개선과 고품질 이미지 생성의 새로운 손실 설계를 제안함으로써, 후속 연구에서 다양한 발산 최적화 기법과 복잡 분포 생성 문제 해결을 위한 기반을 마련했다[1].

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d9fdb7bd-516b-4a4d-8af0-e9dbab0be3f9/1611.04076v3.pdf
