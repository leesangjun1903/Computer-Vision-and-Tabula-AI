# DMD2 : Improved Distribution Matching Distillation for Fast Image Synthesis | 2024 · 155회 인용, Image generation, Accelerate sampling

# 핵심 요약 및 주요 기여

**주요 주장:** 본 논문에서는 기존 Distribution Matching Distillation(DMD)의 **회귀 손실(regression loss)** 의존도를 제거하고, **두 시계열 업데이트 규칙(Two Time-scale Update Rule, TTUR)** 과 **GAN 손실(GAN loss)** 및 **역방향 시뮬레이션(backward simulation)** 을 통합하여, 교사 모델을 능가하는 고품질의 빠른 이미지 합성을 달성하는 새로운 증류 기법 DMD2를 제안한다. 이로써 대규모 텍스트-투-이미지 합성에서의 데이터셋 구축 비용과 계산 부담을 획기적으로 줄이고, 학생 모델의 일반화 성능을 향상시킨다.[1]

# 문제 정의 및 제안 기법

## 1. 해결하고자 하는 문제  
- **확산 모델(diffusion model)** 의 샘플링 과정은 수십~수백 단계의 반복 연산을 필요로 하여 매우 느리고 비용이 높음.  
- 기존 DMD는 학생 모델이 교사 모델의 분포를 정합(distribution matching)하도록 학습하나, **학습 안정성을 위해 대량의 노이즈-이미지 쌍(regression loss)을 사전 생성**해야만 함.  
- 이로 인해 대규모 텍스트-투-이미지 합성(예: LAION-6B)에서는 **수백일(700 A100 GPU days)** 이상의 전처리 비용이 발생하며, 학생 성능이 교사에 묶이는 한계를 보임.[1]

## 2. 제안 방법  
1) **회귀 손실 제거:**  
   - DMD의 분포 정합 손실만으로 학습 시 불안정성을 보이나, 이는 **가짜 확산 비평가(fake critic)** 가 학생 분포를 충분히 추정하지 못하기 때문이라 판단.  
2) **두 시계열 업데이트 규칙(TTUR):**  
   - 가짜 확산 비평가를 **학생 업데이트 1회당 5회** 갱신하여(fake score updates), 분포 추정을 안정화하고 학습을 수렴시킴.[1]
3) **GAN 손실 통합:**  
   - 학생 생성물과 실제 이미지 간 판별(discriminator)을 학습하여, 교사 모델의 점수 함수(score function) 근사 오차를 보완.  
   - GAN 손실은 분포 수준의 정합에 부합하며, **사전 쌍 데이터 없이(real data 사용)** 더 높은 품질을 달성하도록 함 (식 (4)).[1]

$$ L_{\mathrm{GAN}} = \mathbb{E}_{x\sim p_{\mathrm{real}}}\log D(x) + \mathbb{E}_{z\sim \mathcal{N}(0,I)}\log\bigl(1 - D(G(z))\bigr) $$  

4) **다단계 생성기(Multi-step generator) 및 역방향 시뮬레이션:**  
   - 1단계 한정이던 기존 DMD를 **4단계 등 다단계** 샘플링 지원으로 확장.  
   - **학습-추론 불일치(training–inference mismatch)** 를 해결하기 위해, 학습 시 실제 노이즈가 아닌 **학생이 생성한 중간 샘플**을 입력으로 사용(backward simulation), 성능을 추가로 향상시킴.[1]

# 모델 구조

- **학생 생성기(Generator, G):** 1-단계 또는 N-단계 UNet 기반 네트워크.  
- **가짜 확산 비평가(Fake diffusion critic, $$s_{\mathrm{fake}}$$):** 학생 분포의 점수(score) 함수를 근사.  
- **GAN 판별기(Discriminator, D):** 실제 이미지 vs. 학생 이미지를 구분.  
- 학습 과정은 아래 절차를 번갈아 수행:  

1. $$G$$ 최적화: 분포 정합 손실(식 (2)) + GAN 손실(식 (4)) 최소화  

$$ L_{\mathrm{DMD}} = \mathbb{E}_{t,z}\bigl[s_{\mathrm{real}}(F(z),t) - s_{\mathrm{fake}}(F(z),t)\bigr]\frac{\partial G(z)}{\partial z} $$  
  
  2. $$s_{\mathrm{fake}}, D$$ 최적화: fake 데이터에 대한 점수 매칭 + GAN 분류 손실  
  3. 매 $$G$$ 업데이트당 fake 업데이트 5회(TTUR)

# 성능 향상

- **ImageNet-64×64**: 단일 단계 모델에서 **FID 1.28**, 원본 교사(ODE 샘플러)의 2.32 대비 우수한 결과 달성.  
- **COCO 2014**: SDXL 기반 4단계 학생 모델이 **FID 19.32**, 패치 FID 20.86으로 교사(100단계)와 유사한 품질 확보.  
- **제로샷 텍스트-투-이미지**: SD v1.5 1단계 학생이 **FID 8.35**로 기존 DMD 대비 3.14점 개선.  
- **인간 평가**에서도 교사 및 기존 증류 기법 대비 선호도 상승.[1]

# 일반화 성능 향상

- **분포 정합 중심 학습**: 회귀 손실 제거로 학생이 교사 특정 경로에 종속되지 않고, 불완전한 점수 함수 오류를 넘어설 수 있음.  
- **GAN 손실**: 실제 데이터 분포에서 직접 학습하여, 학습 중 발생하는 근사 오차와 편향을 보정.  
- **역방향 시뮬레이션**: 학습-추론 입력 분포 불일치를 해소하여, 다양한 입력 분포에 대한 **강건성(robustness)** 향상.  
- **다단계 확장**: 모델 용량과 최적화 난이도를 줄여, 보다 복잡한 분포에도 적용 가능.

# 한계 및 향후 연구 과제

- **다양성 감소**: 교사 대비 소폭 떨어지는 이미지 다양성(LPIPS 측정).  
- **고정된 가이드 스케일**: 사용자 유연성 제한. **가변 가이드 스케일** 적용 연구 필요.  
- **컴퓨팅 비용**: 대규모 모델 증류는 여전히 고비용. 경량화, 효율적 분산 학습 기법 개발이 과제.  
- **정책 및 윤리**: 합성 이미지 오남용 방지를 위한 모니터링, 편향 완화 연구 병행 필요.

# 향후 영향 및 고려 사항

이 방법론은 **빠른 이미지 합성 분야**에서 새로운 기준을 제시하며, 차세대 실시간 생성 애플리케이션(그래픽 디자인, 교육 콘텐츠 등)에 활용될 전망이다. 향후 연구에서는 **가이드 스케일 제어**, **인간 피드백 기반 정렬(alignment)**, **다양성-품질 균형** 최적화, **경량화된 학습 파이프라인** 구축에 중점을 둘 필요가 있다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/8e069077-ede4-4cc2-9ca8-4496be1c4f4a/2405.14867v2.pdf)
