# Learning Fine-grained Image Similarity with Deep Ranking

## 핵심 주장 및 주요 기여  
**핵심 주장**  
– 기존의 카테고리 수준 이미지 유사도 모델은 동일 카테고리 내의 세밀한 시각적 차이를 반영하지 못하므로, **이미지 간 미세(grained) 유사도 학습**을 위해 픽셀 단위에서 직접 학습하는 새로운 방식이 필요하다.  
**주요 기여**  
1. **Deep Ranking 모델**: 삼중항(triplet) 순위 손실(hinge loss) 함수를 이용해 쿼리–긍정–부정 이미지 간 상대적 유사도 학습  
2. **멀티스케일 네트워크 구조**: 고해상도 ConvNet 경로와 저해상도 두 경로를 결합해 의미적 정보와 세밀한 시각 특징을 동시에 포착  
3. **온라인 중요도 기반 삼중항 샘플링**: 대용량(1,200만 장) 데이터에서 메모리 부담 없이 효과적인 triplet을 선택하는 효율적 스트리밍 샘플링  
4. **공개 평가 데이터셋**: 인간 평가자에 의해 라벨된 약 1.4만 개 triplet으로 미세 유사도 모델의 성능 비교  

***

## 1. 해결하고자 하는 문제  
- **카테고리 수준 유사도 한계**: “자동차” 카테고리만 구분해도 색상·질감 등 세밀 차이는 반영 불가  
- **분류 vs. 순위 학습**: 분류 모델은 클래스 구분에 집중하나, 유사도 순위 모델은 “검은 차” 쿼리에 대해 “진한 회색 차”를 “흰 차”보다 더 유사하게 순위 매김  

## 2. 제안 방법  
### 2.1 Triplet 기반 순위 손실  
쿼리 $$p_i$$, 긍정 $$p_i^+$$, 부정 $$p_i^-$$ 이미지 임베딩 $$f(\cdot)$$ 사이의 거리 비교로 순위 위반을 패널티  

$$
\ell(p_i,p_i^+,p_i^-)
= \max\{0, g + \|f(p_i)-f(p_i^+)\|_2^2 - \|f(p_i)-f(p_i^-)\|_2^2\}
$$  

최종 목적함수  

$$
\min_W \sum_i \ell(p_i,p_i^+,p_i^-) + \lambda \|W\|_2^2
$$  

– $$g$$: 마진(hard-negatives 구분)  
– $$\lambda=0.001$$: 정규화  

### 2.2 멀티스케일 네트워크 구조  
- **고해상도 ConvNet 경로**: 의미적 추상화(semantic invariance)  
- **저해상도 경로 두 개**: 저수준 시각 특징(pixels, 색·질감)  
- 세 경로 임베딩(각 4096차원) L2 정규화 후 선형 결합 → 최종 4096차원  

### 2.3 온라인 중요도 기반 삼중항 샘플링  
- **버퍼당 카테고리별 저장**: 각 카테고리별 버퍼 용량 고정  
- **쿼리 샘플링**: 카테고리 내 총 유사도 점수 비례 확률  
- **긍정/부정 샘플링**: 상위 유사도일수록 더 높은 확률, 마진 기준 필터링  
- **스트리밍 방식**: 메모리에 전 데이터 로드 없이 reservoir sampling 응용  

***

## 3. 성능 향상 및 한계  
### 3.1 성능 비교  
| 방법                          | Similarity Precision | Score@30   |
|------------------------------|----------------------|------------|
| 전통 손수 특징(HOG, SIFT 등) | 62–68%               | 2700–3500  |
| L1HashKPCA + OASIS           | 79.2%                | 6813       |
| **DeepRanking (멀티스케일)** | **85.7%**            | **7004**   |

- **단일 스케일 순위 학습**: 84.6% / 6245 → 멀티스케일로 both 지표↑  
- **ConvNet 분류 모델**: 82.8% / 5772 → 순위 fine-tuning만으로도 유의미 개선  
- **한계**:  
  - 대규모 부트스트랩 데이터(1,400만 장)와 고비용 ‘골든 피처’ 의존  
  - 실시간 검색엔진 대규모 배포 시 연산·메모리 부담  

***

## 4. 일반화 성능 향상 가능성  
- **다중 도메인 적응**: 멀티스케일 경로를 다양한 해상도·스케일 특성 학습에 확장하면, 새로운 카테고리나 스타일 변화 대응력 강화  
- **하드 네거티브 마이닝 강화**: 마진 정규화 $$g$$를 동적·적응적 스케줄링해 더욱 어려운 부정 샘플 발굴  
- **경량화 모델**: 모바일·엣지 환경 대응을 위한 경량화(채널 축소, 지식 증류)와도 호환 가능  
- **정규화 기법 조합**: 배치 정규화·스펙트럴 정규화 추가로 오버피팅 완화  

***

## 5. 향후 연구 및 고려 사항  
- **데이터 효율성**: 대규모 부트스트랩 없이도 성능을 유지하기 위한 반지도학습(semi-supervised)·자기지도학습(self-supervised) 기법  
- **시간·공간 복합 유사도**: 비디오 프레임 연속성이나 위치 정보 같은 추가 모달리티 통합  
- **대규모 배포 최적화**: 근사 최근접탐색(ANN)과 양자화 압축 적용으로 실 서비스 응답 속도 및 메모리 절감  
- **윤리적 고려**: 이미지 유사도 응용 시 개인정보, 저작권, 편향(bias) 문제 검토  

위와 같은 발전 방향을 통해 **멀티스케일 Deep Ranking** 모델은 보다 폭넓은 도메인과 자원 제약 환경에서도 **일반화 성능**을 높이며, 차세대 이미지 검색·추천·검출 시스템의 핵심 기술로 자리매김할 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/ea488397-ce83-4ef4-a478-6afe3853f960/1404.4661v1.pdf
