# A survey of the Vision Transformers and its CNN-Transformer based Variants

최근에는 다양한 컴퓨터 비전 응용 분야에서 CNN(컨볼루션 신경망)을 대체할 수 있는 대안으로 비전 트랜스포머가 인기를 얻고 있습니다.  
이미지의 글로벌 관계에 초점을 맞추는 능력으로 인해 이러한 비전 트랜스포머는 용량이 크지만 CNN에 비해 일반화가 좋지 않을 수 있습니다.  
최근에는 비전 트랜스포머의 컨볼루션 및 셀프 어텐션 메커니즘의 하이브리드화가 로컬 및 글로벌 이미지 표현을 모두 활용하는 능력으로 인해 인기를 얻고 있습니다.  
하이브리드 비전 트랜스포머라고도 알려진 이러한 CNN-Transformer 아키텍처는 비전 애플리케이션에 대한 놀라운 결과를 보여주었습니다.  
최근 이러한 하이브리드 비전 트랜스포머의 수가 급격히 증가함에 따라 이러한 아키텍처에 대한 분류 및 설명이 필요합니다.  
이 Survey는 최근 비전 트랜스포머 아키텍처의 분류 체계, 특히 하이브리드 비전 트랜스포머의 분류 체계를 제시합니다.  
또한 Attention 메커니즘, 위치 임베딩, 다중 규모 처리, 컨볼루션 등 각 아키텍처의 주요 기능도 논의합니다.  
이 설문 조사는 다양한 컴퓨터 비전 작업에서 탁월한 성능을 제공할 수 있는 하이브리드 비전 트랜스포머의 잠재력을 강조합니다.  
또한 이는 빠르게 발전하는 이 분야의 미래 방향을 제시합니다.

# Introduction

디지털 이미지는 본질적으로 복잡하며 사물, 장면, 패턴과 같은 높은 수준의 정보를 나타냅니다.  
이 정보는 컴퓨터 비전 알고리즘에 의해 분석되고 해석되어 객체 인식, 움직임 추적, 특성 추출 등 이미지 내용에 대한 의미 있는 통찰력을 추출할 수 있습니다.  
컴퓨터 비전은 다양한 분야에 적용되어 활발하게 연구되고 있는 분야입니다.  
그러나 이미지 데이터에서 높은 수준의 정보를 추출하는 것은 밝기, 포즈, 배경의 혼잡함 등의 변화로 인해 어려울 수 있습니다.

CNN(Convolutional Neural Network)의 출현은 컴퓨터 비전 영역에 혁명적인 변화를 가져왔습니다.  
이러한 네트워크는 다양한 범위의 컴퓨터 비전 작업, 특히 이미지 인식, 객체 감지 및 분할에 성공적으로 적용되었습니다.  
CNN은 이미지에서 특성과 패턴을 자동으로 학습하는 기능으로 인해 인기를 얻었습니다.  
일반적으로 특성 모티프로 알려진 지역적 패턴은 이미지 전체에 체계적으로 분포되어 있습니다.  
컨볼루션 레이어의 다양한 필터는 다양한 특성 모티프를 포착하도록 설계되었습니다.  
CNN의 풀링 레이어는 차원 축소와 변형에 대한 견고성을 통합하는 데 활용됩니다.  
CNN의 로컬 수준 처리로 인해 공간 정보가 손실될 수 있으며, 이는 더 크고 복잡한 패턴을 처리할 때 성능에 영향을 줄 수 있습니다.

최근 Vaswani et al.이 처음 소개한 이후 트랜스포머로 전환이 이루어졌습니다.  
2017년에는 텍스트 처리 애플리케이션용으로 출시되었습니다.  
2018년에 Parmer 등은 이미지 인식 작업을 위해 트랜스포머를 활용하여 뛰어난 결과를 보여주었습니다.  
이후 다양한 비전 관련 애플리케이션에 트랜스포머를 적용하는 것에 대한 관심이 높아지고 있습니다.  
2020년 Dosovitskiy et al.은 이미지 분석을 위해 특별히 설계된 트랜스포머 아키텍처인 ViT(Vision Transformer)를 출시하여 경쟁력 있는 결과를 보여주었습니다.

ViT 모델은 입력 이미지를 특정 수의 패치로 분할하는 방식으로 작동하며, 각 패치는 이후에 평면화되어 일련의 트랜스포머 레이어에 공급됩니다.  
트랜스포머 레이어를 사용하면 모델이 패치와 해당 기능 간의 관계를 학습하여 이미지의 전역 규모에서 기능 모티프를 식별할 수 있습니다.  
로컬 수용 필드가 있는 CNN과 달리 ViT는 self-attention 모듈을 활용하여 장거리 관계를 모델링하므로 이미지의 전체 보기를 캡처할 수 있습니다.  
ViT의 전역 수용 필드는 공간 정보를 유지하여 이미지 전체에 분산된 복잡한 시각적 패턴을 식별하는 데 도움이 됩니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_01.png)

- multi-attention mechanism and convolution operation

CNN과 ViT는 디자인과 시각적 패턴을 캡처하는 방식의 차이 외에도(그림 1 참조) 귀납적 편향도 다릅니다.  
CNN은 인접 픽셀의 상관 관계에 크게 의존하는 반면 ViT는 최소한의 사전 지식을 가정하므로 레이블이 지정된 데이터에 덜 의존합니다.  
ViT 모델은 객체 인식, 분류, 의미론적 분할 및 기타 컴퓨터 비전 작업에서 탁월한 결과를 얻었습니다.

그러나 ViT의 대용량에도 불구하고 제한된 훈련 데이터의 경우 CNN에 비해 여전히 낮은 성능을 나타냅니다.  
게다가 수용 영역이 넓기 때문에 훨씬 더 많은 계산이 필요합니다.  
따라서 CNN-Transformers라고도 알려진 HVT(Hybrid Vision Transformers) 개념이 CNN과 ViT의 성능을 결합하기 위해 도입되었습니다.  
이러한 하이브리드 모델은 CNN의 컨볼루셔널 레이어를 활용하여 로컬 특성을 캡처한 다음 ViT에 공급되어 self-attention 메커니즘을 사용하여 글로벌 컨텍스트를 얻습니다.  
HVT는 많은 이미지 인식 작업에서 향상된 성능을 보여주었습니다.

최근에는 ViT의 최근 아키텍처 및 구현 발전을 논의하기 위해 다양하고 흥미로운 Survey가 실시되었습니다.  
이러한 문서의 대부분은 주로 ViT와 컴퓨터 비전의 응용 프로그램에 중점을 두고 있지만 NLP 응용 프로그램용으로 개발된 Transformer 모델을 기반으로 하는 자세한 논의도 제공합니다.  
한편, 본 survey는 CNN의 아이디어와 트랜스포머(CNN-Transformer) 아키텍처, 이들의 분류 및 응용을 결합한 최근의 하이브리드 비전 트랜스포머에 주로 중점을 둡니다.  
이 외에도 본 survey는 일반적인 ViT의 분류를 제시하고 핵심 아이디어(건축 설계)를 기반으로 새로 등장하는 접근 방식을 철저하게 분류하려고 합니다.  
이와 관련하여 먼저 ViT 네트워크의 필수 구성 요소를 소개한 다음 다양한 최신 ViT 아키텍처에 대해 논의합니다.

본 문서에서 논의된 보고된 ViT 모델은 기본 두드러진 특성에 따라 크게 6가지 범주로 분류됩니다.  
ViT에 대해 논의한 후에는 컨볼루션 작업과 다중 어텐션 메커니즘(multi-attention mechanism)의 이점을 모두 활용하는 모델인 HVT에 대한 자세한 논의가 이어집니다.  
또한, HVT의 최근 아키텍처와 다양한 컴퓨터 비전 작업에서의 응용에 대해 자세히 설명합니다.  
또한 아키텍처에서 self-attention과 결합하여 컨볼루션 작업을 통합하는 방법을 기반으로 HVT에 대한 분류를 제시합니다.  
우리의 분류 체계는 HVT를 7개의 주요 그룹으로 나누며, 각 그룹은 컨볼루셔널 연산자와 다중 주의 연산자를 모두 활용하는 다양한 방법을 반영합니다.

논문은 다음과 같이 구성됩니다. (그림 2 참조) 

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_02.png)

섹션 1에서는 ViT(Vision Transformers)에 대한 체계적인 이해를 제시하고 CNN과의 차이점 및 하이브리드 비전 트랜스포머의 출현을 강조합니다.  
계속해서 섹션 2에서는 다양한 ViT 변형에 사용되는 기본 개념을 다루고, 섹션 3과 섹션 4에서는 각각 최근 ViT 및 HVT 아키텍처에 대한 분류를 제공합니다.  
섹션 5에서는 특히 컴퓨터 비전 분야에서 HVT의 사용에 중점을 두고 있으며 섹션 6에서는 현재 과제와 미래 방향을 제시합니다.  
마지막으로 7장에서는 Survey를 마무리한다.

# Fundamental Concepts in ViTs
Transformer의 기본 아키텍처 레이아웃은 그림 3에 나와 있습니다.  

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_03.png)

처음에는 입력 이미지가 분할되고 평면화되어 패치 임베딩(Patch Embedding)이라고 알려진 저차원 선형 임베딩으로 변환됩니다.  
그런 다음 위치 임베딩과 클래스 토큰이 이러한 임베딩에 첨부되고 클래스 레이블을 생성하기 위해 트랜스포머의 인코더 블록에 공급됩니다.  
인코더 블록에는 MSA(Multi-Head Attention) 계층 외에도 FFN(Feed-Forward Neural) 네트워크, 정규화 계층 및 잔여 연결이 포함되어 있습니다.  
마지막으로 마지막 헤드(MLP 계층 또는 디코더 블록)는 최종 출력을 예측합니다.  
이러한 각 구성 요소는 다음 하위 섹션에서 자세히 설명됩니다.

## Patch embedding
패치 임베딩은 ViT 아키텍처에서 중요한 개념입니다.  
여기에는 이미지 패치를 벡터 표현으로 변환하는 작업이 포함되며, 이를 통해 ViT는 트랜스포머 기반 접근 방식을 사용하여 이미지를 토큰 시퀀스로 처리할 수 있습니다.  
입력 이미지는 고정된 크기의 겹치지 않는 부분으로 분할되고, 1차원 벡터로 평면화되고, D 임베딩 차원(방정식 1)이 있는 선형 레이어를 사용하여 고차원 특성 공간에 투영됩니다.  

$𝑿_{𝑝𝑎𝑡𝑐ℎ}^{𝑁×𝐷} =𝑅(𝑰_{𝑖𝑚𝑎𝑔𝑒}^{𝐴×𝐵×𝐶})$

이 접근 방식을 통해 ViT는 다양한 패치 간의 장기적인 종속성을 학습하여 이미지와 관련된 작업에서 유망한 결과를 얻을 수 있습니다.  
$𝑰_{𝑖𝑚𝑎𝑔𝑒}$ : 입력 이미지, 크기 : ${𝐴×𝐵×𝐶}$  
$𝑅(.)$ : $𝑿_{𝑝𝑎𝑡𝑐ℎ}$ 크기의 N 개수 패치를 D만큼 생성하는 재구성 함수  , ($𝑁= A/P × B/P$, $𝐷= 𝑃× 𝑃 × C$)  
P : 패치 크기, C : 채널 수

## Positional embedding
ViT는 위치 인코딩을 활용하여 위치 정보를 입력 시퀀스에 추가하고 이를 네트워크 전체에 유지합니다.  
패치 간의 순차 정보는 패치 임베딩 내에 통합된 위치 임베딩을 통해 캡처됩니다.  
ViT 개발 이후 순차 데이터 학습을 위해 수많은 위치 임베딩 기술이 제안되었습니다. 이러한 기술은 세 가지 범주로 분류됩니다:

### Absolute Position Embedding (APE)
위치 임베딩은 인코더 블록 전에 APE를 사용하여 패치 임베딩에 통합됩니다.  

$𝑿=𝑿_{𝑝𝑎𝑡𝑐ℎ}+𝑿_{𝑝𝑜𝑠} \tag{2}$

X : Transformer 입력  
$𝑿_{𝑝𝑎𝑡𝑐ℎ}$ : 패치 임베딩 , $(𝑁+1)×𝐷$ 차원  
$𝑿_{𝑝𝑜𝑠}$ : 학습 가능한 위치 임베딩 , $(𝑁+1)×𝐷$ 차원 , D : 임베딩 차원  

학습 가능한 단일 또는 두 세트의 위치 임베딩에 해당하는 $𝑿_{𝑝𝑜𝑠}$을 학습하는 것이 가능합니다.

### Relative Position Embedding (RPE)
RPE(Relative Position Embedding) 기술은 상대 위치와 관련된 정보를 Attention 모듈에 통합하는 데 주로 사용됩니다.  
이 기술은 패치 간의 공간적 관계가 절대 위치보다 더 큰 비중을 갖는다는 아이디어에 기반을 두고 있습니다.  
RPE 값을 계산하기 위해 학습 가능한 매개변수를 기반으로 하는 조회 테이블이 사용됩니다.  
조회 프로세스는 패치 간의 상대적 거리에 따라 결정됩니다.  
RPE 기술은 다양한 길이의 시퀀스로 확장 가능하지만 훈련 및 테스트 시간이 늘어날 수 있습니다.  

```math
e_{ij} = \frac{x_{i}W^{Q}\left(x_{j}W^{K} + a^{K}_{ij}\right)^{T}}{\sqrt{d_{z}}}
```
$a^{K}_{ij}$ : edge representation for the inputs $x_i$, $x_j$

and output representation $z_i$, $\alpha_{ij}\$ : $𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑸,𝑲,𝑽)$
```math
z_{i} = \sum^{n}_{j=1}\alpha_{ij}\left(x_{j}W^{V} + a_{ij}^{V}\right)
```

![](https://production-media.paperswithcode.com/methods/ceefba40-152a-41b0-840c-6446df1cd89b.png)

### Convolution Position Embedding (CPE)
CPE(Convolutional Position Embedding) 방법은 입력 시퀀스의 2D 특성을 고려합니다.  
2D 컨볼루션은 2D 특성을 활용하기 위해 제로 패딩을 사용하여 위치 정보를 수집하는 데 사용됩니다.  
CPE(컨볼루션 위치 임베딩)를 사용하여 Vision Transformer의 여러 단계에서 위치 데이터를 통합할 수 있습니다.  
CPE는 특히 self-attention 모듈, FFN(Feed-Forward Network) 또는 두 인코더 레이어 사이에 도입될 수 있습니다.

## Attention Mechanism
ViT 아키텍처의 self-attention 메커니즘은 시퀀스의 엔터티 간의 관계를 명시적으로 표현하는 기능으로 인해 핵심 구성 요소입니다.  
이는 글로벌 상황 정보 측면에서 각 엔터티를 나타내고 이들 간의 상호 작용을 포착하여 다른 항목에 대한 한 항목의 중요성을 계산합니다.  
self-attention 모듈은 입력 시퀀스를 Query, Key, Value라는 세 가지 다른 임베딩 공간으로 변환합니다.  
쿼리 벡터가 포함된 Key-Value 쌍 집합이 입력으로 사용되며 출력 벡터는 값의 가중 합계와 소프트맥스 연산자를 사용하여 계산됩니다.  
여기서 가중치는 채점 함수(방정식 3)로 계산됩니다.

$𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑸,𝑲,𝑽)=𝑠𝑜𝑓𝑡𝑚𝑎𝑥 \left( \frac{𝑸 ⋅ 𝑲^𝑇}{ \sqrt{𝑑_𝑘} } \right)⋅𝑽 \tag{3}$

$𝑸$ : Query, $𝑲^𝑇$ : 전치된 key, $𝑽$ : Value matrix  
$sqrt{𝑑_𝑘}$ : 배율 인수, $𝑑_𝑘$ : key matrix 크기  

### Multi-Head Self-Attention (MSA)
제한된 용량으로 인해 단일 헤드 self-attention 모듈은 몇 가지 위치에만 초점을 맞추고 다른 중요한 위치는 무시할 수 있습니다.  
이는 Self-Attention 블록의 병렬 스택을 사용하여 Self-Attention 레이어의 효율성을 높이는 MSA에 의해 해결됩니다.  
다양한 표현 하위 공간(쿼리, 키 및 값)을 attention 계층에 할당함으로써 MSA는 다양한 시퀀스 요소 간의 다양하고 복잡한 상호 작용을 포착할 수 있습니다.  
MSA는 여러 개의 self-attention 블록으로 구성됩니다.  
각 self-attention 블록과 관련된 쿼리, 키 및 값 하위 공간에 대한 학습 가능한 가중치 행렬이 있습니다.  
그 후 출력은 학습 가능한 매개변수 $𝑊^𝑂$를 사용하여 연결되고 출력 공간에 투영됩니다.  
attention 과정의 수학적 표현은 다음과 같습니다:  

$MSA(𝑸,𝑲,𝑽)$ $=𝐶𝑜𝑛𝑐𝑎𝑡(ℎ𝑒𝑎𝑑_1,ℎ𝑒𝑎𝑑_2,⋅⋅⋅,ℎ𝑒𝑎𝑑_ℎ)⋅𝑾^𝑂$  
$ℎ𝑒𝑎𝑑_𝑖 =𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑸_𝑖,𝑲_𝑖,𝑽_𝑖), \text{ where } 𝑖=1,2,...,ℎ$

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcx6fS9%2FbtrTFmhGL1b%2FliEpkqd2KPl2qwvXHPPYD0%2Fimg.png)

모든 입력 시퀀스에 대해 필터를 동적으로 계산하는 Self-attention의 능력은 컨볼루션 프로세스에 비해 상당한 이점을 제공합니다.  
종종 정적인 컨볼루셔널 필터와 달리 self-attention은 입력 데이터의 특정 컨텍스트에 맞게 조정될 수 있습니다.  
Self-attention은 입력 포인트 수나 순열의 변화에도 강력하므로 불규칙한 입력을 처리하는 데 적합합니다.  
반면에 기존의 컨볼루션 절차는 가변 개체가 포함된 입력을 처리하는 데 적합하지 않으며 2D 이미지와 같은 격자형 구조가 필요합니다.  
Self-attention은 순차 데이터를 모델링하는 강력한 도구이며 자연어 처리와 관련된 작업에 효과적인 것으로 나타났습니다.

## Transformer layers
ViT 인코더는 입력 시퀀스를 처리하는 여러 레이어로 구성됩니다.  
이러한 계층은 계층 정규화, 잔여 연결, FFN(피드포워드 신경망) 및 MSA 메커니즘으로 구성됩니다.  
이러한 레이어는 입력 시퀀스의 복잡한 표현을 학습하기 위해 여러 번 반복되는 통합 블록을 생성하도록 배열됩니다.

### Residual connection
인코더/디코더 블록의 하위 계층은 잔여 링크를 활용하여 성능을 향상하고 정보 흐름을 강화합니다.  
추가 정보로 MSA의 출력 벡터에 원본 입력 위치 임베딩이 추가됩니다.  
그런 다음 잔여 연결 뒤에는 계층 정규화 작업이 수행됩니다(방정식 6).

$𝑿_{𝑜𝑢𝑡𝑝𝑢𝑡} = 𝐿𝑎𝑦𝑒𝑟𝑁𝑜𝑟𝑚(𝑿 + 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑿)) \tag{6}$

### Normalization layer
레이어 정규화에는 Pre-LN(Pre-LN)과 같은 다양한 방법이 있는데, 이 방법은 자주 활용되며 정규화 레이어는 MSA 또는 FFN 이전에 잔여 연결 내부에 배치됩니다.  
트랜스포머 모델의 훈련을 향상시키기 위해 배치 정규화를 포함한 다른 정규화 절차가 제안되었지만 특성 값의 변경으로 인해 효율적이지 않을 수 있습니다.

### Feed-forward network
입력 데이터에서 더 복잡한 속성을 얻기 위해 모델에 트랜스포머별 피드포워드 네트워크(FFN)가 사용됩니다.  
여기에는 두 개의 완전히 연결된 레이어와 레이어 사이의 GELU와 같은 비선형 활성화 함수가 포함되어 있습니다(방정식 7).  

$𝐹𝐹𝑁(𝑿)= 𝑏^{[2]}+𝑾^{[2]}∗𝜎(𝑏^{[1]}+𝑾^{[1]}∗𝑿) \tag{7}$

FFN은 self-attention 모듈 이후의 모든 인코더 블록에서 활용됩니다.  
FFN의 숨겨진 레이어는 일반적으로 2048 차원을 갖습니다.  
이러한 FFN 또는 MLP 레이어는 로컬이며 전역 self-attention 레이어와 병진적으로 동일합니다.

식 7에서 비선형 활성화 함수 GELU는 𝜎로 표현됩니다.  
네트워크의 가중치는 $𝑾^{[1]}$ 및 $𝑾^{[2]}$로 표시되는 반면, $𝑏^{[1]}$ 및 $𝑏^{[2]}$는 레이어별 편향에 해당합니다.  

## Hybrid Vision Transformers (CNN-Transformer Architectures)
컴퓨터 비전 작업 영역에서는 비전 트랜스포머가 인기를 얻었지만 CNN에 비해 여전히 이미지별 유도 편향이 부족합니다.  
CNN에서는 지역성, 병진 등분산 및 2차원 이웃 구조가 전체 모델의 모든 레이어에 뿌리내려 있습니다.  
또한 커널은 인접한 픽셀 간의 상관 관계를 활용하여 좋은 특성을 빠르게 추출할 수 있습니다.  
반면 ViT에서는 이미지가 선형 레이어를 통해 인코더 블록에 공급되는 선형 패치(토큰)로 분할됩니다.  
선형 레이어의 특성상 지역 정보를 추출하는 데 그다지 효과적이지 않습니다.

많은 HVT 설계는 특히 패치 및 토큰화를 위한 이미지 처리 워크플로 시작 시 이미지의 로컬 특성을 캡처할 때 컨볼루션의 효율성에 중점을 두었습니다.  
예를 들어 CvT(Convolutional Vision Transformer)는 컨볼루션 투영을 사용하여 이미지 패치의 공간 정보와 하위 수준 정보를 학습합니다.  
또한 CNN의 공간 다운샘플링 효과를 모방하기 위해 토큰 수를 점진적으로 줄이고 토큰 너비를 늘리는 계층적 레이아웃을 활용합니다.  
마찬가지로 CEIT(Convolution-enhanced Image Transformers)는 컨볼루션 작업을 활용하여 이미지-토큰 모듈을 통해 하위 수준 특성을 추출합니다.  
새로운 시퀀스 풀링 기술은 토큰화를 수행하기 위해 conv-pool-reshape 블록을 통합하는 Compact Convolutional Transformer(CCT)에 의해 제공됩니다.  
또한 처음부터 훈련했을 때 CIFAR10과 같은 소규모 데이터세트에서 약 95%의 정확도를 보여줬는데, 이는 일반적으로 다른 기존 ViT로는 달성하기 어렵습니다.

최근 여러 연구에서는 ViT(Vision Transformers)의 로컬 특성 모델링 능력을 향상시키는 방법을 조사했습니다.  
LocalViT는 깊이별 컨볼루션을 사용하여 로컬 특성을 모델링하는 능력을 향상합니다.  
LeViT는 ViT 아키텍처 초기에 4개의 레이어로 구성된 CNN 블록을 사용하여 추론 시 채널을 점진적으로 늘리고 효율성을 향상시킵니다.  
ResT에서도 유사한 방법을 사용하지만 변동하는 이미지 크기를 관리하기 위해 깊이별 컨볼루션 및 적응형 위치 인코딩이 사용됩니다.

추가 데이터가 없으면 CoAtNets의 고유한 심도 컨볼루션 아키텍처와 상대적 Self-Attention은 뛰어난 ImageNet 상위 1 정확도를 달성합니다.  
더 강력한 크로스 패치 연결을 생성하기 위해 Shuffle Transformer는 셔플 작업을 제공하고 CoaT는 다양한 규모의 토큰 간의 관계를 인코딩하기 위해 깊이별 컨볼루션과 교차 주의를 통합하는 하이브리드 접근 방식입니다.  
또 다른 방법인 "Twins"는 분리 가능한 깊이별 컨볼루션과 상대 조건부 위치 임베딩을 통합하여 PVT를 기반으로 합니다.  
최근에는 하이브리드 아키텍처인 MaxVit이 다축 주목이라는 아이디어를 선보였습니다.  
하이브리드 블록은 MBConv 기반 컨볼루션과 블록별 self-attention 및 그리드별 self-attention으로 구성되며, 이 블록이 여러 번 반복되면 계층적 표현을 생성하고 이미지 생성 및 분할과 같은 작업이 가능합니다.  
블록별 및 그리드별 Attention 레이어는 각각 로컬 및 전역 특성을 추출할 수 있습니다.  
컨볼루션 및 트랜스포머 모델의 장점은 이러한 하이브리드 설계에 결합되도록 고안되었습니다.

# Architectural level modifications in vision transformers
최근 몇 년 동안 비전 트랜스포머 아키텍처에서 다양한 수정이 수행되었습니다.  
이러한 수정은 어텐션 메커니즘, 위치 인코딩, 사전 학습 전략, 아키텍처 변경, 확장성 등에 따라 분류될 수 있습니다.  
비전 트랜스포머 아키텍처는 아키텍처 수정 유형에 따라 크게 5가지 주요 클래스로 분류될 수 있습니다.  
즉, (i) 패치 기반 접근 방식, (ii) Knowledge 이동 기반 접근 방식, (iii) 이동된 윈도우 기반 접근 방식, (iv) 어텐션 기반 접근 방식, (v) 다중 트랜스포머 기반 접근 방식 입니다.  
그러나 비전 트랜스포머에 CNN의 유도 바이어스를 도입하면 성능이 향상되는 것으로 관찰되었습니다.  
이와 관련하여 우리는 하이브리드 비전 트랜스포머를 구조 설계에 따라 세 가지 주요 범주로 더 분류했습니다.

## Patch-based approaches
ViT에서 이미지는 먼저 패치 그리드로 분할된 후 평면화되어 일련의 토큰으로 처리되는 선형 임베딩을 생성합니다.  
위치 임베딩과 클래스 토큰이 이러한 임베딩에 추가된 다음 특성 학습을 위해 인코더에 제공됩니다.  
여러 연구에서는 ViT의 성능을 향상시키기 위해 패치 추출 메커니즘의 다양한 방법을 활용했습니다.  
이러한 메커니즘에는 고정 크기 패치, 동적 패치 및 중첩 패치가 포함됩니다.  
이와 관련하여 여러 아키텍처와 해당 패치 기준에 대해 논의합니다.

### T2T-ViT (Tokens-to-Token Vision Transformer)
T2T-ViT(Tokens-to-Token Vision Transformer)는 고정된 크기와 반복적 접근 방식을 활용하여 패치를 생성합니다.  
제안된 토큰 대 토큰 모듈을 반복적으로 활용하여 이미지에서 패치를 생성합니다.  
생성된 패치는 T2T-ViT 네트워크에 공급되어 최종 예측을 얻습니다.

### TNT-ViT (Transformer in Transformer)
Transformer in Transformer ViT (TNT-ViT)는 다양한 크기와 위치를 가진 객체의 표현을 학습하기 위한 다중 레벨 패치 메커니즘을 제시했습니다.  
먼저 입력 이미지를 패치로 나눈 다음 각 패치를 하위 패치로 다시 나눕니다.  
나중에 아키텍처는 다양한 트랜스포머 블록을 활용하여 패치와 하위 패치 간의 관계를 모델링합니다.  
광범위한 실험을 통해 ImageNet 데이터 세트의 이미지 분류 측면에서 TNT-ViT의 효율성이 나타났습니다.

### DPT (Deformable Patch-based Transformer)
Deformable Patch-based Transformer(DPT)는 DePatch라는 적응형 패치 임베딩 모듈을 제시했습니다.  
트랜스포머의 고정 크기 패치로 인해 의미 정보가 손실되어 시스템 성능에 영향을 미칩니다.  
이와 관련하여 제안된 DPT의 DePatch 모듈은 다양한 크기와 강력한 의미 정보를 갖는 패치를 얻기 위해 적응형 방식으로 이미지를 분할합니다.

### CrowdFormer
Yang과 공동 저자는 군중 계산을 위한 ViT 아키텍처인 CrowdFormer를 개발했습니다.  
제안된 아키텍처는 오버랩 패칭 트랜스포머 블록을 활용하여 군중의 전역 상황 정보를 캡처합니다.  
다양한 스케일과 하향식 방식으로 이미지를 고려하기 위해 고정 크기 패치 대신 슬라이딩 윈도우를 사용하여 겹치는 패치를 추출하는 오버랩 패치 레이어가 활용됩니다.  
이러한 겹치는 패치는 효과적인 군중 계산을 위해 상대적인 상황 정보를 유지하는 경향이 있습니다.

## Knowledge transfer-based approaches
이 범주에는 지식 전달(지식 증류) 접근 방식을 활용하는 비전 트랜스포머 아키텍처가 포함됩니다.  
여기에는 교사가 학생에게 지식을 전달하는 것과 마찬가지로 더 큰 네트워크에서 더 작은 네트워크로 지식을 전달하는 작업이 포함됩니다.  
교사 모델은 일반적으로 충분한 학습 능력을 갖춘 복잡한 모델인 반면 학생 모델은 더 간단합니다.  
지식 증류의 기본 아이디어는 교사 모델의 독특한 특성을 획득하고 통합하는 데 있어 학생 모델을 촉진하는 것입니다.  
이는 작은 ViT 모델이 큰 모델보다 더 효율적으로 배포될 수 있으므로 계산 리소스가 제한된 작업에 특히 유용할 수 있습니다.

![](https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-33380-4_13/MediaObjects/548822_1_En_13_Fig1_HTML.png)

### DeiT (Data-efficient Image Transformers)
Deit는 다양한 작업에서 경쟁력 있는 성능을 보여준 ViT의 더 작고 효율적인 버전입니다.  
교사에게는 사전 훈련된 ViT 모델을 사용하고 학생에게는 더 작은 버전을 사용합니다.  
일반적으로 지도 학습과 비지도 학습이 결합되어 사용되며 교사 네트워크가 학생 네트워크를 감독하여 유사한 결과를 생성합니다.  
DeiT의 빠른 추론 시간과 제한된 계산 리소스 외에도 학생 모델은 훈련 데이터를 단순히 기억하는 것이 아니라 데이터에서 가장 중요한 특성과 패턴을 캡처하는 방법을 학습했기 때문에 일반화 성능도 향상되었습니다.

![](https://velog.velcdn.com/images/hseop/post/d06b3d58-f8e6-4867-90c4-dbda1d59bf5e/image.png)

### TaT (Target aware Transformer)
TaT(Target Aware Transformer)는 일대다 관계를 활용하여 교사에서 학생 네트워크로 정보를 교환했습니다.  
특성 맵은 먼저 여러 개의 패치로 분할된 다음 각 패치에 대해 모든 공간 영역 간의 상관 관계를 사용하는 대신 모든 교사의 특성이 모든 학생 특성으로 전송되었습니다.  
그런 다음 패치 내부의 모든 특성을 단일 벡터로 평균화하여 지식 전달을 계산적으로 효율적으로 만들었습니다.

![](https://arxiv.org/html/2205.10793v2/x2.png)

### TinyViT (Tiny Vision Transformer)
Wu et al.은 TinyViT로 알려진 새로운 아키텍처와 함께 빠른 증류 방법을 제안했습니다.  
그들의 주요 개념은 사전 훈련 중에 큰 사전 훈련된 모델의 학습된 특성을 작은 모델에 전달하는 것이었습니다(그림 4).  
강사 모델의 출력 로짓은 메모리와 계산 리소스를 절약하기 위해 미리 디스크에 인코딩된 데이터 증가와 함께 축소 및 저장되었습니다.  
그런 다음 학생 모델은 디코더를 사용하여 저장된 데이터 확대를 재구성하고 지식은 두 모델이 독립적으로 훈련된 출력 로짓을 통해 전송됩니다.  
결과는 대규모 테스트 세트에서 TinyViT의 효율성을 입증했습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_04.png)

## Shifted window-based approaches
여러 ViT 아키텍처는 성능 향상을 위해 전환된 창 기반 접근 방식을 채택했습니다.  
이 접근법은 Liu et al.에 의해 Swin Transformer에서 처음 소개되었습니다.  
Swin Transformer는 ViT와 유사한 아키텍처를 가지고 있지만 그림 5와 같이 이동된 윈도우 방식을 사용합니다.  
이는 겹치지 않는 각 로컬 창 내에서 계산하여 self-attention 계산을 제어하는 동시에 효율성을 향상시키기 위해 창 간 연결을 제공합니다.  
이는 이동된 창 기반 셀프 어텐션을 두 개의 연속 Swin Transformer 블록으로 구현함으로써 달성됩니다.  
첫 번째 블록은 일반 창 기반 셀프 어텐션을 사용하고, 두 번째 블록은 해당 창을 이동하고 일반 창 기반 셀프 어텐션을 다시 적용합니다.  
창 이동 이면의 아이디어는 창 간 연결을 활성화하는 것입니다.  
이는 네트워크가 글로벌 관계를 모델링하는 능력을 향상시키는 데 도움이 될 수 있습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_05.png)

Song et al. 은 다중 규모에서 순환 이동 창 기반 attention을 활용하는 시각적 객체 추적을 위한 새로운 ViT 아키텍처를 제안했습니다.  
이 접근 방식은 window attention에 대한 pixel attention을 향상시키고 다양한 규모의 주의를 집계하기 위해 창 간 다중 규모 주의를 가능하게 합니다.  
이를 통해 추적 개체의 무결성이 보장되고 대상 개체에 대해 가장 미세한 일치 항목이 생성됩니다.  
또한 순환 이동 기술은 위치 정보로 창 샘플을 확장하므로 정확도와 계산 효율성이 향상됩니다.  
위치 정보를 어텐션 메커니즘에 통합함으로써 모델은 시간이 지남에 따라 객체 위치의 변화를 더 효과적으로 처리하고 객체를 보다 효과적으로 추적할 수 있습니다.  
전반적으로 제안된 아키텍처는 ViT 기반 모델을 사용하여 시각적 객체 추적의 정확성과 효율성을 향상시키는 유망한 결과를 보여주었습니다.

## Attention-based approaches
성능을 향상시키기 위해 self-attention 모듈을 수정하는 다양한 ViT(Vision Transformer) 아키텍처가 제안되었습니다.  
이러한 모델 중 일부는 조밀한 전역 어텐션 메커니즘을 활용하는 반면, 다른 모델은 희소 어텐션 메커니즘을 활용하여 공간 정보 없이 이미지에서 전역 수준 종속성을 캡처합니다.  
이러한 유형의 어텐션 메커니즘은 계산 비용이 많이 드는 것으로 알려져 있습니다.  
성능 및 계산 복잡성 측면에서 어텐션 모듈을 개선하기 위해 많은 작업이 수행되었습니다.

### CaiT (Class-Attention in Image Transformers)
Touvron et al. 은 심층 트랜스포머의 성능을 향상시키기 위한 새로운 접근 방식을 도입했습니다.  
CaiT라는 아키텍처에는 self-attention 모듈과 class attention 모듈이 포함되어 있습니다.  
self-attention 모듈은 일반 ViT 아키텍처와 동일하지만 초기 레이어에 클래스 토큰(클래스 정보)이 추가되지 않습니다.  
클래스 임베딩은 나중에 아키텍처의 클래스 어텐션 모듈에 추가됩니다.  
그들의 접근 방식은 몇 가지 매개 변수를 사용하여 좋은 결과를 보여주었습니다.

### DAT (Deformable attention transformer)
Xia와 공동 저자는 보다 신뢰할 수 있는 영역에 초점을 맞추기 위해 데이터 기반 어텐션 메커니즘을 제안했습니다.  
그들의 아키텍처는 각 단계마다 로컬 어텐션 레이어와 변형 가능한 어텐션 레이어가 있는 모듈식 디자인을 가지고 있습니다.  
제안된 DAT 아키텍처는 벤치마크 데이터 세트에서 모범적인 성능을 보여주었습니다.

### SeT (patch-based Separable Transformer)
Sun et al.은 ViT 아키텍처에서 두 가지 서로 다른 어텐션 모듈을 사용하여 이미지의 글로벌 관계를 완전히 포착했습니다(그림 6).  
그들은 초기 레이어에서 로컬 상호 작용을 학습하기 위해 픽셀 단위의 어텐션 모듈을 제안했습니다.  
나중에 그들은 글로벌 수준의 정보를 추출하기 위해 패치별 어텐션 모듈을 활용했습니다.  
SeT는 ImageNet 및 MS COCO 데이터 세트를 포함한 여러 데이터 세트에서 다른 방법보다 우수한 결과를 보여주었습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_06_a.png)

## Multi-transformer-based approaches
많은 접근 방식에서는 multi-scale 특성이 필요한 다양한 작업의 성능을 향상시키기 위해 아키텍처에서 여러 ViT를 활용했습니다.  
이 섹션에서는 이러한 유형의 다중 트랜스포머 기반 ViT 아키텍처에 대해 설명합니다.

### CrossViT (Cross Vision Transformer)
Chen과 공동 저자들은 CrossViT라는 이름의 이중 분기를 갖는 ViT 아키텍처를 제안했습니다.  
제안된 모델의 주요 혁신은 CrossViT가 도메인 관련성이 높은 특성을 생성할 수 있도록 다양한 크기의 이미지 패치를 조합한 것입니다.  
더 작은 패치 토큰과 더 큰 패치 토큰은 계산 복잡성이 다양한 두 개의 별도 분기를 사용하여 처리됩니다.  
두 가지 분기는 효율적인 교차 관심 모듈을 사용하여 여러 번 함께 융합됩니다.  
이 모듈은 비패치 토큰을 생성하여 지점 간 지식 이전을 가능하게 합니다.  
이러한 과정을 통해 Attention Map 생성은 2차형이 아닌 선형적으로 이루어집니다.  
이로 인해 CrossViT는 2차 주의를 사용하는 다른 모델보다 계산 효율성이 더 높아졌습니다.

### Dual-ViT (Dual Vision Transformer)
Dual-ViT(Dual Vision Transformer)는 self-attention 메커니즘의 계산 비용을 줄이는 새로운 ViT 아키텍처입니다.  
이 아키텍처는 두 가지 개별 경로를 활용하여 글로벌 및 로컬 수준 정보를 캡처합니다.  
의미론적 분기는 코스 세부 사항을 학습하는 반면, 픽셀 경로는 이미지에서 더 미세한 세부 사항을 캡처합니다.  
이 두 분기는 모두 통합되어 병렬로 학습됩니다.  
제안된 DualViT는 기존의 다른 모델에 비해 더 적은 수의 매개변수를 사용하여 ImageNet 데이터세트에서 좋은 결과를 보여주었습니다.

### MMViT (Multiscale Multiview Vision Transformer)
MMViT(Multiscale Multiview Vision Transformers)는 다중 스케일 특성 맵과 다중 뷰 인코딩을 트랜스포머 모델에 통합합니다.  
MMViT 모델은 여러 특성 추출 단계를 활용하여 다양한 해상도에서 입력의 여러 보기를 병렬로 처리합니다.  
각 규모 단계에서 교차 어텐션 블록을 활용하여 다양한 관점에서 데이터를 병합합니다.  
이 접근 방식을 사용하면 MMViT 모델이 여러 해상도에서 입력의 고차원 표현을 얻을 수 있어 복잡하고 강력한 특성 표현이 가능해집니다.

### MPViT (Multi-Path Vision Transformer)
MPViT는 다중 규모 패치 기술과 다중 경로 기반 ViT 아키텍처를 활용하여 다양한 규모의 특성 표현을 학습합니다.  
그들이 제안한 다중 규모 패치 기술은 CNN을 활용하여 다양한 규모의 특성 맵을 생성합니다(그림 7).  
나중에 그들은 다중 스케일 패치 임베딩을 처리하기 위해 여러 트랜스포머 인코더를 활용합니다.  
마지막으로 각 인코더의 출력을 집계하여 집계된 출력을 생성합니다.  
제안된 MPViT는 ImageNet 데이터세트에 대한 기존 접근 방식과 비교하여 우수한 결과를 보여주었습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_07.png)

## Details and taxonomy of HVTs (CNN-Transformer architectures)
하이브리드 비전 트랜스포머는 CNN과 트랜스포머 아키텍처의 장점을 결합하여 이미지의 로컬 패턴과 글로벌 컨텍스트를 모두 캡처하기 위한 모델을 만듭니다.  
하이브리드 비전 트랜스포머는 여러 이미지 관련 작업에서 유망한 결과로 인해 연구 커뮤니티에서 귀중한 관심을 얻었습니다.  
연구자들은 CNN과 트랜스포머를 병합하는 다양한 접근 방식을 활용하여 이 분야에서 다양한 아키텍처를 제안했습니다.  
이러한 접근 방식에는 일부 CNN 레이어를 트랜스포머 블록으로 교체하거나, CNN에 다중 어텐션 메커니즘을 도입하거나, CNN을 사용하여 로컬 특성 및 트랜스포머를 추출하여 장거리 종속성을 캡처하는 등이 포함되지만 이에 국한되지는 않습니다.  
이와 관련하여 우리는 컨볼루션 작업과 비전 트랜스포머 아키텍처의 통합 패턴을 기반으로 몇 가지 하위 범주를 정의합니다.  
여기에는 (1) 초기 계층 통합, (2) 측면 계층 통합, (3) 순차 통합, (4) 병렬 통합, (5) 블록 통합, (6) 계층적 통합, (7) 어텐션 기반 통합, (8) 채널 부스팅 통합을 포함합니다.

### Early-layer integration
이미지의 장거리 종속성은 비전 트랜스포머로 잘 포착되지만, 귀납적 편향이 없기 때문에 이를 훈련하려면 많은 데이터가 필요합니다.  
반면에 CNN은 이미지 관련 귀납적 편향을 내재하고 이미지에 존재하는 높은 수준의 상관관계를 로컬로 포착합니다.  
따라서 연구자들은 CNN과 트랜스포머의 이점을 결합하기 위해 하이브리드 비전 트랜스포머를 설계하는 데 중점을 두고 있습니다.  
트랜스포머 아키텍처에서 컨볼루션과 어텐션을 융합하는 가장 최적의 방법을 찾기 위해 많은 작업이 수행되었습니다.  
CNN은 아키텍처의 지역성을 통합하기 위해 다양한 수준에서 활용될 수 있습니다.  
다양한 연구에서는 먼저 로컬 패턴을 캡처한 다음 장거리 종속성을 학습하여 이미지에 대해 보다 최적화된 로컬 및 글로벌 관점을 갖는 것이 유익하다는 아이디어를 제안했습니다.

#### Hybrid ViT
최초의 비전 트랜스포머(ViT)는 2020년 Dosovitskiy et al.에 의해 제안되었습니다.  
그들의 작업에서 그들은 이미지 패치를 토큰 시퀀스로 간주하고 이를 트랜스포머 기반 네트워크에 공급하여 이미지 인식 작업을 수행하는 아이디어를 제안했습니다.  
논문에서 그들은 ViT의 하이브리드 버전을 제시함으로써 하이브리드 비전 트랜스포머의 기반을 마련했습니다.  
하이브리드 아키텍처에서는 원시 이미지 패치 대신 CNN 특성 맵에서 입력 시퀀스를 얻었습니다.  
입력 시퀀스는 특성 맵을 공간적으로 평면화하여 생성되었으며 패치는 1×1 필터를 사용하여 생성되었습니다.  
그들은 ResNet50 아키텍처를 활용하여 ViT에 대한 입력으로 특성 맵을 얻었습니다.  
또한 특성 맵 추출을 위한 최적의 중간 블록을 식별하기 위해 광범위한 실험을 수행했습니다.

#### DETR (Detection Transformer)
Carion et al.은 2020년에 기존 이미지에서 객체 감지를 수행하기 위한 DETR(감지 트랜스포머)을 제안했습니다.  
제안된 엔드투엔드 접근 방식에서는 처음에 CNN을 활용하여 입력을 ViT 아키텍처에 공급하기 전에 처리했습니다.  
CNN 백본의 특성 맵은 고정 크기 위치 임베딩과 결합되어 ViT 인코더에 대한 입력을 생성했습니다.  
ViT 디코더의 출력은 최종 예측을 위해 피드포워드 네트워크에 공급되었습니다.  
DETR은 Faster R-CNN과 같은 다른 혁신적인 탐지 모델과 비교할 때 더 나은 성능을 보여주었습니다.  
그들의 자세한 아이디어는 그림 8에 나와 있습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_08.png)

#### LeViT (LeNet-based Vision Transformer)
Graham et al.은 2021년에 하이브리드 ViT "LeViT"를 제안했습니다.  
그들의 모델에서는 처음에 입력 처리를 위해 컨볼루션 레이어를 활용했습니다.  
제안된 아키텍처는 CNN과 ViT 아키텍처의 MSA를 결합하여 입력 이미지에서 로컬 및 전역 특성을 추출합니다.  
LeViT 아키텍처는 처음에 이미지 해상도를 줄이고 로컬 특성 표현을 얻기 위해 4계층 CNN 모델을 활용했습니다.  
그런 다음 이러한 표현은 MLP 및 Attention 레이어를 갖춘 ViT에서 영감을 받은 다단계 아키텍처에 공급되어 출력을 생성했습니다.

#### CPVT (Conditional Positional Encodings for Vision Transformers)
CPVT는 Chu et al.이 제안했습니다.  
그들의 작업에서 그들은 ViT의 성능을 향상시키기 위해 조건부 위치 임베딩의 새로운 체계를 고안했습니다(그림 9).  
이와 관련하여 그들은 위치 임베딩을 보다 지역적이고 변환적으로 동등하게 만들기 위해 깊이별 컨볼루션을 활용하는 위치 인코딩 생성기(PEG / Positional Encoding Generators)를 제안했습니다.  
그들은 또한 더 많은 위치 정보를 아키텍처에 통합하기 위해 PEG를 활용하는 제안된 방식을 기반으로 ViT를 개발했으며 좋은 결과를 보여주었습니다.  
또한 클래스 토큰 대신 최종 MLP 레이어 위의 전역 평균 풀링 레이어를 사용하면 성능이 향상되는 것으로 나타났습니다.  
Xiao et al.은 그들의 연구에서는 ViT의 초기 레이어에서 CNN 레이어를 활용하면 성능이 향상될 수 있다고 추정했습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_09.png)

### Lateral-layer integration
마지막 선형 레이어 대신 또는 후처리 레이어와 같이 트랜스포머 네트워크 끝의 CNN 레이어 또는 블록을 사용하는 모델이 이 범주에 속합니다.

#### DPT (Dense Prediction Transformer)
Ranftl et al.은 자연 영상 분할을 위한 조밀한 예측 트랜스포머 “DPT”를 제안했습니다.  
DPT는 ViT를 인코더로, CNN을 디코더로 사용하는 인코더-디코더 기반 설계를 가지고 있습니다.  
백본 아키텍처를 통해 글로벌 관점과 장기적인 종속성을 포착했습니다.  
학습된 전역 표현은 CNN을 활용하여 얻은 이미지 기반 임베딩으로 디코딩되었습니다.  
ViT 기반 인코더의 출력은 밀도 높은 예측을 수행하기 위해 다양한 수준에서 디코딩되었습니다.

#### LocalViT (Local Vision Transformer)
Li et al.은 연구에서 이미지 분류를 위해 ViT 아키텍처에 지역성을 통합했습니다.  
LocalViT의 아키텍처는 이미지의 전역 수준 특성을 캡처하는 데 특화된 MSA 모듈을 갖춘 기존 ViT와 같습니다.  
ViT 인코더의 피드포워드 네트워크는 Attention 모듈에서 학습된 인코딩의 입력을 받아 최종 예측을 수행합니다.  
LocalVit은 깊이별 컨볼루션을 사용하여 로컬 정보를 아키텍처에 통합함으로써 FFN을 수정합니다.

### Sequential integration
이 범주에서는 일부 순차적 통합을 수행하여 ViT 아키텍처에서 CNN의 이점을 활용한 인기 있는 하이브리드 ViT 중 일부에 대해 설명합니다.

#### CoAtNet (Convolution and Attention networks)
Dai et al.은 단일 아키텍처에서 컨볼루션과 어텐션 메커니즘을 병합하여 일반화 및 용량을 늘리는 가장 최적이고 효율적인 방법을 찾기 위해 광범위한 연구를 수행했습니다.  
이에 대해 그들은 여러 개의 컨볼루션 블록과 트랜스포머 블록을 수직으로 쌓아 CoAtNet을 도입했습니다.  
컨볼루션 블록의 경우 깊이별 컨볼루션을 기반으로 하는 MBConv 블록을 사용했습니다.  
그들의 연구 결과에 따르면 두 개의 컨볼루션 블록과 두 개의 트랜스포머 블록을 순차적으로 적층하면 효율적인 결과가 나타납니다.

![](https://user-images.githubusercontent.com/67839539/138133065-337bb5ac-3dca-4ce8-af51-990c5ff23316.png)

#### CMT (CNNs Meet Transformers)
성공적인 성능에도 불구하고 ViT는 세 가지 주요 문제에 직면합니다.  
a) 로컬 이웃의 상관 관계를 고려하여 낮은 수준의 특성을 캡처할 수 없음,  
b) MSA 메커니즘으로 인해 계산 및 메모리 소비 측면에서 비용이 많이 들음,  
c) 고정된 크기의 입력 토큰, 임베딩 입니다.  
이러한 문제를 극복하기 위해 2021년 이후 CNN과 ViT의 하이브리드화 붐이 일고 있습니다.  
Guo et al.은 2021년에 CMT(CNNs Meet Transformers)라는 하이브리드 ViT를 제안했습니다.  
CNN에서 영감을 받은 CMT는 초기 스템 블록과 CNN 레이어 및 CMT 블록의 순차적 스택으로 구성됩니다.  
설계된 CMT 블록은 ViT 아키텍처에서 영감을 얻었으므로 기존 MSA 대신 경량 MSA 블록을 포함했으며 MLP 계층은 IRFFN(Inverted Residual Feed-Forward Network)으로 대체되었습니다.  
또한 CMT 블록에는 LPU(Local Perception Unit)를 추가하여 네트워크의 표현 용량을 늘립니다.  
아키텍처는 그림 10에 나와 있습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_10_a.png)

#### BoTNet (Bottleneck Transformers)
컨볼루션 레이어는 이미지의 많은 구조 요소의 주요 구성 요소인 하위 수준 특성을 캡처하므로 Srinivas et al.은 CNN과 ViT의 이점을 모두 활용하기 위해 하이브리드 ViT, BoTNet(시각 인식을 위한 병목 트랜스포머)을 도입했습니다.  
BoTNet의 아키텍처는 Attention 메커니즘이 마지막 세 블록에 통합된 ResNet 블록의 순차적 조합일 뿐입니다.  
ResNet 블록에는 2개의 1×1 컨볼루션과 1개의 3×3 컨볼루션이 포함되어 있습니다.  
MSA는 로컬 특성 외에도 장기적인 종속성을 캡처하기 위해 3×3 컨볼루션 대신 추가되었습니다.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb6GMsp%2FbtrbEOCer3m%2F0zJCk0ymzgAwa4vPywbx11%2Fimg.png)

### Parallel integration
이 범주에는 CNN과 트랜스포머 아키텍처를 병렬로 사용하고 최종적으로 예측을 결합하는 하이브리드 비전 트랜스포머가 포함됩니다.

#### Conformer (Convolution-augmented Transformer)
2021년 Peng et al.은 자연 이미지에서 시각적 인식을 수행하는 연구를 수행했습니다.  
이에 그들은 Conformer라는 아키텍처를 제안했다.  
ViT의 인기로 인해 Conformer의 아키텍처도 ViT를 기반으로 했습니다.  
네트워크의 인식 능력을 향상시키기 위해 CNN의 이점과 다중 헤드 셀프 어텐션 메커니즘을 통합했습니다.  
하이브리드 ViT인 Conformer에는 두 개의 별도 분기, 즉 로컬 인식을 캡처하는 CNN 분기와 글로벌 수준 특성을 캡처하는 트랜스포머 분기가 포함되어 있습니다.  
각 분기가 로컬-글로벌 컨텍스트를 인식하도록 하기 위해 CNN 분기에서 트랜스포머 분기로 후속 연결이 구축되었습니다.  
최종 예측은 CNN 분류기와 트랜스포머 분류기로부터 얻어졌습니다.  
교차 엔트로피 손실 함수는 각 분류기를 훈련하는 데 사용되었습니다.  
Conformer는 DeiT, VIT 등 다른 우수한 ViT 아키텍처보다 더 나은 성능을 보여주었습니다.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fm9Pei%2FbtrfaqdqUES%2F0sXbsoInGq3E2Z2XQFaed0%2Fimg.png)

#### Mobile-Former (MobileNet-based Transformer)
Chen et al.은 CNN과 트랜스포머를 위한 두 가지 다른 경로를 갖춘 동시 하이브리드 ViT 아키텍처를 제안했습니다.  
다른 하이브리드 ViT와 마찬가지로 Mobile-Former는 CNN 모델을 사용하여 공간 상관 관계를 학습하고 트랜스포머를 사용하여 이미지의 장기적인 종속성을 캡처하여 로컬 정보와 글로벌 정보를 융합했습니다.  
CNN 아키텍처는 매개변수 수가 줄어든 역 잔차 블록을 사용하는 MobileNet을 기반으로 했습니다.  
두 가지 간의 정보는 연결을 사용하여 동기화되었으며, 이를 통해 CNN 경로는 글로벌 정보를 인식하고 트랜스포머는 로컬 정보를 인식하게 되었습니다.  
두 가지 분기의 연결된 출력과 풀링 레이어는 최종 예측을 위해 2레이어 분류기에 공급되었습니다.  
그림 11은 자세한 아키텍처를 보여줍니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_11_a.png)

#### BossNAS (Block-wisely Self-supervised Neural Architecture Search)
Li et al.은 하이브리드 아키텍처를 평가하기 위해 검색 공간(HyTra)을 개발했으며 각 블록을 별도로 훈련해야 한다고 조언했습니다.  
HyTra 검색 공간 내의 모든 레이어에서 CNN과 다양한 해상도의 트랜스포머 블록을 병렬로 자유롭게 선택 가능한 형태로 활용했습니다.  
이 광범위한 검색 영역에는 공간 규모가 점차 작아지는 기존 CNN과 콘텐츠 길이가 고정된 순수 트랜스포머가 포함됩니다.

![](https://user-images.githubusercontent.com/61453811/112087643-4a874b00-8bc9-11eb-9440-757429034d81.png)

### Hierarchical integration
#### MaxViT (Multi-Axis Attention-based Vision Transformer)
MaxViT는 Tu 등이 "Multi-Axis Attention Based Vision Transformer"라는 논문에서 소개한 ViT(Vision Transformer) 아키텍처의 변형입니다.  
이는 차단된 로컬 주의와 확장된 전역 주의로 구성된 다축 어텐션 메커니즘을 도입했습니다.  
이는 이전 아키텍처에 비해 효율적이고 확장 가능한 어텐션 메커니즘임이 입증되었습니다.  
MBConv 기반의 convolution과 Multi-Axis 기반 attention으로 구성된 새로운 하이브리드 블록이 기본 요소로 도입되었습니다.  
기본 하이브리드 블록은 분류, 객체 감지, 분할 및 생성 모델링에 사용할 수 있는 CNN 기반 백본과 유사한 계층적 백본을 얻기 위해 여러 단계에 걸쳐 반복되었습니다.  
MaxViT는 초기 단계를 포함하여 전체 네트워크에 걸쳐 로컬 및 글로벌로 볼 수 있습니다.

![](https://github.com/google-research/maxvit/raw/main/doc/maxvit_arch.png)

#### CvT (Convolutional Vision Transformer)
CvT는 2021년 Wu et al.에 의해 도입되었습니다.  
CvT의 아키텍처에는 계층적 프레임워크를 구성하기 위한 CNN과 같은 여러 단계가 포함되어 있습니다.  
그들은 두 가지 방법으로 아키텍처에 컨볼루션을 추가했습니다.  
처음에는 토큰 시퀀스를 추출하기 위해 컨볼루셔널 토큰 임베딩을 사용했는데, 이는 네트워크에 지역성을 통합했을 뿐만 아니라 시퀀스 길이를 점차적으로 단축했습니다.  
둘째, 그들은 인코더 블록의 각 self-attention 블록 앞의 선형 투영을 대체하기 위해 깊이별 분리 가능한 컨볼루션을 사용하는 컨볼루션 투영을 제안했습니다.  
CvT는 이미지 인식에 대한 다른 접근 방식보다 성능이 뛰어났습니다.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FDWpH2%2FbtrJRtkcYZK%2FGkSd7ZN7CXXXCfVKE78WpK%2Fimg.png)

#### Visformer (Vision-Friendly Transformer)
Visformer는 비전 친화적인 Transformer로 소개되었습니다.  
효율적인 성능을 위해 모듈식 설계를 적용했습니다.  
아키텍처에는 ViT 네트워크에 대한 몇 가지 수정 사항이 있었습니다.  
Classification Token 대신 Global Average Pooling을 채용하였고, 2020년에는 Batch Normalization을 사용하였다.  
또한 각 단계에서는 Self Attention 대신 Convolution을 활용하였으나 마지막 2단계에서만 Attention과 Conveolution을 채택하였다.  
컨볼루션 블록은 ResNeXt에서 영감을 받았습니다.

![](https://production-media.paperswithcode.com/social-images/shwPstfsrrpytunQ.png)

#### ViTAE (Vision Transformer Advanced by Exploring intrinsic Inductive Bias)
저자는 두 가지 기본 셀 유형(그림 12 참조), 즉 reduction cells (RC) 과 normal cells (NC)을 결합한 ViTAE라는 새로운 비전 트랜스포머를 제안했습니다.  
RC는 입력 이미지를 축소하고 이를 풍부한 다중 규모 상황별 토큰에 포함하는 데 사용되는 반면, NC는 토큰 시퀀스 내에서 로컬 및 장기 종속성을 동시에 모델링하는 데 사용됩니다.  
이 두 가지 유형의 셀의 기본 구조도 유사하며 병렬 어텐션 모듈, 컨볼루션 레이어 및 FFN으로 구성됩니다.  
RC는 피라미드 축소 모듈에서 다양한 팽창률을 갖는 여러 컨볼루션을 활용하여 토큰에 상황 정보를 포함합니다.  
저자는 또한 이전 방법보다 더 나은 성능을 보여주는 보다 최적화된 버전인 ViTAEv2를 제시했습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_12_a.png)

#### ConTNet (Convolution-Transformer Network)
이 분야에서 직면한 과제를 해결하기 위해 컴퓨터 비전 작업을 위해 새로운 ConTNet(Convolution-Transformer Network)이 제안되었습니다.  
ConTNet은 여러 ConT 블록을 쌓아서 구현됩니다(그림 13 참조).  
ConT 블록은 표준 트랜스포머 인코더(STE)를 컨볼루션 계층과 유사한 독립 구성 요소로 처리합니다.  
구체적으로, 특성 맵은 동일한 크기의 여러 패치로 분할되고 각 패치는 (슈퍼) 픽셀 시퀀스로 평면화되어 STE에 입력됩니다.  
패치 임베딩을 재구성한 후 결과 특성 맵은 다음 컨볼루셔널 레이어 또는 STE 모듈로 전달됩니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_13_a.png)

#### MoCoViT (Mobile Convolutional Vision Transformer)
본 논문에서 저자는 모바일 컨볼루셔널 네트워크와 트랜스포머 설계의 장점을 결합하여 성능과 효율성을 향상시키는 MoCoViT(Mobile Convolutional Vision Transformer)를 소개합니다.  
MoCoViT의 트랜스포머 블록을 가볍고 모바일 장치에 적합하게 만들기 위해  1) self-attention 모듈과 2) 피드포워드 네트워크의 두 가지 주요 수정 작업을 수행했습니다.  
이들은 어텐션 맵 계산을 최적화하기 위해 Branch Sharing 기술을 사용하는 MoSA(Mobile Self-Attention) 모듈을 도입했습니다.  
둘째, MoFFN(모바일 피드 포워드 네트워크)은 트랜스포머에서 모바일 MLP 역할을 하여 계산을 크게 줄입니다.

![](https://github.com/smitheric95/MoCoViT-PyTorch/raw/main/figures/figure2.png)

### Attention-based integration
이 섹션에서는 지역성을 통합하기 위해 어텐션 메커니즘에 CNN을 활용한 HVT 아키텍처에 대해 설명합니다.

#### EA-AA-ResNet (Evolving Attention with Residual Convolutions)
토큰 간의 기본 종속성을 캡처하는 데 있어 독립적인 self-attention 레이어의 일반화가 제한되어 있기 때문에 Wang et al.은 컨볼루셔널 모듈을 추가하여 어텐션 메커니즘을 확장했습니다.  
구체적으로 그들은 Evolving Attention(EA)이라는 이전 레이어에서 상속된 지식을 활용하여 각 레이어의 어텐션 맵을 일반화하기 위해 잔여 연결이 있는 컨볼루셔널 유닛을 채택했습니다.  
제안된 EA-AA-ResNet 아키텍처는 다양한 계층에 걸쳐 주의 지도를 연결하고 컨볼루션 모듈을 사용하여 어텐션의 일반적인 패턴을 학습함으로써 어텐션 메커니즘을 확장합니다.

![](https://ar5iv.labs.arxiv.org/html/2102.12895/assets/x2.png)

#### ResT (ResNet Transformer)
ViT를 ResNet 백본과 결합하여 글로벌 및 로컬 특성을 모두 효과적으로 캡처할 수 있는 하이브리드 아키텍처입니다.

![](https://pbs.twimg.com/media/E2sCJnNX0AMPjpI.png:large)

#### CeiT (Convolution-Enhanced Image Transformer)
CeiT는 Yuan et al.이 2021년에 "컨볼루션 디자인을 ViT에 통합(Incorporating Convolution Designs into Visual Transformers)"이라는 논문에서 제안했습니다.  
제안된 CeiT는 낮은 수준의 특성을 추출하고, 지역성을 캡처하고, 장거리 종속성을 학습하는 데 있어 CNN과 ViT의 이점을 결합했습니다.  
CeiT에서는 기존 ViT 아키텍처에서 세 가지 주요 발전을 이루었습니다.  
그들은 패치 추출 방식인 MLP 레이어를 수정하고 ViT 아키텍처 위에 마지막 레이어를 추가했습니다.  
패치 추출을 위해 그들은 CNN 기반 블록을 활용하여 입력을 처리하는 I2T(Image-to-Tokens) 모듈을 제안했습니다.  
원시 입력 이미지를 활용하는 대신 초기 컨볼루션 블록에서 학습된 하위 수준 특성을 사용하여 패치를 추출했습니다.

I2T는 ViT에서 CNN의 이점을 최대한 활용하기 위해 아키텍처에 컨볼루션, 최대 풀링 및 배치 정규화 레이어를 포함했습니다. 그들은 ViT 인코더의 기존 MLP 계층 대신 LeFF(Locally-enhanced Feed-Forward) 계층을 활용했으며, 여기서 깊이별 컨볼루션을 활용하여 더 많은 공간적 상관 관계를 포착했습니다. 또한 ViT의 다양한 계층의 출력을 체계적으로 결합하기 위해 마지막 클래스 LCA(Token Attention) 계층이 고안되었습니다. CeiT는 여러 이미지 및 장면 인식 데이터 세트(ImageNet, CIFAR 및 Oxford-102 포함)에서 유망한 결과를 보여줬을 뿐만 아니라 ViT에 비해 계산 효율성도 뛰어납니다.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbrFARZ%2FbtrbodVpDxJ%2Fc16qBD7qTgjgOgprQ3srPk%2Fimg.png)

### Channel boosting-based integration
채널 부스팅(CB)은 딥러닝에서 CNN 모델의 표현 학습 능력을 높이기 위해 사용되는 아이디어입니다.  
CB에서는 원본 채널 외에도 전이 학습 기반 보조 학습기를 사용하여 부스트 채널을 생성하여 이미지에서 다양하고 복잡한 패턴을 캡처합니다.  
CB 기반 CNN(CB-CNN)은 다양한 비전 관련 작업에서 뛰어난 성능을 보여왔습니다.  
Liaquat et al.의 연구에서 그들은 CB 기반 HVT 아키텍처를 제안했습니다.  
CB-HVT에서는 CNN과 ViT 기반 보조 학습자를 활용하여 강화된 채널을 생성했습니다.  
CNN 기반 채널은 이미지 패턴에서 로컬 수준의 다양성을 포착한 반면, Pyramid Vision Transformer(PVT) 기반 채널은 전역 수준의 상황 정보를 학습했습니다.  
해당 아키텍처의 개요는 그림 14에 나와 있습니다.  
저자는 림프구 평가 데이터 세트에서 CB-HVT를 평가했으며 합리적인 성능을 보여주었습니다.

![](https://wikidocs.net/images/page/236673/Fig_TR_CV_Survey_14.png)
 
# Applications of ViT and HVTs
비전 트랜스포머와 하이브리드 비전 트랜스포머는 이미지 및 비디오 인식, 객체 감지, 분할, 이미지 복원 및 의료 이미지 분석을 포함한 다양한 비전 기반 애플리케이션에서 최근 몇 년 동안 점점 더 보편화되었습니다.  
CNN(컨볼루션 신경망)과 트랜스포머 기반 모듈을 결합하여 복잡한 시각적 패턴을 해석할 수 있는 강력한 접근 방식인 하이브리드 비전 트랜스포머를 만듭니다.  
하이브리드 비전 트랜스포머의 몇 가지 주목할만한 응용 분야는 아래에 설명되어 있습니다.

## Image/video recognition
CNN은 시각적 데이터에서 복잡한 정보를 자동으로 추출하는 능력으로 인해 이미지 및 비디오 처리에 광범위하게 활용되었습니다.  
그럼에도 불구하고 ViT는 이미지 및 비디오 인식을 포함한 다양하고 까다로운 작업에서 뛰어난 성능을 달성함으로써 컴퓨터 비전 분야에 혁명을 일으켰습니다.  
ViT의 성공은 이미지의 장거리 의존성을 포착할 수 있는 Self-Attention 메커니즘에 기인합니다.  
최근에는 CNN과 트랜스포머의 성능을 결합한 하이브리드 비전 트랜스포머(HVTs / hybrid vision transformers)가 인기를 얻었습니다.  
이미지와 비디오 모두에서 인식을 위해 HVT를 기반으로 다양한 방법이 제안되었습니다.  
Xiong et al.은 세밀한 3D 객체 인식을 향상시키기 위해 ViT와 CNN을 기반으로 하는 하이브리드 다중 모드 접근 방식을 제안했습니다.  
그들의 접근 방식은 ViT 네트워크를 사용하여 객체의 전역 정보를 인코딩하고 객체의 RGB 및 깊이 뷰를 통해 CNN 네트워크를 사용하여 객체의 로컬 표현을 인코딩합니다.

그들의 기술은 CNN 전용 및 ViT 전용 기준선보다 성능이 뛰어납니다.  
또 다른 기술에서 Tiong et al.은 얼굴-주위 교차 식별을 수행하기 위한 새로운 하이브리드 어텐션 비전 트랜스포머(HA-ViT / hybrid attention vision transformer)를 제시했습니다.  
HA-ViT는 하이브리드 어텐션 모듈에서 깊이별 컨볼루션과 컨볼루션 기반 MSA를 동시에 활용하여 로컬 및 글로벌 특성을 통합합니다.  
제안된 방법론은 FPCI 정확도 측면에서 세 가지 벤치마크 데이터 세트보다 성능이 뛰어납니다.  
Wang et al.은 HVT 기반 아키텍처를 사용하여 시각적 장소 인식을 위한 새로운 접근 방식을 제안했습니다.  
그들의 방법은 CNN과 ViT를 결합하여 지역 세부 정보, 공간적 맥락 및 높은 수준의 의미 정보를 캡처함으로써 시각적 장소 인식 시스템의 견고성을 향상시키는 것을 목표로 합니다.  
차량을 인식하기 위해 Shi et al.은 특성 추출을 위해 SE-CNN 아키텍처를 사용한 다음 ViT 아키텍처를 사용하여 글로벌 상황 정보를 캡처하는 융합 네트워크를 개발했습니다.  
그들이 제안한 접근 방식은 도로 인식 작업에 대한 우수한 정확도 값을 보여주었습니다.

## Image generation
이미지 생성은 컴퓨터 비전에서 흥미로운 작업이며 많은 다운스트림 작업의 기준선 역할을 할 수 있습니다.  
생성적 적대 신경망(GAN)은 이미지 생성을 위한 최선의 옵션으로 간주됩니다.  
그러나 Transformer 기반 GAN은 이 작업에서도 뛰어난 성능을 보여주었습니다.  
최근 연구자들은 HVT 기반 GAN을 활용하여 다양한 벤치마크 데이터 세트에서 뛰어난 성능을 입증했습니다.  
Torbunov et al.은 이미지 생성을 위한 하이브리드 GAN 모델인 UVCGAN을 보고했습니다.  
UVCGAN 모델의 아키텍처는 일부 수정된 원본 CycleGAN 모델을 기반으로 합니다.  
UVCGAN의 생성기는 UNet과 ViT 병목 현상을 기반으로 하는 하이브리드 아키텍처입니다.  
실험 결과는 원본 이미지와 생성된 이미지 간의 강력한 상관 관계를 유지하면서 이전의 최고 성능 모델에 비해 우수한 성능을 보여주었습니다.  
또 다른 연구에서는 Zhao et al.이 MRI 재구성을 위해 SwinGAN을 도입했습니다. 그들은 Swin Transformer U-Net 기반 생성기 네트워크와 CNN 기반 판별기 네트워크를 활용했습니다.

SwinGAN으로 생성된 MRI 이미지는 보다 효과적인 정보를 캡처할 수 있는 능력으로 인해 우수한 재구성 품질을 보여주었습니다.  
Tu et al.은 제안된 SWCGAN에 Swin 트랜스포머와 CNN 레이어를 결합했습니다.  
아키텍처에서 처음에는 CNN 레이어를 사용하여 로컬 수준 특성을 캡처한 다음 이후 레이어에서는 Residual Dense Swin Transformer Blocks "RDST"를 활용하여 글로벌 수준 특성을 캡처했습니다.  
개발된 방법은 원격탐사 영상에서 기존 접근법에 비해 좋은 재구성 성능을 보였다.  
최근 Bao et al.은 얼굴 이미지를 재구성하기 위해 공간 주의 유도 CNN-Transformer 집계 네트워크(SCTANet / spatial attention-guided CNN-Transformer aggregation network)를 제안했습니다.  
그들은 심층적인 특성 추출을 위해 HAA(Hybrid Attention Aggregation) 블록에서 CNN과 트랜스포머를 모두 활용했습니다.  
그들의 실험 결과는 다른 기술보다 더 나은 성능을 보여주었습니다.  
Zheng et al.은 그들의 접근 방식에서 의료 영상 생성을 위한 HVT 기반 GAN 네트워크를 제시했습니다.  
L-former라는 접근 방식에서는 얕은 레이어에 트랜스포머를 사용하고 더 깊은 레이어에 CNN을 사용합니다.  
그들의 접근 방식은 기존 GAN 아키텍처에 비해 뛰어난 성능을 보여주었습니다.

## Image segmentation
CNN과 ViT 기반 접근 방식은 이미지 분할과 같은 복잡한 이미지 관련 작업에서 뛰어난 성능을 보여주었지만 현재는 성능 향상을 위해 두 접근 방식의 장점을 결합하는 데 중점을 두고 있습니다.  
이에 대해 Wang et al.은 포도 분할을 위한 DualSeg라는 새로운 의미론적 분할 방법을 제시했습니다.  
그들의 방법은 Swin Transformer와 CNN을 결합하여 글로벌 기능과 로컬 특성의 장점을 모두 활용합니다.  
또 다른 연구에서 Zhou와 공동 저자는 터널 균열을 분할하기 위해 SCDeepLab이라는 하이브리드 접근 방식을 제안했습니다.  
그들의 접근 방식은 터널 라이닝의 균열 분할에서 다른 CNN 전용 및 트랜스포머 전용 기반 모델보다 성능이 뛰어났습니다.

Feng et al.은 파손 표면을 감지하기 위해 금속 커플러에서 분할 인식을 수행했습니다.  
이를 위해 자동 특성 추출을 위한 CNN과 특성 융합 및 전역 모델링을 위한 HCT(Hybrid Convolution and Transformer) 모듈을 활용하여 엔드 투 엔드 HVT 기반 접근 방식을 제안했습니다.  
최근 Xia와 Kim은 ViT 또는 CNN 기반 시스템의 한계를 해결하기 위해 HVT 접근 방식인 Mask2Former를 개발했습니다.  
개발된 접근 방식은 ADE20K 및 Cityscapes 데이터 세트 모두에서 다른 기술에 비해 더 나은 결과를 얻었습니다.  
Yuan et al.은 원격탐사 이미지의 의미론적 분할을 위해 MCAFNet이라는 HVT 기반 방법을 제안했습니다.

## Image Restoration
컴퓨터 비전에서 중요한 작업은 이미지 복원으로, 손상된 버전에서 원본 이미지를 복원하는 경향이 있습니다.  
이미지 복원 기반 시스템은 CNN 사용에서 ViT 모델로 전환되었으며, 최근에는 CNN과 트랜스포머의 장점을 결합한 하이브리드 트랜스포머로 전환되었습니다.  
Yi et al.은 단일 적외선 이미지 블라인드 디블러링을 수행하기 위해 AutoEncoder 기반 하이브리드 방법을 제안했습니다.  
그들의 접근 방식은 객체와 배경 사이의 상황 관련 정보를 추출하기 위해 하이브리드 컨볼루션-트랜스포머 블록을 활용합니다.  
학습 프로세스의 수렴을 가속화하고 우수한 이미지 디블러링 결과를 달성하기 위해 이 연구에서는 다단계 학습 기술과 혼합 오류 기능도 통합했습니다.

![](https://ars.els-cdn.com/content/image/1-s2.0-S1350449523000981-gr2_lrg.jpg)

또 다른 기술로 Chen et al.은 Convolution의 로컬 모델링 기능과 Self-Attention 모듈의 글로벌 모델링 능력을 결합한 Dual-former라는 효율적인 이미지 복원 아키텍처를 개발했습니다.  
제안된 아키텍처는 이전에 제시된 방법보다 훨씬 적은 GFLOPS(GPU FLoating point Operations Per Second)를 소비하면서 여러 이미지 복원 작업에서 뛰어난 성능을 달성합니다.  

![](https://ars.els-cdn.com/content/image/1-s2.0-S1051200424001106-gr002_lrg.jpg)

높은 계산 복잡성 문제를 해결하기 위해 Fang et al.은 경량 이미지 초해상도를 위해 하이브리드 네트워크인 HNCT를 활용했습니다.  
HNCT는 CNN과 ViT의 장점을 활용하고 로컬 및 비로컬 사전 변수를 모두 고려하는 특성을 추출하여 가볍지만 효과적인 초해상도 모델을 만듭니다. 실험 결과는 더 적은 매개변수를 사용하는 기존 접근 방식에 비해 HNCT의 결과가 향상되었음을 보여줍니다.

![](https://github.com/leesangjun1903/Computer-Vision/blob/main/image/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202024-09-25%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%205.44.19.png)

Zhao et al.은 효율적이고 효과적인 실제 이미지 노이즈 제거를 위해 TECDNet(Transformer Encoder and Convolutional Decoder Network)이라는 하이브리드 노이즈 제거 모델을 개발했습니다.  
TECDNet은 상대적으로 낮은 계산 비용을 유지하면서 뛰어난 노이즈 제거 결과를 얻었습니다.  

![](https://github.com/zhaoM0/TECDNet/raw/main/fig/Fig-1.png)

최근 Chen et al.은 적외선 및 가시 이미지 융합을 위한 엔드투엔드 HVT 기반 이미지 융합 접근 방식을 제시했습니다.  
제안된 기법은 거친 특성을 추출하기 위한 두 개의 분기를 가진 CNN 모듈과 이미지의 전역 및 공간 관계를 얻기 위한 ViT 모듈로 구성됩니다.  
그들의 방법은 글로벌 정보에 집중할 수 있었고 CNN 기반 방법의 단점을 극복할 수 있었습니다.  
또한, 조직적, 공간적 정보를 유지하기 위해 특화된 손실 함수가 설계되었습니다.

![](https://ars.els-cdn.com/content/image/1-s2.0-S0925231223000437-gr3_lrg.jpg)

## Feature extraction
특성 추출은 컴퓨터 비전에서 이미지에서 관련 시각적 정보를 식별하고 추출하는 데 필수적입니다.  
처음에는 CNN이 이 목적으로 사용되었지만 이제는 이미지 분류뿐만 아니라 포즈 추정, 얼굴 감지와 같은 기타 응용 분야에서 인상적인 결과로 인해 트랜스포머가 주목을 받았습니다.

Li와 Li는 그들의 작업에서 작물 질병을 식별하기 위한 효과적인 특성 추출을 위해 CNN과 트랜스포머의 장점을 병합하는 하이브리드 접근 방식인 ConVit을 제시했습니다.  
개발된 접근 방식의 실험 결과는 식물 질병 식별 작업에서 좋은 성능을 보였습니다.  
Li et al.은 다시 캡처된 장면 이미지 식별을 위해 계단식 접근 방식을 제안했습니다.  
그들의 접근 방식에서 그들은 처음에는 CNN 레이어를 사용하여 로컬 특성을 추출했고 나중에 더 깊은 레이어에서는 트랜스포머 블록을 활용하여 글로벌 수준 이미지 표현을 학습했습니다.  
제안된 접근 방식의 높은 정확도 값은 다시 캡처된 이미지를 식별하는 데 있어 효율성을 입증했습니다.

Li와 공동 저자는 스트립 강철 표면의 결함을 감지하기 위해 HVT 아키텍처를 개발했습니다.  
그들의 접근 방식은 CNN 모듈을 활용한 다음, 패치 임베딩 블록과 두 개의 트랜스포머 블록을 활용하여 상위 도메인 관련 특성을 추출했습니다.  
그들의 실험은 기존 방법에 비해 좋은 분류 성능을 보여주었습니다.  
최근 Rajani et al.은 다양한 해저 유형을 분류하기 위한 인코더-디코더 접근법을 제안했습니다.  
그들이 개발한 방법은 MLP 블록을 CNN 기반 특성 추출 모듈로 대체한 ViT 기반 아키텍처입니다.  
수정된 아키텍처는 실시간 계산 요구 사항을 충족하면서 뛰어난 결과를 달성합니다.

## Medical image analysis
CNN 기반 접근 방식은 다양하고 복잡한 패턴을 캡처할 수 있는 능력으로 인해 의료 이미지 분석에 자주 사용되었습니다.  
그러나 글로벌 수준의 이미지 표현을 모델링해야 하는 필요성으로 인해 연구자들은 의료 이미지 분석 영역에서 Transformers를 활용하도록 영감을 받았습니다.  
최근 여러 연구에서 CNN과 트랜스포머를 통합하여 의료 이미지의 로컬 및 글로벌 이미지 특성을 모두 캡처하여 보다 포괄적인 분석이 가능하도록 제안했습니다.  
이러한 하이브리드 아키텍처(CNN-transformer)는 수많은 의료 영상 관련 애플리케이션에서 엄청난 성능을 보여왔습니다.  
Tragakis et al.은 의료 영상을 분할하기 위한 새로운 FCT(Fully Convolutional Transformer) 접근 방식을 제안했습니다.  
FCT는 효과적인 이미지 표현을 학습하는 CNN의 능력과 장기적인 종속성을 캡처하는 Transformer의 능력을 결합하여 아키텍처에 ViT와 CNN을 모두 적용했습니다.

개발된 접근 방식은 기존의 다른 아키텍처와 비교하여 다양한 의료 과제 데이터 세트에서 뛰어난 성능을 보여주었습니다.  
또 다른 연구에서 Heidari et al.은 Swin Transformer 모듈과 CNN 기반 인코더를 활용하여 다중 스케일 특성 표현을 캡처하는 HVT인 HiFormer를 제안했습니다.  
실험 결과는 다양한 벤치마크 데이터 세트에서 의료 이미지를 분할하는 데 있어 HiFormer의 효율성을 입증했습니다.  
논문에서 Yang과 동료들은 컨볼루션 연산과 트랜스포머 블록을 결합하여 의료 이미지를 분석하는 TSEDeepLab이라는 새로운 하이브리드 접근 방식을 제시했습니다.  
특히, 이 접근 방식은 초기 단계에서 로컬 특성을 학습하기 위해 컨볼루셔널 레이어를 활용한 다음 트랜스포머 블록에서 처리하여 글로벌 패턴을 추출합니다.  
그들의 접근 방식은 여러 의료 이미지 분할 데이터 세트에서 탁월한 분할 정확도와 강력한 일반화 성능을 보여주었습니다.

## Object Detection
객체 감지는 감시, 로봇 공학, 자율 주행과 같은 광범위한 실제 응용 분야에서 중요한 컴퓨터 비전 작업입니다.  
ViT는 또한 이미지 픽셀 간의 장거리 종속성을 캡처하고 전체 이미지에서 복잡한 개체 패턴을 식별할 수 있는 self-attention 메커니즘으로 인해 개체 감지에서 인상적인 성능을 보여주었습니다.  
최근에는 CNN과 Self-Attention 메커니즘을 결합하여 객체 감지 성능을 향상시키는 HVT에 많은 관심이 모아지고 있습니다.  
Beal et al.은 자연 이미지에서 객체 감지를 위해 ViT-FRCNN이라는 HVT 접근 방식을 제안했습니다.  
접근 방식에서 그들은 Faster R-CNN 객체 감지기를 위해 ViT 기반 백본을 활용했습니다.  
ViT-FRCNN은 더 나은 일반화 능력으로 향상된 탐지 결과를 보여주었습니다.

Chen et al.은 원격 감지 이미지 감지를 위한 단일 스테이지 하이브리드 검출기를 소개했으며, 제안된 접근 방식인 MDCT는 아키텍처에서 CNN과 트랜스포머를 모두 활용했으며 다른 단일 스테이지 검출기에 비해 더 나은 성능을 보여주었습니다.  
Lu et al.은 무인 항공기(UAV) 이미지에서 객체 감지를 위한 HVT 기반 접근 방식을 개발했습니다.  
제안된 접근 방식은 트랜스포머 기반 백본을 활용하여 전역 수준 정보가 있는 특성을 추출한 다음 다중 규모 특성 학습을 위해 FPN에 공급했습니다.  
제안된 방법은 기존 방법에 비해 좋은 성능을 보였다.  
Yao와 그의 동료들은 개별 트랜스포머와 CNN 기반 분기를 활용하여 글로벌 및 로컬 수준 특성을 학습하는 융합 네트워크를 제안했습니다.  
실험 결과, 개발된 방법은 다른 방법에 비해 만족스러운 성능을 보였다.

## Pose Estimation
인간 자세 추정은 다양한 시나리오에서 중요한 점을 식별하는 경향이 있습니다.  
CNN과 트랜스포머 모두 자세 추정 작업에서 모범적인 성능을 보여주었습니다.  
현재 연구자들은 정확한 자세 추정을 위해 로컬 및 글로벌 수준 정보를 통합하기 위해 CNN과 트랜스포머를 통합된 방법으로 결합하는 데 주력하고 있습니다.  
Zhao et al.은 인간 자세 추정을 위한 새로운 이중 파이프라인 통합 트랜스포머 "DPIT"를 제시했습니다.  
Zhao의 접근 방식에서는 처음에 두 개의 CNN 기반 분기를 사용하여 로컬 특성을 추출한 다음 트랜스포머 인코더 블록을 사용하여 이미지의 장거리 종속성을 캡처합니다.  
또 다른 기술에서 Wang과 공동 저자는 CNN과 트랜스포머 분기를 사용하여 로컬 및 전역 이미지 표현을 학습한 다음 이를 통합하여 최종 출력을 생성했습니다.  
이들의 접근 방식은 기존의 다른 접근 방식에 비해 상당한 개선을 보여주었습니다.  
Hampali와 공동 저자는 Keypoint Transformer라는 하이브리드 포즈 추정 방법을 개발했습니다.  
제안된 방법에서는 인간의 관절을 2D 키포인트로 효율적으로 추정하기 위해 CNN과 트랜스포머 기반 모듈을 모두 활용했습니다.  
실험 결과는 InterHand2.6M을 포함한 데이터 세트에 대한 이 접근 방식의 예시적인 결과를 보여주었습니다.

# Challenges
컨볼루션 작업을 아키텍처에 통합하는 데 중점을 둔 비전 트랜스포머의 몇 가지 과제는 다음과 같습니다.

- Transformer는 모델링 용량이 높지만 CNN에 비해 이미지별 유도 바이어스가 낮을 수 있습니다. 따라서 이미지별 특성을 캡처하기 위한 트랜스포머 교육에는 충분한 양의 데이터가 필요합니다. 제한된 양의 훈련 데이터를 사용할 수 있는 경우 Transformer는 CNN에 비해 성능이 저하될 수 있습니다.
- 트랜스포머에는 데이터 종속성을 모델링하기 위한 조밀한 행렬 곱셈이 포함됩니다. 이로 인해 2차 계산이 복잡해지고 메모리에 상당한 오버헤드가 발생할 수 있습니다. 이는 고해상도 이미지의 모델링과 체적 분석 및 분할과 같은 밀도가 높은 응용 분야에서의 사용을 금지합니다.
- Transformer는 계산 비용이 많이 들고 훈련 및 배포를 위해 GPU와 같은 강력한 하드웨어 리소스가 필요할 수 있습니다. 이로 인해 실제 애플리케이션에 트랜스포머를 배포하는 것이 금지될 수 있으며 에지 장치에 배포하기가 어려워질 수 있습니다.
- HVT 아키텍처가 직면한 주요 과제는 트랜스포머 및 컨볼루션 레이어 모두에서 학습된 특성을 효율적으로 병합하는 것입니다. 트랜스포머 레이어는 공간적 위치와 무관한 전역 특성을 학습하는 반면, 컨볼루션 레이어는 공간적으로 상관된 로컬 특성을 학습합니다. 이러한 특성을 효과적으로 결합하면 잠재적으로 비전 작업 성능이 향상될 수 있습니다.
- HVT는 학습 용량이 크므로 복잡한 이미지 데이터를 높은 정확도로 처리할 수 있습니다. 그러나 이는 이 데이터를 효과적으로 학습하고 일반화하려면 대규모 교육 데이터세트가 필요하다는 의미이기도 합니다. 이는 특히 주석이 달린 데이터를 획득하는 것이 어렵고 시간이 많이 걸리는 의료 이미지의 경우 어려울 수 있습니다. 대량의 라벨링된 데이터가 필요하면 리소스와 시간 측면에서 상당한 오버헤드가 될 수 있으며, 의료 영상에서 HVT의 개발 및 적용을 방해할 수 있습니다.
- 마지막으로 컨볼루션 레이어와 트랜스포머 레이어를 창의적으로 결합하는 새로운 아키텍처를 연구할 여지가 여전히 많습니다. 예를 들어, 최근 연구에서는 다양한 주의 범위를 갖는 트랜스포머 레이어와 학습된 커널 크기를 갖는 컨볼루셔널 필터의 사용을 조사했습니다. 새로운 아키텍처를 조사하면 비전 작업의 성능이 더욱 향상될 수 있습니다.
  
# Future directions
비전 트랜스포머는 일반적으로 크기가 크고 수십억 개의 매개변수를 가질 수 있습니다.  
경량 아키텍처 설계에 주의를 기울일 필요가 있습니다.  
복잡성이 높으면 추론 지연이 발생하고 에너지 소비에 상당한 오버헤드가 발생합니다.  
상당한 추론률을 갖춘 효율적인 트랜스포머를 위한 새롭고 혁신적인 설계 원리를 탐구할 필요가 있습니다.  
이러한 탐구는 다양한 실제 애플리케이션, 에지 장치 및 위성과 같이 계산적으로 제한된 시스템에서 트랜스포머를 실제로 배포하는 데 중요합니다.  
지식 증류는 이러한 맥락에서 유망한 접근 방식으로 떠오르며, 대용량 모델과 단순한 모델 간에 지식을 효과적으로 전달하여 데이터 효율적이고 컴팩트한 모델을 생성할 수 있는 잠재력을 제공합니다.

비전 트랜스포머는 고용량 모델이지만 CNN이 제공하는 이미지별 유도 바이어스가 부족할 수 있습니다.  
HVT(Hybrid Vision Transformer)는 CNN과 트랜스포머의 장점을 결합할 수 있습니다.  
적절한 유도 바이어스를 통합함으로써 HVT는 이미지 분석 및 컴퓨터 비전 분야에서 상당한 발전을 이룰 수 있습니다.  
그러나 잠재력을 완전히 분석하려면 특정 비전 애플리케이션에 맞게 컨볼루션 및 self-attention 메커니즘을 정렬할 수 있는 적절한 방법을 탐색하는 것이 중요합니다.  
HVT의 로컬 및 글로벌 처리 기능은 비전 관련 작업 이상의 잠재적 이점을 제공하여 광범위한 비전 응용 분야에 매우 유망합니다.

HVT의 성능을 더욱 향상시키려면 더 나은 하이브리드 및 딥 아키텍처를 고안하는 데 도움이 될 수 있는 이미지 콘텐츠 및 관련 작업을 더 깊이 이해하는 것이 중요합니다.  
CNN-Transformer 아키텍처의 하이브리드 및 동적 특성 추출 메커니즘과 결합된 수작업 연산자의 잠재적 활용에 대한 조사는 가까운 미래에 특히 중요할 수 있습니다.  
컨볼루션 및 self-attention 메커니즘을 모두 사용하여 새롭고 효과적인 블록을 개발하는 것도 유망한 연구 분야입니다.

요약하면, HVT의 미래는 밝아 보이며 이미지 분석 및 그 이상 분야의 다양한 응용 분야에 대한 엄청난 잠재력이 있습니다.  
이점을 극대화하기 위해 연구 노력은 특정 비전 작업에 대한 컨볼루션 및 셀프 어텐션 메커니즘을 정렬하고, 이미지 콘텐츠 및 작업에 대한 더 깊은 이해를 얻고, 컨볼루션 및 셀프 어텐션 메커니즘을 모두 통합하는 새롭고 효과적인 블록을 개발하는 데 중점을 두어야 합니다.

# Conclusion
ViT는 특정 이미지 관련 작업에서 CNN 기반 시스템보다 우수한 성능으로 인해 연구 커뮤니티 내에서 상당한 주목을 받았습니다.  
ViT 아키텍처에 통합된 MSA 모듈은 이미지 내의 글로벌 상호 작용 모델링을 허용하여 성공에 기여합니다.  
성능을 향상시키기 위해 다양한 패치 기반 접근 방식, 지식 증류 기반 접근 방식, 어텐션 기반 접근 방식, 다중 트랜스포머 기반 접근 방식 및 하이브리드 접근 방식을 포함하여 다양한 아키텍처 개선이 ViT에 구현되었습니다.  
이 문서에서는 ViT의 아키텍처 분류를 자세히 살펴볼 뿐만 아니라 ViT 아키텍처의 기초가 되는 기본 개념도 살펴봅니다.

ViT는 인상적인 학습 능력을 가지고 있지만 이미지에서 지역적 관계를 포착할 수 있는 귀납적 편향이 부족하여 일반화가 제대로 이루어지지 않을 수 있습니다.  
결과적으로 이러한 모델은 학습하는 데 막대한 양의 데이터가 필요하며 계산 비용이 많이 들 수 있습니다.  
이러한 과제를 해결하기 위해 연구자들은 self-attention과 convolution 메커니즘을 모두 활용하여 로컬 및 글로벌 정보를 모두 학습하는 CNN-Transformers라고도 알려진 HVT를 개발했습니다.

여러 연구에서는 컨볼루션 특정 유도 바이어스를 트랜스포머에 통합하여 일반화 및 용량을 향상시키는 방법을 제안했습니다.  
통합 방법론에는 초기 계층 통합, 측면 계층 통합, 순차 통합, 병렬 통합, 계층적 통합 및 채널 부스팅 기반 통합이 포함됩니다.  
통합 방법론을 기반으로 HVT 아키텍처에 대한 분류를 소개하는 동시에 다양한 실제 컴퓨터 비전 애플리케이션에서 어떻게 사용되는지에 대한 개요도 제공합니다.  
현재의 어려움에도 불구하고 우리는 HVT가 지역 및 글로벌 수준에서 학습을 수행할 수 있는 능력으로 인해 엄청난 잠재력을 가지고 있다고 믿습니다.
