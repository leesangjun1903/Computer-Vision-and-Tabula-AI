# Progressive Distillation for Fast Sampling of Diffusion Models | Image generation

## 1. 핵심 주장 및 주요 기여  
**핵심 주장**  
- 기존 확산 모델(diffusion models)은 수백에서 수천 스텝의 샘플링 평가가 필요해 느리지만, **Progressive Distillation** 기법을 통해 샘플링 스텝 수를 단계적으로 절반으로 줄이면서도 높은 샘플 품질을 유지할 수 있다[1].  

**주요 기여**  
- 새로운 **파라미터화(parameterization)** 와 **손실 가중치** 설계를 통해 소수 스텝에서도 안정적인 학습이 가능하도록 함.  
- **Progressive Distillation** 알고리즘: N 스텝 teacher 모델로부터 N/2 스텝 student 모델을 학습시키고, 이를 반복하여 4 스텝까지 샘플링 스텝을 축소.  
- CIFAR-10, ImageNet, LSUN 벤치마크에서 4 스텝 샘플링으로도 FID 3.0 (CIFAR-10) 등 최첨단 품질 달성 및 전체 증류 시간이 원본 모델 학습 시간에 준함.  

## 2. 문제 제기, 제안 방법, 모델 구조, 성능 및 한계  

### 2.1 해결하고자 하는 문제  
- **샘플링 속도**: 고품질 이미지를 생성하려면 수백~수천 번의 네트워크 평가가 필요해 실용성 제약.  

### 2.2 제안 방법  
1. **파라미터화 개선**  
   - 직접 $$x$$ 예측, $$(x, \epsilon)$$ 예측 결합, 또는 $$v\equiv \alpha_t\epsilon - \sigma_t x$$ 예측 중 선택  
   - 사전 실험 결과, $$v$$ 예측이 SNR 변화에 독립적이며 안정적이나, $$x$$ 예측이 약간 더 우수한 성능[1].  
2. **손실 가중치 설계**  
   - 기존 SNR 가중치는 SNR=0에서 0이 돼 수렴 불가.  
   - 제안: **Truncated SNR** ($$\max(\alpha_t^2/\sigma_t^2,1)$$) 또는 **SNR+1** ($$1 + \alpha_t^2/\sigma_t^2$$) 사용[1].  
3. **Progressive Distillation 알고리즘**  
   - Algorithm 2:  
     - Teacher 모델 $$\hat x_\eta$$ 로부터 두 번의 DDIM 스텝 진행 → intermediate $$z_{t''}$$ 획득  
     - Student 모델 $$\hat x_\theta$$ 에서 한 번 스텝으로 동일 결과 얻도록 목표값 $$\tilde x$$ 계산:  

```math
         \tilde x =\frac{z_{t''} - (\sigma_{t''}/\sigma_t) z_t}{\alpha_{t''} - (\sigma_{t''}/\sigma_t)\alpha_t}
```
       
  - - - MSE 손실 $$\|\tilde x - \hat x_\theta(z_t)\|^2$$ 로 student 학습  
  - - - 스텝 수 $$N\to N/2$$ 반복[1].  

### 2.3 모델 구조  
- **U-Net** 기반 백본, BigGAN 스타일 업/다운샘플링 적용.  
- CIFAR-10: 최고 해상도 32×32, 채널 256, 드롭아웃 0.2.  
- ImageNet/LSUN: 64×64, 128×128 해상도, 채널 수 및 어텐션 적용 레이어 차등.  

### 2.4 성능 향상  
- **샘플링 스텝 vs. FID** (CIFAR-10):  
  | Steps | 1    | 2    | 4    | 8    | 16   | … | 256  |  
  |-------|------|------|------|------|------|---|------|  
  | FID   | 9.12 | 4.51 | 3.00 | 2.57 | 2.35 | … | 2.08 |  
- 4 스텝만에 **FID 3.0** 달성, 8192→4 스텝 2,048× 가량 가속화[1].  

### 2.5 한계  
- **극단적 스텝 축소 시 화질 저하**: 1–2 스텝에서는 품질 급격 저하.  
- **모델 크기 제약**: student 모델은 teacher와 동일 구조; 경량화 미실험.  
- **일반화**: distillation 과정 중 특정 이산 시점만 학습하므로, 연속적 시간 일반화 능력 감소 위험.  

## 3. 모델의 일반화 성능 향상 가능성  
- 파라미터화 및 손실 가중치 선택이 **저 SNR** 영역에서도 안정적 예측 가능하도록 설계됨으로써, 소수 스텝에서도 과적합 없이 **다양한 시점**에 대한 견고한 일반화 기대.  
- 향후 student 모델 아키텍처를 작게 조정하거나 연속적 시간 특성 재학습을 통해 **연속적 샘플링 경로** 일반화 성능 강화 가능.  

## 4. 향후 연구에 미치는 영향 및 고려 사항  
- **고속 생성 모델** 개발 가속: 실시간 이미지·비디오 생성, 모바일·장착 장치 응용.  
- **경량화 student 모델** 탐구: 모델 크기 축소와 속도 향상의 추가 이득.  
- **다양한 데이터 모달리티**(오디오·텍스트·그래프)로의 확장: 확산 모델의 범용적 고속화.  
- **일반화 보장**: distillation된 모델의 연속적 시간 예측 특성 및 out-of-distribution 견고성 평가 필요.  
- **손실 가중치·파라미터화** 추가 연구: 더 강건한 스텝 독립적, SNR 독립적 설계 탐색.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/5893d562-914f-417d-bb03-cf159725c227/2202.00512v2.pdf
