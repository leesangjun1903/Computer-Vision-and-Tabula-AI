# WRN : Wide Residual Networks | Image classification

## 핵심 주장과 주요 기여

Wide Residual Networks(WRNs) 논문은 **깊이보다 폭이 더 효과적**이라는 혁신적인 관점을 제시합니다. 기존 ResNet이 깊이 증가에 집중했다면, 이 연구는 네트워크의 폭(너비)을 늘리는 것이 성능 향상과 훈련 효율성 측면에서 우월함을 실증적으로 보여줍니다.[1]

주요 기여는 다음과 같습니다:
- **Residual block 구조의 체계적 분석**: 다양한 블록 타입, 레이어 수, 폭에 대한 종합적 실험
- **WRN 아키텍처 제안**: 깊이를 줄이고 폭을 늘린 새로운 구조
- **Dropout의 효과적 활용법**: Residual block 내 적절한 위치에서의 dropout 적용
- **최첨단 성능 달성**: CIFAR, SVHN, COCO 등에서 새로운 state-of-the-art 결과

## 해결하고자 하는 문제

### 기존 문제점
1. **Diminishing feature reuse**: 매우 깊은 네트워크에서 일부 블록만 유용한 표현을 학습하고 나머지는 정보 공유가 미미함[1]
2. **훈련 비효율성**: 수천 개 레이어의 얇은 네트워크는 GPU의 병렬 연산에 비효율적
3. **성능 개선의 한계**: 깊이 증가 시 marginal gain 대비 계산 비용이 과도하게 증가

## 제안하는 방법

### 수학적 정의
Residual block은 다음과 같이 정의됩니다:

$$ x_{l+1} = x_l + F(x_l, W_l) $$

여기서 $$x_l$$과 $$x_{l+1}$$은 l번째 유닛의 입력과 출력, $$F$$는 residual function, $$W_l$$은 블록 파라미터입니다.[1]

### 핵심 설계 요소
- **Widening factor k**: 특성 평면(feature plane) 수를 k배 증가
- **Deepening factor l**: 블록당 컨볼루션 레이어 수
- **표기법**: WRN-n-k (n: 총 레이어 수, k: 폭 증가 계수)

## 모델 구조

### 기본 구조
WRN은 다음과 같은 계층적 구조를 가집니다:[1]

| Group | Output Size | Block Type |
|-------|-------------|------------|
| conv1 | 32×32 | [3×3, 16] |
| conv2 | 32×32 | [3×3, 16×k; 3×3, 16×k] ×N |
| conv3 | 16×16 | [3×3, 32×k; 3×3, 32×k] ×N |
| conv4 | 8×8 | [3×3, 64×k; 3×3, 64×k] ×N |
| avg-pool | 1×1 | [8×8] |

### 블록 타입 실험
다양한 블록 구성을 실험한 결과, B(3,3) 구조가 최적임을 확인했습니다:
- B(3,3): 기본 블록 (최적)
- B(3,1,3): 추가 1×1 레이어 포함
- B(1,3,1): "straightened" bottleneck
- 기타 변형들

## 성능 향상 및 결과

### 주요 성과

| Dataset | Model | Error Rate | 비교 대상 |
|---------|-------|-----------|----------|
| CIFAR-10 | WRN-28-10 | 4.00%[1] | ResNet-164: 5.46% |
| CIFAR-100 | WRN-28-10 | 19.25%[1] | ResNet-164: 24.33% |
| SVHN | WRN-16-8 | 1.54%[1] | 기존 최고 성능 초과 |

### 효율성 개선
- **WRN-40-4**: ResNet-1001과 유사한 정확도, **8배 빠른 훈련**[1]
- **WRN-28-10**: ResNet-1001 대비 36배 적은 레이어, 더 높은 성능
- GPU 병렬 연산에 최적화된 구조로 훨씬 효율적인 계산

### Dropout의 효과
Residual block 내 컨볼루션 레이어 사이에 dropout을 적용하여 추가 성능 향상을 달성했습니다:[1]
- CIFAR-10: 0.11% 개선
- CIFAR-100: 0.4% 개선
- SVHN에서 특히 효과적 (데이터 증강 없이도 우수한 성능)

## 일반화 성능 향상

### 다양한 태스크에서의 성공
논문은 여러 도메인에서 WRN의 우수한 일반화 성능을 입증했습니다:[1]

- **이미지 분류**: CIFAR-10/100, SVHN, ImageNet
- **객체 검출**: COCO 2016에서 WRN-34-2 기반 모델이 state-of-the-art 달성
- **전이 학습**: Inception 대비 더 나은 일반화 특성

### 일반화 메커니즘
1. **풍부한 표현 학습**: 더 넓은 네트워크가 더 다양하고 robust한 특성 학습
2. **효과적인 정규화**: Dropout과 batch normalization의 조합
3. **균형잡힌 아키텍처**: 깊이와 폭의 최적 비율로 과적합 방지

## 한계점

1. **메모리 사용량**: 폭 증가로 인한 제곱적 파라미터 증가
2. **하드웨어 의존성**: GPU 메모리 제약으로 인한 확장성 한계
3. **ImageNet에서의 제한적 개선**: CIFAR 대비 상대적으로 작은 성능 향상
4. **이론적 이해 부족**: 왜 폭이 깊이보다 효과적인지에 대한 이론적 근거 미흡

## 향후 연구에 미치는 영향

### 패러다임 전환
이 논문은 "더 깊게"에서 "더 넓게"로의 패러다임 전환을 촉발했습니다. 후속 연구들은 다음과 같은 방향으로 발전했습니다:

1. **EfficientNet**: 깊이, 폭, 해상도의 균형잡힌 스케일링
2. **RegNet**: 네트워크 설계 공간의 체계적 탐색
3. **Vision Transformer**: 완전히 다른 아키텍처로의 전환

### 실용적 영향
- **산업계 적용**: 훨씬 효율적인 모델 설계 가능
- **AutoML 발전**: 아키텍처 탐색 공간의 확장
- **모바일/엣지 컴퓨팅**: 효율성 중시 설계의 중요성 부각

## 향후 연구 고려사항

1. **이론적 분석**: 폭의 효과에 대한 더 깊은 이론적 이해 필요
2. **메모리 효율성**: 폭 증가 시 메모리 사용량 최적화 방안 연구
3. **다양한 도메인 적용**: NLP, 음성 등 다른 영역에서의 wide network 효과 검증
4. **하이브리드 접근**: 깊이와 폭의 최적 조합을 위한 adaptive scaling 방법론
5. **정규화 기법**: Wide network에 특화된 새로운 정규화 방법 개발

이 논문은 네트워크 아키텍처 설계에 대한 근본적인 관점 변화를 제시했으며, 현재까지도 효율적인 딥러닝 모델 설계의 핵심 원칙으로 활용되고 있습니다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/372d8e7e-81df-479e-ac4c-8cd3a925da1d/1605.07146v4.pdf
