# Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights | Domain adaption

## 1. 핵심 주장 및 주요 기여
**Piggyback**는 기존에 학습된 네트워크 가중치를 그대로 고정한 채, *이진 마스크*(binary masks)만을 학습하여 새로운 작업에 적응(adapt)시키는 방법이다. 이 방식은 다음을 보장한다:
1. **기존 작업 성능 보존**: 가중치를 변경하지 않아 *catastrophic forgetting*이 발생하지 않는다.
2. **매우 작은 파라미터 오버헤드**: 신규 작업마다 네트워크 파라미터 1비트만 추가(≈3.12% 증가).
3. **작업 순서 불감성**: 마스크가 작업마다 독립적이므로, 작업 추가 순서에 따른 성능 저하가 없다.
4. **높은 성능**: ImageNet 사전학습 네트워크에서 시작하여, VGG-16, ResNet, DenseNet 등 다양한 구조로 수십 개의 분류 및 분할(segmentation) 작업에서 개별 네트워크와 유사하거나 더 우수한 정확도 달성.

## 2. 문제 정의와 제안 방법

### 2.1 해결하고자 하는 문제
- **연속학습(Continual Learning)**: 새로운 작업이 연속적으로 주어질 때, 기존 작업 성능을 유지하면서 새로운 작업도 학습하고자 함.
- **파라미터 효율성**: 각 작업마다 전체 네트워크를 복제(fine-tune)하지 않고, 최소한의 추가 파라미터로 다수 작업을 수행.

### 2.2 제안 방법
각 레이어의 고정된 사전학습 가중치 $$W$$에 대해, 동일한 크기의 실수형 마스크 가중치 $$m^r$$를 학습하고, 임계값 $$\tau$$를 이용해 이진 마스크 $$m\in\{0,1\}$$를 얻어 적용한다.

- **이진화**  

$$
    m_{ji} = 
    \begin{cases}
      1 & \text{if } m^r_{ji} \ge \tau,\\
      0 & \text{otherwise.}
    \end{cases}
  $$
- **순전파**  

$$
    y = (W \odot m)\,x,
    \quad\text{즉}\quad
    y_j = \sum_i w_{ji}\,m_{ji}\,x_i.
  $$
- **역전파**  
  이진화 비미분성에도 불구하고, $$m$$에 대한 그라디언트

$$
    \frac{\partial E}{\partial m_{ji}}
    = \delta y_j \, w_{ji}\,x_i
    \quad\Rightarrow\quad
    \delta m = (\delta y\,x^T)\odot W
  $$
  
  를 이용하여 실수형 마스크 $$m^r$$를 업데이트한다.

- **파라미터 오버헤드**  
  각 마스크는 원본 가중치 당 1비트만을 추가로 사용하므로, 전체 네트워크 크기의 약 3.12%만 증가.

## 3. 모델 구조 및 학습 세부사항
- **백본(backbone)**: ImageNet 사전학습된 VGG-16, VGG-16-BN, ResNet-50, DenseNet-121, Wide ResNet-28 등.
- **마스크 학습**:  
  - 초기화: $$m^r$$를 소량(1e-2)로 균일 초기화, 임계값 $$\tau=5\text{e-}3$$.  
  - 옵티마이저: Adam (마스크), SGD (마지막 분류기층).
  - 배치 정규화(batch-norm) 파라미터: 기본적으로 고정. 도메인 쉬프트 큰 작업(예: WikiArt)에서는 작업별로 학습 시 성능 향상(≈2–4% 개선).

- **확장**: 분류층 외에, FCN 기반 분할(segmentation)에서도 deconvolution 레이어에 대해 동일한 마스킹 적용. PASCAL VOC 세그멘테이션에서 mIOU 61.41 얻어(고정 VGG-16 세그멘테이션 61.08 대비).

## 4. 성능 및 한계

| 작업 유형             | Piggyback 정확도                | 개별 네트워크 정확도        | 오버헤드                |
|----------------------|-------------------------------|----------------------------|-------------------------|
| CUBS Birds (VGG-16)   | 79.01%                        | 79.01%                     | +3.12%                  |
| Sketch (ResNet-50)   | 79.91%                        | 80.78%                     | +3.12% (+BN: +1 MB)     |
| WikiArt (DenseNet-121)| 69.19%                        | 76.41%                     | +3.12% (+BN: +1 MB)     |
| Visual Decathlon     | 총점 2,838 /10,000 (1.28× 파라미터)| 2,851 (DAN, 2.17×)         | 1.28×                   |
| PASCAL Segmentation  | mIOU 61.41                    | 61.08                      | 마스크 17 MB, 레이어 7.5 MB |

**한계**  
1. **도메인 쉬프트**가 클수록(예: WikiArt) 고정 배치정규화 시 성능 감소.  
2. **작업 간 지식 공유 부재**: 마스크 학습이 백본만 참조하므로, 이전에 추가된 작업의 특화 필터를 상호 활용하지 못함.

## 5. 연구 영향 및 향후 고려사항
- **영향**: 파라미터 효율적 연속학습 분야에 새로운 패러다임 제시. 리소스 제약 환경(엣지 디바이스)에서 다중 작업 지원 가능.  
- **향후 연구**:  
  1. **작업 간 전이 학습** 통합: 마스크 간 정보 교환 메커니즘 설계.  
  2. **배치정규화 자동 최적화**: 도메인 차이에 따라 자동으로 BN 파라미터 조정.  
  3. **비선형 마스크 확장**: 이진 → 삼진 또는 실수 마스크로 필터 다양성 증대.  
  4. **객체 탐지·분할 연속학습**: 공간적 구조 활용해 piggyback 적용 범위 확대.

Piggyback는 최소한의 추가 메모리로 기존 네트워크를 유지하면서 새로운 작업을 효율적으로 학습하는 방법론으로, 엣지 컴퓨팅 및 지속적 학습 시나리오에서 핵심적인 역할을 할 전망이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/09357a37-c0c5-421d-b2f1-fd00bffd33f3/1801.06519v2.pdf
