# Token Merging for Fast Stable Diffusion | 2023 · 167회 인용, Image generation, Fast generation

## 1. 핵심 주장과 주요 기여

**Token Merging for Fast Stable Diffusion**은 생성 이미지의 자연스러운 중복성을 활용해 redundant tokens을 병합함으로써 diffusion 모델의 속도를 향상시키는 방법을 제시합니다. 이 연구의 주요 기여는 다음과 같습니다:[1]

- **추가 훈련 없이** 기존 Stable Diffusion 모델의 토큰 수를 최대 60%까지 줄일 수 있음[1]
- 이미지 생성 속도를 **최대 2배** 향상시키고 메모리 사용량을 **최대 5.6배** 감소시킴[1]
- xFormers와 같은 기존 최적화 기법과 함께 사용 가능하여, 대용량 이미지에서 **최대 5.4배** 빠른 성능 달성[1]

## 2. 해결하고자 하는 문제

### 문제 정의
Diffusion 모델들은 transformer 기반 구조를 사용하기 때문에 계산 복잡도가 토큰 수의 제곱에 비례하여 증가합니다. 기존 최적화 방법들(Flash Attention, xFormers 등)은 여전히 모든 토큰에 대해 계산을 수행해야 하므로, 근본적인 계산량 감소에는 한계가 있었습니다.[1]

### 제안하는 방법

#### Token Merging과 Unmerging
논문에서는 기존 Token Merging (ToMe) 기법을 diffusion 모델에 적용하기 위해 **unmerging** 개념을 도입했습니다.[1]

**Merging 수식:**
두 개의 유사한 토큰 $$x_1, x_2 ∈ \mathbb{R}^c$$가 주어졌을 때, 이들을 하나의 토큰으로 병합:

$$x^*_{1,2} = \frac{x_1 + x_2}{2}$$

**Unmerging 수식:**
병합된 토큰을 다시 분리:

$$x'_1 = x^*_{1,2}, \quad x'_2 = x^*_{1,2}$$

#### 개선된 토큰 분할 방법
기존 ToMe의 교대 분할 방식 대신, **2×2 영역에서 무작위로 하나의 destination 토큰을 선택**하는 방법을 도입하여 더 나은 시각적 품질을 달성했습니다.[1]

## 3. 모델 구조

ToMe for Stable Diffusion은 U-Net 기반 Stable Diffusion 모델의 각 transformer 블록에 적용됩니다. 구체적인 적용 방법:[1]

- **적용 위치**: self-attention 모듈에만 적용 (cross-attention과 MLP는 제외)[1]
- **적용 범위**: 최소 4096개 이상의 토큰을 가진 블록에만 적용[1]
- **처리 과정**: 각 블록의 시작에서 토큰을 병합하고, 계산 후 unmerging하여 skip connection 수행[1]

## 4. 성능 향상 및 한계

### 성능 향상
실험 결과에 따르면 ToMe for Stable Diffusion은 다음과 같은 성능을 보입니다:[1]

| 토큰 감소율 | FID 점수 | 속도 향상 | 메모리 절약 |
|-------------|----------|-----------|-------------|
| 10% | 32.86 | 1.21× | 1.14× |
| 30% | 32.80 | 1.50× | 1.99× |
| 50% | 33.02 | 1.87× | 3.83× |
| 60% | 33.37 | 2.03× | 5.68× |

### 한계점
- **정보 손실**: unmerging 과정에서 완전한 정보 복원이 불가능[1]
- **콘텐츠 변화**: 높은 토큰 감소율에서는 이미지 내용이 크게 변할 수 있음[1]
- **세부사항 손실**: 배경이나 복잡한 텍스처에서 일부 세부사항이 손실될 수 있음[1]

## 5. 일반화 성능 향상 가능성

이 연구는 일반화 성능 측면에서 여러 중요한 시사점을 제공합니다:

### 훈련 없는 적응성
ToMe for Stable Diffusion의 가장 큰 장점은 **추가 훈련 없이** 기존 모델에 직접 적용 가능하다는 점입니다. 이는 다양한 도메인의 diffusion 모델에 쉽게 적용할 수 있는 일반화 가능성을 시사합니다.[1]

### Dense Prediction Task로의 확장
기존 ToMe가 분류 작업에만 적용되었던 것과 달리, 이 연구는 dense prediction 작업인 이미지 생성에 성공적으로 적용하여 **토큰 병합 기법의 적용 범위 확장** 가능성을 보여줍니다.[1]

### 다른 최적화 기법과의 호환성
xFormers와 같은 기존 최적화 기법과 함께 사용 가능하여, 다양한 최적화 전략과 결합할 수 있는 **상호 보완적 특성**을 보입니다.[1]

## 6. 향후 연구에 미치는 영향과 고려사항

### 연구에 미치는 영향
1. **효율적인 생성 모델 개발**: 대규모 모델의 실용적 배포를 위한 새로운 방향 제시
2. **토큰 병합 기법의 확산**: 다른 dense prediction 작업으로의 적용 가능성 확대
3. **하드웨어 제약 환경 대응**: 제한된 자원에서도 고품질 이미지 생성 가능

### 향후 연구 시 고려사항
1. **더 나은 unmerging 전략 개발**: 현재의 단순한 평균 방식을 넘어선 고급 기법 필요[1]
2. **Proportional attention과 key 기반 유사도**: 분류 작업에서 효과적이었던 기법들의 diffusion 적용 연구[1]
3. **다른 dense prediction 작업 적용**: 세그멘테이션, 객체 탐지 등 다양한 작업으로의 확장
4. **품질-속도 트레이드오프 최적화**: 더 정교한 토큰 선택 및 병합 전략 개발

이 연구는 생성 AI 모델의 효율성 향상에 있어 중요한 이정표가 되며, 특히 실제 환경에서의 모델 배포와 관련된 연구 방향에 큰 영향을 미칠 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b31a86ca-af7b-40ba-a60b-741a8a663255/2303.17604v1.pdf)
