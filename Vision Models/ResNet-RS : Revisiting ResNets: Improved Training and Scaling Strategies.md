# "Revisiting ResNets: Improved Training and Scaling Strategies" 

## 1. 핵심 주장과 주요 기여

이 논문의 **핵심 주장**은 **훈련 방법과 스케일링 전략이 새로운 아키텍처 설계보다 더 중요할 수 있다**는 것입니다[1]. 연구진은 ResNet이라는 기존 아키텍처를 현대적 훈련 기법으로 재검토하여 놀라운 성능 향상을 달성했습니다.

**주요 기여사항:**
- **ResNet-RS 모델군 개발**: TPU에서 EfficientNet 대비 1.7x-2.7x 속도 향상 달성[1]
- **준지도 학습에서 86.2% ImageNet 정확도** 달성하며 EfficientNet-B5 대비 4.7배 빠른 속도 실현[1]
- **훈련 방법론의 중요성 입증**: 아키텍처 변경 없이도 3.2% 성능 향상 달성[1]
- **새로운 스케일링 전략 제시**: 기존 compound scaling의 한계 극복[1]

## 2. 해결하려는 문제와 제안 방법

### 문제 정의
기존 연구들이 **새로운 아키텍처와 훈련 방법론을 동시에 도입**하여 각각의 기여도를 구분하기 어려웠고, 이로 인해 아키텍처 개선의 실제 효과가 과대평가되는 문제가 있었습니다[1].

### 제안 방법

#### A. 개선된 훈련 방법
1. **코사인 학습률 스케줄링**
2. **확장된 훈련 기간** (350 에포크)
3. **라벨 스무딩** (smoothing rate: 0.1)
4. **확률적 깊이** (stochastic depth)
5. **RandAugment 데이터 증강**
6. **최종 레이어 드롭아웃**
7. **정규화 조합 시 가중치 감쇠 감소**:

$$ \text{weight decay} = 1 \times 10^{-4} \rightarrow 4 \times 10^{-5} $$

[1]

#### B. 개선된 스케일링 전략
**전략 1**: 과적합이 발생할 수 있는 환경에서는 깊이 스케일링, 그렇지 않으면 너비 스케일링[1]

**전략 2**: 이미지 해상도를 기존보다 천천히 증가 (EfficientNet의 compound scaling 대비)[1]

### 모델 구조
- **기본 구조**: ResNet + ResNet-D 수정 + Squeeze-and-Excitation (SE ratio: 0.25)[1]
- **ResNet-D 수정사항**: 
  - 7×7 합성곱을 3개의 3×3 합성곱으로 대체
  - 다운샘플링 블록에서 스트라이드 크기 조정
  - 스킵 연결에서 평균 풀링 사용[1]

## 3. 성능 향상 및 일반화 성능

### 성능 향상 결과
- **훈련 방법만으로**: ResNet-200이 79.0% → 82.2% (+3.2%) 향상[1]
- **아키텍처 변경 추가**: 82.2% → 83.4% (+1.2%) 추가 향상[1]
- **총 개선의 3/4이 훈련 방법에서 기인**[1]

### 일반화 성능 향상
**전이 학습 성능**에서 개선된 지도 학습 방법(RS)이:
- **SimCLR 대비 10개 태스크 중 5개에서 우수한 성능**[1]
- **SimCLRv2 대비 10개 태스크 중 8개에서 우수한 성능**[1]

**다양한 도메인으로의 일반화**:
- 비디오 분류 (Kinetics-400): 73.4% → 77.4% (+4.0%)[1]
- CIFAR-100, Pascal 검출/분할, ADE 분할, NYU 깊이 추정 등 다양한 하위 태스크에서 성능 향상[1]

이는 **자기지도 학습이 더 범용적인 표현을 생성한다는 기존 믿음에 도전**하는 중요한 결과입니다[1].

## 4. 한계점

### 주요 한계
1. **훈련 체제 의존성**: 스케일링 전략이 훈련 에포크 수에 크게 의존[1]
2. **태스크 특화성**: 일부 개선사항이 아키텍처 변경보다 태스크에 특화될 수 있음[1]
3. **하드웨어 특이성**: 성능 향상이 TensorFlow on TPUv3 환경에 특화됨[1]

## 5. 미래 연구에 대한 영향과 고려사항

### 연구 방법론의 변화
1. **공정한 비교를 위한 표준화**: 아키텍처 비교 시 훈련 방법 통제 필요[1]
2. **실용적 지표 사용**: FLOPs/파라미터보다 실제 지연시간/메모리 사용량 중시[1]
3. **하드웨어-아키텍처 공동 설계**의 중요성 부각[1]

### 향후 연구 시 고려사항

**실험 설계**:
- 스케일링 규칙 개발 시 완전한 수렴까지 훈련 필요[1]
- 소규모 실험에서의 외삽 지양[1]
- 다중 스케일링 차원 동시 고려[1]

**평가 프로토콜**:
- 하드웨어 관련 지표 우선 사용[1]
- 일반화 검증을 위한 전이 학습 성능 테스트[1]
- 적절히 훈련된 기준선과의 비교[1]

**훈련 전략 개발**:
- 다중 정규화 기법 조합 시 정규화 강도 조정[1]
- 훈련 체제에 따른 스케일링 전략 선택[1]
- 광범위한 적용성 확보를 위한 다양한 하위 태스크 테스트[1]

이 연구는 **"단순함이 종종 승리한다"**는 교훈을 제시하며, 복잡한 새로운 아키텍처보다는 **기존 아키텍처의 적절한 훈련과 스케일링이 더 효과적일 수 있음**을 보여줍니다[1]. 이는 향후 컴퓨터 비전 연구의 패러다임 전환을 촉진할 것으로 예상됩니다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/8243b2c9-fef3-4a8b-83b1-57424724bc89/2103.07579v1.pdf
