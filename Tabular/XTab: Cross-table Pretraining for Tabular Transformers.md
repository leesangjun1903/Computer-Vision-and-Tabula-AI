# XTab: Cross-table Pretraining for Tabular Transformers

**핵심 주장 및 주요 기여**  
XTab은 서로 다른 테이블 간에 공통적인 표 형태 데이터를 학습할 수 있도록 탭형 트랜스포머를 사전학습(pretraining)하는 프레임워크이다. 기존 탭형 모델은 단일 도메인의 데이터에만 사전학습되어 새로운 테이블로의 지식 이전(generalization)이 불가능하나, XTab은 다양한 도메인의 테이블을 연합학습(federated learning) 방식으로 학습하여 1) 여러 테이블 간 일관된 가중치 초기화(shared backbone)를 확보하고, 2) 새로운 테이블 학습 속도와 성능을 크게 향상시킨다.[1]

## 해결하고자 하는 문제  
표 데이터는 열(column)의 수와 타입이 케이스마다 달라 표준화된 토크나이저(tokenizer)를 적용하기 어렵고, 단일 테이블 기반 사전학습 기법은 도메인 간 지식 이전에 한계가 있다. 따라서 서로 다른 구조와 속성을 지닌 테이블 환경에서도 공통의 사전학습 모델을 통해 새로운 테이블에 빠르게 적응할 수 있는 방법이 필요하다.[1]

## 제안하는 방법  
XTab은 세 가지 주요 구성 요소로 이루어진다(그림1).  

1. **데이터 특화(featurizer) 블록**: 각 테이블의 열 개수 $$c$$와 임베딩 차원 $$d$$에 대응하여 수치형 열은 $$x_k W_k + b_k$$, 범주형 열은 학습 가능한 임베딩 매트릭스에서 조회하는 방식으로 행(row)을 $$\mathbb{R}^{c\times d}$$ 텐서로 변환한다.  
2. **공통 백본(transformer backbone)**: 열 수가 다른 입력 시퀀스 길이 $$c$$에도 적응 가능한 트랜스포머 블록 $$N$$개를 공유하며, 주로 FT-Transformer, Fastformer, Saint-v 변형을 사용한다.  
3. **데이터 특화(projection head) 블록**: 각 테이블별로 복원(reconstruction), 대조(contrastive: InfoNCE), 혹은 지도(supervised) 손실을 계산하는 헤드를 별도로 구성한다.

사전학습은 연합학습(FedAvg)을 통해 각 클라이언트(테이블)에서 데이터 특화 블록과 공유 백본을 함께 최적화하고, 서버가 공유 가중치 $$\mathbf{w}_S$$만 주기적으로 평균화한다:  

$$
\mathbf{w}_S^{(t+1)} = \frac{1}{K}\sum_{k=1}^K \bigl(\mathbf{w}_S^{(t)} - \eta \nabla_{\mathbf{w}_S} \mathcal{L}_k(\mathbf{w}_S^{(t)}, \mathbf{w}_{NS,k})\bigr)
$$

여기서 $$K$$는 테이블 수, $$\eta$$는 학습률이다.[1]

## 모델 구조  
데이터 특화 블록(초록색)과 공유 백본(회색)으로 분리되어 있으며, 학습 후 파인튜닝 시 데이터 특화 블록만 무작위 초기화하고 공유 백본만 초기 가중치로 사용하여 새로운 테이블에 적응한다(그림1).[1]

## 성능 향상  
- 84개 OpenML-AMLB 태스크에서 FT-Transformer에 XTab 사전학습 적용 시, 랜덤 초기화 대비 평균 윈율(win rate)이 50–71%까지 증가하고, 오류(error) 감소율 역시 유의하게 향상되었다.[1]
- 다양한 사전학습 목표(복원, 대조, 지도) 및 트랜스포머 변형(Fastformer, Saint-v) 모두에서 일관되게 성능 개선을 보였다.[1]

## 한계  
- 여전히 CatBoost 같은 트리 기반 모델 성능에는 미치지 못하며, 특히 파인튜닝(epoch 수가 많을수록) 시 ‘망각(catastrophic forgetting)’ 현상이 발생해 사전학습 장점이 감소한다.[1]
- 제로샷(zero-shot) 적용에는 한계가 있으며, 파인튜닝용 데이터가 어느 정도 필요하다.

## 일반화 성능 향상 관점  
XTab이 가장 중점을 둔 부분은 **새로운 테이블에 대한 빠른 적응**이다. 1) 다양한 테이블에서 공유 백본을 학습함으로써 공통 특성을 포착하고, 2) 데이터 특화 블록만 새로 학습해도 높은 성능을 달성한다. 이로 인해 파인튜닝 데이터가 제한적일 때도 베이스라인 대비 학습 속도가 최대 2배 이상 가속되며, 성능 저하 없이 일반화할 수 있음을 실험적으로 입증했다.[1]

## 향후 연구 방향 및 고려 사항  
XTab은 표 형태 데이터의 범용 사전학습 가능성을 제시했으나, 다음 사항이 추가 연구로 고려되어야 한다:  
- **트리 모델과의 격차 해소**: 트랜스포머 기반과 트리 기반 모델의 성능 차이를 메우기 위한 하이브리드 아키텍처 혹은 추가 사전학습 기법 연구.  
- **대규모·다중 모달 확장**: 텍스트·이미지 등 다른 모달과 결합한 멀티모달 사전학습 및 대형 파운데이션 모델과 연계 연구.  
- **망각 완화**: 파인튜닝 시 공유 백본의 정보 손실을 최소화하는 지속학습(continual learning) 전략 적용.  
- **사전학습 효율성**: 연합학습 단계 통신 비용과 계산 자원을 절감하는 알고리즘 최적화.

이와 같은 방향으로 후속 연구가 진행된다면, 다양한 도메인 간 지식 이전이 가능한 탭형 트랜스포머의 활용 범위가 더욱 확대될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/9c9ae4e4-5ffb-406f-b25e-1d9e035db571/2305.06090v1.pdf)
