# TabPFNv2 : Accurate predictions on small data with a tabular foundation model

**주요 주장**  
TabPFN(Tabular Prior-data Fitted Network)은 수천에서 만 건 이하의 샘플을 갖는 소규모·중규모 표 형식 데이터에 대해, 기존의 그래디언트 부스팅 트리(GBT) 계열 모델(예: CatBoost, XGBoost)보다 훨씬 높은 예측 정확도를 적은 학습 시간으로 달성하는 **탐색적 기초 모델(foundation model)**이다.[1]

**핵심 기여**  
- **인컨텍스트 학습(in-context learning)**을 표 형식 데이터에 적용하여, 사전 생성된 수백만 개의 합성 데이터셋에서 학습한 **하나의 변환기(transformer)** 모델이 데이터셋 전체를 한 번에 처리하며 학습·예측을 수행.  
- 표 구조를 고려한 **2차원 셀(Cell) 어텐션** 아키텍처 설계(특징별(attend features)·샘플별(attend samples) 어텐션), 순서 불변성(invariance) 및 메모리·연산 최적화.  
- **합성 데이터 생성(prior) 파이프라인**: 인과적 구조모델(SCM) 기반의 그래프 샘플링, 신경망·의사결정트리·분류화·잡음 주입 등 다양하고 현실적인 데이터 특성 반영.  
- 10,000샘플·500특징 이하 분류 및 회귀 벤치마크에서 2.8초 만에 기본 설정으로 최고 성능 달성, GBT를 4시간 튜닝한 결과보다 우수.[1]
- 모델 미세조정(fine-tuning), 데이터 생성·밀도 추정(density estimation), 재사용 가능한 임베딩 학습, SHAP 기반 해석 가능성 제공.  

# 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

## 해결하고자 하는 문제  
표 형식(tabular) 데이터는 여러 도메인에서 널리 사용되나, 다음과 같은 어려움을 지님:[1]
- 데이터셋마다 통계·스케일·형식(이산·연속·누락값 등)이 달라 **전이 학습·일괄 학습**이 어려움.  
- 딥러닝은 대규모 데이터에 강하지만, 소규모·중규모 표 데이터에서 전통적 방법(GBT)이 여전히 우위.  
- 비지도 학습 및 전이 학습, 불확실성 예측이 제한적.

## 제안하는 방법  
TabPFN은 **인컨텍스트 학습(ILC)**을 활용해, 사전 생성된 합성 데이터셋을 통해 **학습 알고리즘 자체**를 학습하는 접근법이다.[1]

1. **합성 데이터(prior) 생성**  
   - 구조적 인과 모델(SCM)을 무작위 DAG로 샘플링.  
   - 각 엣지에 신경망·의사결정트리·분류화·잡음 주입 모듈 적용.  
   - 데이터셋 크기·특징 수·잡음 패턴·결측값 등 하이퍼파라미터 샘플링.  
   - 약 1억 개의 합성 데이터셋 생성.  

2. **사전 학습(pre-training)**  
   - 변환기 인코더(transformer encoder) 변형 모델 $$q_\phi$$를 학습.  
   - 입력: 전체 학습 샘플 $$(X_{\text{train}}, y_{\text{train}})$$과 평가 샘플 $$X_{\text{test}}$$.  
   - 손실 함수: 교차엔트로피 $$\mathcal{L}=-\mathbb{E}\_{\text{datasets}}\big[\log q_\phi(y_{\text{test}}\mid X_{\text{train}},y_{\text{train}},X_{\text{test}})\big]$$로 베이지안 사후예측분포 근사.[1]

3. **실제 데이터셋 예측**  
   - 사전 학습된 하나의 모델에 학습·평가 데이터를 함께 입력하여 단일 순방향(forward)으로 예측.  
   - 학습·평가 사이 상태 캐싱으로 연산 재활용.

### 모델 구조  
- **2D TabPFN 레이어**: 각 테이블 셀을 토큰으로 간주,  
  1) **1D 특징 어텐션**: 각 샘플(row)의 특징들 간 상호작용 학습  
  2) **1D 샘플 어텐션**: 같은 특징(column) 샘플들 간 정보 교환  
  3) **MLP 서브층** 및 잔차 연결·LayerNorm(half-precision)  
- **인과합성 prior** 설계로 모델의 **일반화 성능** 강건  
- **추가 최적화**: 플래시 어텐션, 체크포인팅, GPU 메모리 절약 기법  

## 성능 개선  
- **분류**: 29개 데이터셋, 정규화된 ROC AUC 0.939(default)·0.952(튜닝) vs. CatBoost 0.752·0.822.[1]
- **회귀**: 28개 데이터셋, 정규화된 RMSE 0.923·0.968 vs. 0.872·0.875.[1]
- **속도**: 분류 2.8s, 회귀 4.8s (GPU 사용 시)로 4h 튜닝 GBT 대비 5,140배·3,000배 빠름.[1]
- **강건성**: 결측값·범주형·잡음·불필요 특징·샘플 수 변화에 큰 성능 저하 없음.  
- **기초모델 특성**: 미세조정, 데이터 생성·밀도 추정, SHAP 해석, 재사용 임베딩 등 보유.

## 한계  
1. **스케일**: 샘플 ≤10,000·특징 ≤500 범위 밖 확장성 추가 연구 필요.  
2. **메모리·연산**: $$\mathcal{O}(n^2+m^2)$$ 연산·$$\mathcal{O}(nm)$$ 메모리, 대규모 데이터 부담.  
3. **추론 속도**: 최적화되지 않은 상황에서 고성능 GBT보다 느릴 수 있음.  

# 일반화 성능 향상 가능성  
- **합성 prior 설계**: 인과 구조 모델 기반 다양한 실험 설계로, **분포 불일치**(dataset shift) 및 **결측값 패턴**에 대한 내성 획득.  
- **인컨텍스트 학습**: 학습·평가 데이터 동시 처리, **베이지안 예측** 근사로 불확실성 표현 및 다모달 분포 모델링.  
- **Fine-tuning**: 유사한 태스크 군에서 미세조정을 통해 **양적 성능 이득** 확인(사인 곡선 예제에서 분포 유사도 증가 시 성능 개선).[1]
- **PHE(Post hoc ensembling)**: TabPFN 모델 여러 구성 조합, 그리디 앙상블 선택으로 추가 성능 향상 가능.  

# 연구 영향 및 향후 고려사항

**연구 영향**  
TabPFN은 표 형식 데이터에 대한 전통적 인간 설계 알고리즘 한계를 돌파하고, **합성 데이터 기반 기초 모델 패러다임**을 개척함으로써 다양한 과학·산업 분야에서 표 데이터 분석 속도와 정확도를 크게 향상시킬 잠재력을 지님.

**향후 연구 고려점**  
- **대규모 데이터 확장**: 샘플·특징 수 확장 및 분산 처리 전략.  
- **시간·다중 모달 데이터**: 시계열·이미지·유전 정보 등 특화 prior 설계.  
- **이론적 분석**: ICL 기반 PFN의 이론적 근거 및 베이지안 해석 심화.  
- **실험적 공정**: 실제 산업·의료 데이터 적용 및 하드웨어 최적화.  

TabPFN은 표 데이터를 다루는 연구자 및 실무자에게 강력한 도구를 제공하며, 후속 연구를 통해 더 넓은 적용 범위와 성능 개선이 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/c6f936b7-65bb-4557-94e8-773f35e13995/s41586-024-08328-6.pdf)
