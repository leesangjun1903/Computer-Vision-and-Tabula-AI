# Why do tree-based models still outperform deep learning on tabular data?

# 주요 요지  
**트리 기반 모델이 탭형(tabular) 데이터에서 여전히 우수한 이유**는 깊은 신경망(Deep Learning)이 갖지 못한 세 가지 암묵적 귀납 편향(inductive bias)에 기인한다. 1) **비정보성(uninformative) 특성에 강인함**, 2) **데이터의 방향성(orientation)을 보존**, 3) **불규칙(irregular) 함수 학습 능력**이다. 아울러, 저자들은 탭형 데이터 전용 표준 벤치마크(45개 데이터셋, 20 000시간 규모 하이퍼파라미터 탐색)와 모든 원시 결과(raw results)를 공개하여, 후속 연구의 토대를 제공한다.[1]

## 문제 정의  
딥러닝은 이미지·언어 분야에서 혁신적 성과를 보였으나, 컬럼별 이질성과 소규모 샘플, 극단치(extreme values)가 공존하는 탭형 데이터에서는 전통적 트리 앙상블(랜덤포레스트·XGBoost 등)에 비해 성능이 뚜렷이 밀린다. 기존 연구들은 서로 다른 평가 방법·소규모 데이터셋·과도한 튜닝 편향으로 결과가 재현 불가하거나 모호했다.[1]

## 제안 방법  
– **표준화된 탭형 데이터 벤치마크**  
  45개의 중규모(≈10K 샘플) 데이터셋 선별·전처리(결측 제거·카테고리 축소 등) 및 20 000시간 규모 랜덤 탐색(random search) 기반 하이퍼파라미터 튜닝 절차 확립.[1]
– **대규모 비교 평가**  
  MLP, ResNet, FT-Transformer, SAINT 등 최신 탭형 전용 신경망과 RandomForest·GBT·XGBoost를 CPU/GPU 환경에서 동등한 예산 내 비교.[1]
– **귀납 편향 탐구를 위한 데이터 변환 실험**  
  1) **목표 함수(target) 스무딩**: 가우시안 커널 $$K(x, x') = \exp[-\tfrac12(x - x')^\top \Sigma^{-1}(x - x')]$$ 기반으로  

$$\tilde Y(X_i)=\frac{\sum_{j}K(X_i,X_j)\,Y(X_j)}{\sum_{j}K(X_i,X_j)}$$  
  
  을 적용해 불규칙 패턴 학습 민감도를 측정.[1]
  
  2) **특성 제거 및 추가**: 랜덤포레스트 중요도에 따라 비정보성 특성 제거·추가 실험으로 모델별 강인성 평가.[1]
  3) **무작위 회전(rotation)**: 회전 불변성(rotational invariance)이 성능에 미치는 영향 분석.[1]

## 모델 구조  
– MLP·ResNet 계열: 피드포워드 네트워크에 드롭아웃·배치 정규화·스킵 커넥션 적용  
– FT-Transformer·SAINT: 토큰 임베딩과 셀프어텐션, 샘플 간 어텐션(row attention) 포함  
– 트리 모델: RandomForest, GradientBoostingTrees, XGBoost  

## 성능 향상 및 한계  
모든 하이퍼파라미터 예산 구간에서 트리 모델이 우위. 신경망은 학습 속도·튜닝 비용 측면에서도 뒤처진다.  
– **불규칙 함수 학습**: 신경망은 스무딩에 둔감해 복잡 비선형 패턴 적응 어려움.[1]
– **비정보성 특성 민감도**: MLP 유사 구조가 특성 제거 시 성능 급격 저하.[1]
– **회전 불변성의 역효과**: 무작위 회전 후 트리 모델 성능 유지, 신경망 순위 역전.[1]
한계로 작은/매우 큰 데이터셋, 결측치·고카디널리티 범주 처리, 추가 정규화·데이터 증강 기법 검토가 남아 있다.[1]

## 일반화 성능 개선 관점  
트리 모델의 편향이 탭형 데이터의 **불규칙·비정보성·비회전성** 특성을 자연스럽게 반영하여, 과적합 대비 일반화 능력이 우수하다. 신경망은 다음을 고려해야 한다:  
- **타겟 인식 스무딩** 또는 **주기적 임베딩**으로 고주파(target high-frequency) 패턴 포착  
- **비회전성 학습** 유도: 임베딩 레이어 등으로 방향성 정보 유지  
- **강인한 특성 선택**: 특성 중요도 기반 스키밍(skim)·정규화 기법 적용  

## 향후 연구에의 영향 및 고려 사항  
이 벤치마크는 탭형 전용 신경망 개발의 표준을 제시하며, 이후 연구는 다음을 고려해야 한다.  
1) **데이터 귀납 편향 설계**: 스무딩, 회전 불변성 차단, 특성 선택 메커니즘을 모델에 내장  
2) **결측치·고카디널리티 처리**: 트리 모델 대비 공정 비교를 위한 전처리·네트워크 설계  
3) **다양한 규모 실험**: 초소규모(<3K) 및 대규모(>50K) 환경에서의 재평가  
4) **정규화·데이터 증강**: 균형적 비교를 위한 MLP·Transformer용 탭형 특화 기법 통합  
이로써 탭형 데이터 특성에 맞춘 딥러닝의 실제 적용 가능성과 일반화 성능 강화 방향이 구체화될 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/00f81d45-b26b-4c92-81b1-368c4a71e9be/2207.08815v1.pdf)
