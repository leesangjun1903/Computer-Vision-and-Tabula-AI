# SubTab: Subsetting Features of Tabular Data for Self-Supervised Representation Learning

**핵심 주장**  
SubTab은 탭ular 데이터의 자기지도 학습을 위해 전체 특성 공간을 여러 부분집합(view)으로 나누어, 각 부분집합으로부터 원본 전체 데이터를 복원하도록 학습함으로써 은닉 표현의 질을 크게 향상시킨다.[1]

**주요 기여**  
- 탭ular 데이터에 특화된 *multi-view* 자기지도 학습 프레임워크 제안  
- 부분집합별 은닉 표현을 평균 등으로 결합하는 *협업적 추론(collaborative inference)* 개념 도입  
- 특징 하위 집합만으로 전체 데이터를 복원하는 방식이 단순 노이즈 기반 변형보다 효과적임 입증  
- MNIST 탭ular 변환 데이터에서 CNN 기반 SOTA 모델과 동등한 성능(98.31%) 달성 및 실제 의료·유전체·메타게놈 등 4개 데이터셋에서 기존 기법들 대비 유의미한 성능 향상 달성[1]

***

## 1. 해결하고자 하는 문제

탭ular 데이터는 이미지·텍스트·오디오와 달리 공간·시간·의미적 구조가 명확치 않아, 일반적인 데이터 증강(잘라내기·회전·노이즈 주입 등)이 효과적이지 못하며, 자기지도 학습 연구가 미흡했다. 특히 단순 노이즈 주입 기반 오토인코더는 모든 특성을 균등 취급하여 불필요 특성 변형 시 표현 학습에 한계가 있다.[1]

***

## 2. 제안 방법

### 2.1 데이터 분할 및 모델 구조  
- 입력 데이터 $$X\in\mathbb{R}^{N\times D}$$를 $$K$$개의 특성 부분집합 $$\{x_k\}_{k=1}^K$$으로 분할  
- 각 부분집합을 공용 인코더 $$E$$에 투입하여 은닉 표현 $$h_k = E(x_k)$$ 획득  
- 공용 디코더 $$D$$는 $$h_k$$로부터 전체 특성 공간 $$\hat{X}_k = D(h_k)$$을 재구성  
- 선택적으로 표현 $$h_k$$에 대한 투영 네트워크 $$G$$를 통해 투영 $$z_k = G(h_k)$$ 생성[1]

### 2.2 학습 목적 함수  

$$
L_\text{total} = L_r + \lambda_c L_c + \lambda_d L_d
$$

- **복원 손실 (Reconstruction Loss)**  

$$
  L_r = \frac{1}{K}\sum_{k=1}^K \frac{1}{N}\sum_{i=1}^N \|X_i - \hat{X}_{k,i}\|^2
  $$

- **대조 손실 (Contrastive Loss, NT-Xent)**  

$$
  L_c = \frac{1}{J}\sum_{(a,b)} \frac{-1}{2N}\sum_{i=1}^N \log\frac{\exp(\mathrm{sim}(z_{a,i},z_{b,i})/\tau)}{\sum_{j\neq i}\exp(\mathrm{sim}(z_{a,i},z_{b,j})/\tau)}
  $$

- **거리 손실 (Distance Loss)**  

$$
  L_d = \frac{1}{J}\sum_{(a,b)} \frac{1}{N}\sum_{i=1}^N \|z_{a,i}-z_{b,i}\|^2
  $$

여기서 $$J$$는 부분집합 간 페어 수, $$\mathrm{sim}(\cdot,\cdot)$$은 코사인 유사도, $$\tau$$는 온도 파라미터이다.[1]

### 2.3 테스트 시 협업적 추론  
테스트 시 각 부분집합 $$h_k$$들의 평균·합·최댓값 등으로 결합하여 최종 표현 $$\bar{h} = \mathrm{mean}(h_1,\dots,h_K)$$을 생성하고, 선형 분류기 등 하류 과제에 활용한다.[1]

***

## 3. 성능 향상 및 한계

### 성능 향상  
- MNIST 탭ular 설정에서 98.31%로 CNN 기반 SOTA와 동등 수준 달성  
- TCGA(38개 클래스), Obesity·Income·BlogFeedback 데이터셋에서도 기존 자기지도·지도 학습 기법 대비 평균 5–15% 이상의 성능 개선  
- 하위 집합만으로도 표현 학습이 가능, 누락 특성 상황에서도 강건한 예측력 보장[1]

### 한계 및 확장 가능성  
- 대조·거리 손실 사용 시 복합도 증가: 조합 수에 비례하여 학습 비용이 $$O(K^2)$$로 상승  
- 특성 순서 고정 전제: 기존 MLP 기반 구조의 순서 의존성 해결 위해 순열 불변 아키텍처 필요  
- 추후 연구: 중요 특성 식별을 위한 계층적 SubTab, 어텐션 기반 가중 결합, 도메인 간 전이 학습, 분산 학습 활용 등[1]

***

## 4. 일반화 성능 향상 가능성

SubTab의 *multi-view* 구성은 다음과 같은 일반화 장점이 있다.  
- 다양한 특성 부분집합에 대한 복원 과제는 모델이 전역 정보뿐 아니라 부분적·국소적 패턴까지 포괄하는 표현 학습 유도  
- 노이즈 제거가 아닌 부분 정보에서 전체 정보를 재구성하도록 강제함으로써 과적합 위험 완화  
- 누락 특성이나 도메인 변화 시에도 가용 파트만으로 예측 가능하여 *out-of-distribution* 견고성 강화  
- 협업적 추론 시 가중 합 등의 메커니즘 도입으로 중요 특성에 대한 적응적 대응 가능[1]

***

## 5. 향후 연구 시 고려 사항 및 영향

- **계층적 중요도 탐색**: SubTab의 부분집합 분할을 세분화하여 개별 특성 중요도 계층적으로 식별  
- **순열 불변 네트워크**: 특성 순서 무관 학습 구조(예: deep sets) 적용으로 순서 의존성 해소  
- **확률적 가중 결합**: 어텐션 메커니즘으로 각 부분집합 표현을 가중 결합하여 표현의 적응적 강화  
- **분산·전이 학습**: 서로 다른 기관 간 공통 특성 그룹화로 데이터 프라이버시 보호하면서 모델 공유  
- **타 도메인 확장**: 이미지·텍스트·시계열 등에도 SubTab 아이디어 적용 가능, 다중 모달 표현 학습으로 확장  

SubTab은 Tabular 데이터 자기지도 학습 연구를 촉진하고, 의료·금융·법률 등 다양한 분야에서 고품질 표현 학습을 통한 실제 응용을 가속화할 전망이다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/c6c12f28-cf28-4fcb-98ce-b4b990ad920a/2110.04361v2.pdf)
