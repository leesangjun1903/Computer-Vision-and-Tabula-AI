# Estimating the Class Prior in Positive and Unlabeled Data through Decision Tree Induction

# 주요 주장 및 기여 요약

**Estimating the Class Prior in Positive and Unlabeled Data through Decision Tree Induction(이하 TIcE)** 논문은, Positive and Unlabeled(PU) 데이터에서 **클래스 사전확률(class prior)**을 추정하는 간단하면서도 효율적인 기법을 제안한다. 주요 기여는 다음과 같다.  
- **선택 확률(label frequency)**이 모든 속성 부분공간(subdomain)에서 동일하다는 관찰을 활용하여, 각 부분공간의 레이블 비율이 사전확률의 하한(lower bound)을 제공함을 보임.[1]
- 이 하한이 실제 사전확률에 근접해지는 부분공간을 **결정 트리(top-down) 분할**로 탐색하는 방법을 제안.  
- 제안 기법(TIcE)이 최신 기법들과 유사한 정확도를 유지하면서도 **10배 이상의 연산 속도 개선**을 달성함을 실험적으로 검증.[1]

# 문제 설정 및 제안 방법 상세

## 문제 정의  
PU 학습에서는 입력 $$x$$와 레이블 $$y\in\{0,1\}$$가 있지만, 관측된 레이블 $$s$$는 양성($$y=1$$)일 때만 라벨링되어 $$s=1$$, 그 외는 미관측($$s=0$$)이다. 선택 완전임(random selection) 가정  

$$
\Pr(s=1\mid x,y=1)=c
$$

하에, $$c$$는 **레이블 빈도(label frequency)**이며, 클래스 사전확률 $$\alpha$$와 관계는  

$$
\Pr(y=1\mid x)=\frac{1}{c}\Pr(s=1\mid x)
$$

$$
\alpha=\frac{\Pr(s=1)}{c}
$$

이다.[1]

## 하한식 유도  
부분공간 $$A$$에서 레이블 확률은  

$$
\Pr(s=1\mid x\in A)=c\,\Pr(y=1\mid x\in A)
$$

이며, 양성 부분공간($$\Pr(y=1\mid x\in A)=1$$)에서는  

$$
c=\Pr(s=1\mid x\in A)
$$

일반적으로  

$$
c\ge\Pr(s=1\mid x\in A)
$$

이므로, 데이터를 속성 조건부로 분할한 후 각 부분공간의 $$L/T$$ 비율이 $$c$$의 하한이 된다. 표본 오차 보정을 위해 치비셰프 부등식에서 유도된 오차항 $$\epsilon$$을 포함하여 실험적으로 안정적인 하한식을 사용한다.[1]

## 결정 트리 기반 탐색  
TIcE는 서로 독립적인 두 데이터 셋(트리 구축용 vs 추정용)을 k-겹 교차검증으로 분리한 뒤,  
1. **트리 데이터**에서 **max-bepp**(positive purity) 기준으로 분할을 반복해 “화이트리스트”가 될 만한 순수 양성 부분공간 후보를 탐색.  
2. 각 후보 부분공간에 대해 **추정 데이터**에서 하한식을 계산하고, 그 중 최대값을 최종 $$c$$ 추정치로 선택.  
3. 사전추정치 $$c_{\text{prior}}$$로 오차항을 더 정교하게 조정한 뒤 다시 반복하여 최종 추정치 $$\hat c$$ 도출.[1]

# 모델 구조 및 성능/한계

## 모델 구조  
- **트리 깊이 제한** 및 **최대 분할 수(M)**를 설정하여 연산 복잡도를 $$O(mn)$$으로 유지.  
- **best-first 탐색**을 통해 분할 후보 노드를 우선순위 큐로 관리, 빠른 수렴 보장.  
- 교차검증($$f$$ folds)으로 평균화하여 **추정치 분산 감소**.

## 성능 향상  
- **정확도**: 대표 PU 데이터셋에서 최신 기법(KM2, AlphaMax 등)과 유사한 클래스 사전추정 오차 $$|\hat\alpha-\alpha|\approx0.09$$ 달성.  
- **속도**: 평균 처리 시간 0.76ms/예제로, 타 기법(1.10–52.92ms) 대비 **최소 1.5배에서 최대 70배** 빠름.[1]

## 한계  
- **레이블 빈도 매우 낮거나 데이터 차원이 높은 경우**, 순수 부분공간 탐색이 어려워 하한이 넓어질 수 있음.  
- **수치형 특성 균등 분할** 사용이 최적 분할이 아닐 수 있어, 데이터 분포에 따라 추가 전처리가 필요할 수 있음.  
- 의존 가정(“선택 완전임”) 위배 시 성능 저하 가능.

# 일반화 성능 향상 가능성

- **무작위 분할 기반 교차검증**으로 과적합을 방지하여 추정 안정성 보장.  
- **하한식의 보수성**으로 과대추정을 억제, 모델이 드물게 발생하는 노이즈에 덜 민감.  
- 추가로, **전처리 단계**에서 특징 공간 축소(PCA 등)나 **균등 분할 대신 정보량 기반 분할**을 사용하면, 고차원·희소 데이터에서도 일반화 성능이 개선될 수 있다.

# 향후 연구 방향 및 고려사항

- **비완전 라벨(noisy positivity)** 상황 확장: 양성 레이블에 노이즈가 섞인 경우에도 강건하도록 하한식 수정 연구.  
- **비이산/연속 특성 분할 최적화**: 균등 분할이 아닌 정보 이득 기반 최적 점 탐색으로 성능 향상.  
- **다중 클래스 및 멀티레이블 PU** 확장: 이진에서 다중 분류 환경으로 일반화.  
- **딥러닝 통합**: 결정 트리가 아닌 신경망 기반 하위영역 탐색 기법과 결합하여 대규모·비정형 데이터에 적용.  

위 고려사항들은 PU 데이터에서 클래스 사전추정의 현실적 제약을 완화하고, 다양한 응용 분야에서 일반화 가능성을 높이는 핵심 포인트가 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/1746082e-65d4-4e87-9e93-23510b708f1e/11715-Article-Text-15243-1-2-20201228.pdf)
