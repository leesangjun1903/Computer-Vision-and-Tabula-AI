# Consistent feature attribution for tree ensembles

# 핵심 요약 및 주요 기여  
**Consistent feature attribution for tree ensembles** 논문은 기존 트리 앙상블의 특성 중요도(feature attribution) 방법들이 **일관성(consistency)** 을 보장하지 못해, 모델이 특정 특성에 더 의존하게 개선되어도 그 특성 중요도가 오히려 감소할 수 있음을 지적한다. 이를 해결하기 위해, 조건부 기댓값을 기반으로 한 **SHAP (SHapley Additive exPlanation) 값**을 유일하게 일관성과 지역 정확성(local accuracy)을 만족하는 가산(feature-additive) 기여값으로 정의하고, SHAP 값을 트리 모델에 효율적으로 계산하는 **Tree SHAP 알고리즘**을 제안하여 XGBoost에 통합했다.[1]

# 1. 해결하고자 하는 문제  
트리 앙상블(예: 그래디언트 부스팅, 랜덤 포레스트)에서  
모델 전체(global) 또는 개별 예측(local)에 대한 특성 중요도를 산출할 때,  
- **Gain-based** 또는 **Saabas path-based** 방법은 결정 경로상 분할(split)만 고려  
- **불일관성(inconsistency)**: 모델이 특정 특성에 더 의존하도록 변경되어도 그 특성 중요도가 감소 가능[1]

이로 인해 모델 해석의 신뢰성(trustworthiness)과 투명성이 저해된다.  

# 2. 제안하는 방법  
## 2.1 가산 특성 기여 모델(Additive Feature Attribution)  
모델 설명을 이진 변수로 변환한 선형 함수로 정의한다:[1]

$$
g(z') = \phi_{0} + \sum_{i=1}^{M} \phi_{i} z'_i
$$  

여기서 $$z'_i \in \{0,1\}$$는 특성 $$i$$의 관측 여부, $$\phi_i$$는 기여값이다.  

## 2.2 SHAP 값 정의  
Shapley 값 이론을 활용하여, 특성 $$i$$의 기여값 $$\phi_i$$를 다음과 같이 계산한다:[1]

$$
\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! \,(M - |S| - 1)!}{M!} \Bigl[\,f_{x}(S \cup \{i\}) - f_{x}(S)\Bigr]
$$  

여기서  
- $$N$$: 전체 특성 집합, $$M = |N|$$  
- $$f_x(S) = E\bigl[f(x)\mid x_S\bigr]$$: 특성 집합 $$S$$ 관측 시 모델 출력의 조건부 기댓값  
- Shapley 값은 **일관성(consistency)**, **지역 정확성(local accuracy)**, **누락성(missingness)** 을 동시에 만족하는 유일한 해임을 보장한다.[1]

## 2.3 Tree SHAP: 효율적 계산  
기본 SHAP 계산은 $$O(2^M)$$의 지수 시간 복잡도를 가지나, 트리 구조에 특화된 **Tree SHAP** 알고리즘을 통해 복잡도를  
- 균형 트리: $$O(T\,L\,\log^2 L)$$  
- 비균형 트리: $$O(T\,L\,D^2)$$  
로 줄여 대규모 모델(수천 개의 트리, 수백 개 특성)에도 실시간 설명이 가능하도록 한다.[1]
- 핵심 아이디어: 결정 경로별 가능한 모든 특성 조합 흐름을 다수 경로 동시 추적  
- `EXTEND`/`UNWIND` 메서드를 이용해 경로별 조합 가중치 관리  

# 3. 모델 구조 및 성능  
- **구현**: XGBoost에 Tree SHAP 통합  
- **실험**:  
  - 이상적인 예시(Supervised clustering)에서 기존 path 방법 대비 클러스터링 성능(R²) 개선  
  - 복잡한 실제 애플리케이션에서도 SHAP 기반 해석이 모델 이해도 및 하위 그룹 식별에 유리함을 입증[1]

# 4. 일반화 성능 향상 가능성  
- **로버스트한 특성 기여 해석**: 일관성 보장으로 과적합된 특성 중요도 왜곡 방지  
- **슈퍼바이즈드 클러스터링** 등 다양한 다운스트림 태스크에서 더 신뢰성 높은 설명 제공  
- 결과적으로, **특성 기여 기반 파인튜닝** 및 **피처 엔지니어링** 단계에서 모델 일반화 성능 개선에 기여 가능  

# 5. 한계 및 향후 고려 사항  
- **데이터셋 수준(global) SHAP gain**: 논문에서는 인스턴스 수준 로컬 SHAP만 다룸  
- **계산 비용**: Tree SHAP는 매우 효율적이나, 초대형 트리 앙상블에는 여전히 연산 부담 존재  
- **조건부 기댓값 추정**: 입력 특성 간 강한 종속성이 있을 때 정확도 저하 우려  
- **패키지 통합 및 버전 호환성**: 다양한 프레임워크(예: LightGBM, CatBoost)로의 확장 검토 필요  

# 6. 향후 연구 영향 및 고려 사항  
- **모델 해석 연구**: SHAP 이론에 기반한 일관성 있는 기여 해석이 해석 가능성(XAI) 분야 표준이 될 전망  
- **엔드-투-엔드 워크플로우**: SHAP 기반 피처 선택(feature selection) 및 하이퍼파라미터 튜닝 자동화  
- **다른 모델 클래스로 확장**: 딥러닝, SVM 등에도 SHAP 효율화 알고리즘 적용 연구  
- **실험적 검증**: 다양한 도메인(의료,金融,자연어처리)에서 SHAP 기반 일반화 성능 및 안정성 평가  

위 기여를 통해, 트리 앙상블 기법을 사용하는 모든 연구자와 실무자가 특성 중요도 해석의 **신뢰성과 일관성**을 확보하고, 모델 일반화 및 응용 단계에서 보다 견고한 의사결정을 할 수 있을 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ca477619-29a1-40b5-8a1b-df1ecf9358d2/1706.06060v6.pdf)
