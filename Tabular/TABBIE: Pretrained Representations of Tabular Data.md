# TABBIE: Pretrained Representations of Tabular Data

**핵심 주장 및 기여**  
TABBIE(Tabular Information Embedding)는 텍스트 없이 순수하게 표(tabular data)만을 대상으로 한 사전학습(self-supervised pretraining) 방법으로, 표의 셀(cell), 행(row), 열(column) 단위 임베딩을 효율적으로 학습한다. 기존 TaBERT·TaPas 등은 표와 텍스트를 연결해 BERT 기반의 마스크드 언어모델링을 사용하지만, TABBIE는  
- **이종 텍스트 분리**: 행·열 각각을 Transformer로 인코딩한 뒤 매 레이어마다 평균화하여 셀 단위로 컨텍스트화  
- **간단한 사전학습 목표**: ELECTRA 스타일의 ‘셀 변조 여부(corrupt cell detection)’ 분류  
- **경량화**: 긴 시퀀스 트렁케이션 없이 12-layer, 768-dim 모델을 8GPU로 일주일 내 훈련  

을 통해  
1. **표 기반 예측 과제**(열 완성·행 완성·열 타입 분류)에서 TaBERT 대비 대체로 우수하거나 동등한 성능  
2. **훈련 효율성 대폭 개선** (8 V100×1주 vs. 128 V100×6일)  
3. **셀·행·열 단위 표현 학습**으로 복합적인 표 의미론 및 수치적 트렌드 이해 가능  

을 보인다.[1]

***

## 1. 해결 문제  
실제 활용되는 주요 작업들은 표만을 입력으로 하는 경우가 많다.  
- 누락된 셀 값 채우기 (missing cell imputation)  
- 유사한 열·행 검색 및 군집화  
- 표 구조 분해(Table structure decomposition) 후 변조된 셀 검출 등  

그러나 기존 사전학습 모델들은 표와 텍스트(캡션·질의문)를 함께 입력시켜야 하며, 이로 인한 시퀀스 길이 및 연산 복잡도로 인해  
- 표 단독 과제에서 저조한 성능  
- 긴 시퀀스 트렁케이션으로 정보 손실  

문제가 있었다.[1]

***

## 2. 제안 방법  
### 2.1 모델 구조  
- **초기화**: 각 셀 콘텐츠를 BERT-base-uncased CLS 토큰(768-d)으로 인코딩 후, **행·열 위치 임베딩** 추가  
- **양분화된 Transformer**  
  - **Row Transformer**: 같은 행의 셀 간 상호작용  
  - **Column Transformer**: 같은 열의 셀 간 상호작용  
- **레이어별 평균화**: 행·열 Transformer 출력 $$\mathbf{r}\_{i,j}^{(L)},\mathbf{c}_{i,j}^{(L)}$$를  

$$
    \mathbf{x}_{i,j}^{(L+1)} = \tfrac{1}{2}\bigl(\mathbf{r}_{i,j}^{(L)} + \mathbf{c}_{i,j}^{(L)}\bigr)
  $$  
  
  으로 합쳐 다음 레이어 입력으로 활용.[1]

- **CLS 토큰**: 각 행·열 앞에 별도 CLSROW·CLSCOL 토큰을 삽입하여 전체 행·열 표현 추출 가능.[1]

### 2.2 사전학습 목표  
- **Corrupt Cell Detection**: ELECTRA 방식의 이진 분류  

$$
    P(\text{corrupt}\mid i,j)=\sigma\bigl(\mathbf{w}^\top \mathbf{x}_{i,j}^{(L)}\bigr)
  $$  
  
  각 셀이 원본인지 변조된(샘플링·스왑) 것인지 예측하여 교차 엔트로피 손실 계산.[1]

- **셀 변조 전략**  
  1. **빈도 기반 샘플링**: 전체 코퍼스의 셀 분포에서 무작위 추출  
  2. **테이블 내 스왑**: 같은 행 혹은 열 내 셀 교환  
- 다양한 변조를 통해 텍스트·수치 패턴, 테이블 구조를 심층 학습.[1]

***

## 3. 성능 향상  
### 3.1 열 완성(Column Population)  
seed 열이 1개일 때도 TABBIE는 MAP·MRR 기준 TaBERT 대비 4.8–7.8pt 우수.

### 3.2 행 완성(Row Population)  
2개 이상의 seed 행에서는 TABBIE가 TaBERT 대비 일관된 성능 향상, 1개 행만 주어질 때는 TaBERT 우위.

### 3.3 열 타입 예측(Column Type Prediction)  
1,000개·전체 학습 데이터 양 모두에서 TABBIE·TaBERT이 기존 SATO·Sherlock 대비 약 6–10pt 높은 F1.

***

## 4. 한계 및 일반화 성능  
- **MIX 변조 전략**이 downstream 과제에서는 FREQ 대비 다소 낮은 성능을 보이나, 복잡한 변조(detached intra-column swap) 검출에 유리.  
- **수치적 트렌드 이해**: 증·감 순서 위반 탐지에서 MIX 모델만 일관성 있게 식별 가능.  
- **군집화 활용**: FinTabNet 재무표를 k-means로 클러스터링 시, 동일 회사·유사 유형 표가 같은 군집에 배치되어 일반화 잠재력 확인.  
- **제약**: 표 크기(최대 30×20) 및 셀 문자열 길이(300자) 트렁케이션 적용, 비(非)영어 데이터 비중 낮음.[1]

***

## 5. 일반화 성능 향상 관점  
TABBIE의 구조·목표는 다음 연구에 유용할 수 있다.  
- **다양한 언어·도메인 확장**: 비영어·의료·금융 도메인 표로 확대  
- **텍스트 병합 방식 개선**: 표+추가 텍스트 동시 활용을 위한 융합 모듈 추가  
- **더 정교한 변조 전략**: 셀 경계·합병 오류, 구조적 변조를 반영한 학습 목표  
- **경량화 활용**: 모바일·엣지 환경에서 표 임베딩 제공

***

## 6. 향후 연구 고려 사항  
- **비정형 표 대응**: 중첩·혼합 셀, 병합된 셀 처리 능력 강화  
- **다중 모달 결합**: 표 이미지 → 구조 추출 → 텍스트 임베딩 통합  
- **대규모 저자원 학습**: 소량 데이터 환경에서의 전이학습·어댑터 기법 적용  
- **공정성·프라이버시**: 특정 열·행에 대한 편향 완화 및 민감 정보 검출·가림

TABBIE는 순수 표 데이터만으로도 강력한 사전학습 효과를 입증했으며, 표 기반 AI 응용을 위한 핵심 플랫폼으로 자리매김할 수 있을 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/62a78fe3-18bd-4fcc-8d1c-0ac0956b631b/2105.02584v1.pdf)
