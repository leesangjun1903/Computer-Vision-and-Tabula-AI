# Robust Decision Trees Against Adversarial Examples

**핵심 주장 및 주요 기여**  
이 논문은 **결정 트리 기반 모델도 신경망과 마찬가지로 적대적 예제(adversarial examples)에 취약**함을 보이고, 입력 특성의 최악의(최소 성능) 섭동(worst-case perturbation) 하에서 분할(split)을 최적화하는 **Max–Min 내성 강화(max–min robust optimization) 프레임워크**를 제안한다. 제안 기법은 정보 이득(information gain) 기반 단일 결정 트리와 XGBoost 등 대규모 부스팅 트리에 모두 적용 가능하며, **적은 연산 오버헤드로 모델의 적대적 강인성(robustness)을 획기적으로 향상**시킨다.[1]

***

## 1. 해결하고자 하는 문제  
- 기존 연구는 주로 선형 모델과 신경망의 적대적 강인성에 집중되었으나, **트리 기반 모델(tree-based models)의 적대적 취약성은 거의 연구되지 않음**.[1]
- 트리 모델도 비차분화(discrete), 비연속적(non-smooth) 특성으로 인해 공격·방어 기법이 다르며, 기존 방어는 미분가능한 모델에 국한됨.

## 2. 제안 방법  
### 2.1. Max–Min Saddle Point Formulation  
각 노드 분할(feature j, 임계값 τ)에 대해, 훈련 샘플 집합 I가 ‖·‖∞ ≤ ε 범위 내에서 최악의 섭동을 받았을 때의 스코어(score)를 최소화(min)하고, 그 최소 성능을 최대로(max) 높이는 **max–min** 문제로 정의:  

$$
\max_{j,\,\tau}\;\min_{\{x_i'\}_{i\in I}:\,\|x_i' - x_i\|_\infty \le \epsilon}\;S(j,\tau;\{x_i'\})
$$  

여기서 $$S$$는 정보 이득 또는 부스팅 스코어이다.[1]

### 2.2. 정보 이득 기반 단일 결정 트리  
- 정보 이득(score):  

$$
  \mathrm{IG} = H(Y) - \frac{|I_L|}{|I|}H(Y|x_j \le \tau) - \frac{|I_R|}{|I|}H(Y|x_j > \tau)
  $$  

- 내성 점수(robust score)는 노드 내 모호 영역(ambiguous set)에 속하는 샘플들이 양쪽 자식 노드로 옮겨질 때 정보 이득이 최소가 되도록 조정.[1]
- 모호 영역의 샘플 수 $$n_0,n_1$$을 정수계획 문제로 풀면 NP-hard하므로, **근사 알고리즘**(알고리즘 1, 3)을 사용해 $$O(d\,|\!I\!|^2)$$ 시간에 구현.

### 2.3. Gradient Boosted Decision Tree (GBDT)  
- XGBoost 등에서 사용되는 2차 테일러 전개 기반 스코어:  

$$
  S = \frac{1}{2}\Bigl[\sum_{i\in I_L}\tfrac{g_i^2}{h_i+\lambda} + \sum_{i\in I_R}\tfrac{g_i^2}{h_i+\lambda} - \sum_{i\in I}\tfrac{g_i^2}{h_i+\lambda}\Bigr] - \gamma,
  $$  
  
  여기서 $$g_i,h_i$$는 1·2차 도함수, $$\lambda,\gamma$$는 정규화 상수.[1]
- 모호 영역의 모든 샘플을 네 가지 케이스(무섭동·전부 왼쪽·전부 오른쪽·양측 교환)로 근사하여 최소 스코어를 구하고, 이 값을 분할 스코어로 사용(알고리즘 2).

## 3. 모델 구조  
- **단일 결정 트리**: 깊이(depth) ≤ 8, ε = 0.2…0.3  
- **부스팅 모델**: XGBoost 기반, 트리 수(K) = 20…300, 깊이 = 8, shrinkage = 0.05…0.3  
- 정보 이득 트리는 ID3/CART 방식, GBDT는 MSE·로지스틱·순위손실 등 일반 손실 함수 지원

## 4. 성능 향상  
- **단일 트리**: 자연 트리에 비해 적대적 왜곡(distortion) 크기 2–3배 증가.  
- **GBDT**: MNIST, Fashion-MNIST 등에서 왜곡이 3–5배 증가, 정확도 손실 ≤ 1%로 유지.  
- **랜덤 포레스트**에서도 평균 왜곡 2배 이상 증가  
- 일부 소형 데이터셋에서 테스트 정확도도 소폭 개선(암묵적 정규화 효과)

## 5. 한계  
- ε 범위 내 섭동만 고려하므로, **보다 복잡한 공격(multi-modal perturbations)** 에 대한 보장은 미흡  
- 네 가지 케이스 근사로 인해 최적 내성 점수와 차이가 존재  
- 연산 복잡도는 자연 트리 대비 증가하나, 부스팅에서는 각 분할별 O(1) 근사로 실시간 학습 가능

## 6. 일반화 성능 향상 관점  
- 분할 시 **모호 영역 축소**에 집중하여 분할 경계가 **과적합**을 억제하고 **결정 경계 단순화**  
- ε > 0에서 학습된 모델은 **노이즈에 강건**할 뿐 아니라, 테스트 데이터의 랜덤 노이즈 및 도메인 이동에도 더 안정적  
- 결정 경계가 매끄러워짐으로써 작은 데이터셋에서도 과도한 분할을 방지, 암묵적 정규화로 작용[1]

## 7. 향후 연구에 대한 영향 및 고려점  
- **확률적 섭동 모델**: ε-ball 외에도 가우시안 노이즈, 대역폭 제한된 섭동 등 다양한 불확실성 모델로 일반성 확장  
- **연속 최적화 기반 분할**: 정수 변수 근사를 넘어, 분할 기준을 연속화해 내성 점수 최적화  
- **조합 공격에 대한 내성**: 특성 삭제, 순열(permutation) 공격 등 다양한 위협 벡터 결합  
- **해석 가능성 강화**: 내성 강화에 따른 결정 경계 변화 시각화 및 규칙 추출  
- **다중 출력·회귀 문제**로의 확장 및 **비이를산 분류**(multiclass imbalance) 상황에서의 적용  

위 방향은 트리 기반 모델의 **적대적 강인성**과 **일반화 성능**을 동시에 개선하는 후속 연구에 핵심 고려사항이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/563b15d4-25ce-4227-958c-d324909b608e/1902.10660v2.pdf)
