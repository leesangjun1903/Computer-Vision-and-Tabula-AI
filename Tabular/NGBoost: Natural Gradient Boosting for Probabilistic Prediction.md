# NGBoost: Natural Gradient Boosting for Probabilistic Prediction

**핵심 주장 및 기여**  
NGBoost는 기존 회귀 모델의 점 추정(point estimate)을 넘어서 입력 특성에 따라 조건부 확률 분포 $$P(y\mid x)$$를 예측하는 범용적 부스팅 알고리즘이다. 분포 파라미터를 다중 파라미터 부스팅으로 추정하고, 학습 역학을 안정화하기 위해 자연(내추럴) 그래디언트를 도입함으로써, 유연성·확장성·사용 편의성을 확보하면서도 기존 확률적 예측 기법과 동등하거나 우수한 성능을 보인다.[1]

## 1. 해결하고자 하는 문제  
전통적 회귀 모델은 입력 $$x$$에 대해 예측값 $$\hat{y}=E[y\mid x]$$만 반환하나, 실제 응용 분야(의료 예측, 기상 예보 등)에서는 예측 불확실성 추정이 필수적이다. 즉, 임의 구간에 대한 확률 질의를 지원하려면 조건부 분포 $$P(y\mid x)$$를 추정해야 한다. 기존 방법은  
- 동분산성 가정 기반(동일 분산)  
- GAMLSS와 같이 사전 지정된 모델 형태  
- 베이지안 MCMC 등 계산 복잡도가 높음  
등으로 한계가 있었다.[1]

## 2. 제안 방법  
NGBoost는 다음 요소를 모듈화하여 구성된다:  
- **베이스 학습기** $$f$$ (예: 회귀 트리)  
- **분포 패밀리** $$P_{\theta}$$ (예: 정규분포 $$N(\mu,\sigma)$$)  
- **적절 스코어링 규칙** $$S$$ (예: 로그우도, CRPS)  

### 2.1 스코어링 규칙과 발산  
점 추정 손실 대비, 확률 분포를 평가하는 데는 적절(Proper) 스코어링 규칙 $$S(P,y)$$을 사용하여,  

$$
E_{y\sim Q}[S(Q,y)] \le E_{y\sim Q}[S(P,y)]
$$

를 만족하는 발산 $$D_S(Q\parallel P)$$를 유도한다.[1]

### 2.2 자연 그래디언트  
일반 그래디언트 $$\nabla S(\theta,y)$$는 매개변수화에 민감하므로, 스코어링 규칙에 대응하는 발산 공간에서 최적의 학습 속도와 안정성을 위해 자연 그래디언트  

$$
\tilde\nabla S(\theta,y) = I_S(\theta)^{-1}\,\nabla S(\theta,y)
$$

를 사용한다. 여기서 $$I_S(\theta)$$는 발산에 따른 리만 계량(Riemannian metric)이다.[1]

### 2.3 다중 파라미터 부스팅  
각 입력 $$x$$에 대해 분포 파라미터 $$\theta\in\mathbb R^p$$를 예측하도록 $$p$$개의 베이스 학습기를 단계별로 학습한다. 각 단계 $$m$$에서 모든 샘플의 자연 그래디언트 $$g_i^{(m)}$$를 추정하고, 이를 목표값으로 트리를 학습한 후 단계별 스케일링 계수 $$\rho^{(m)}$$와 학습률 $$\eta$$를 적용하여  

$$
\theta(x)=\theta^{(0)} - \eta\sum_{m=1}^{M}\rho^{(m)}f^{(m)}(x)
$$

와 같이 갱신한다.[1]

## 3. 모델 구조  
NGBoost는 알고리즘 1과 같이 초기 분포 파라미터 $$\theta^{(0)}$$ 설정, 반복적인 자연 그래디언트 계산·부스팅·라인서치로 구성된다. 베이스 학습기로 결정 트리를 사용하면, 단계별로 평균과 로그분산(log σ)을 예측하는 두 개의 트리가 함께 학습된다.[1]

## 4. 성능 향상 및 한계  
### 4.1 성능 평가  
UCI 벤치마크 9개 데이터셋에서 평균 음의 로그우도(NLL) 기준으로 MC Dropout·Deep Ensembles·Concrete Dropout 등 기존 방법과 동등하거나 우수한 성능을 기록하였다. 특히 샘플 수가 적은 데이터셋에서 확률적 예측 품질을 크게 개선함이 관찰되었다.[1]

### 4.2 한계  
- 분포 파라미터 수 $$p$$만큼 베이스 학습기 및 다수의 작은 행렬 역행렬 계산이 필요해, $$O(p^3N)$$ 연산이 발생할 수 있으나, 대부분의 분포는 $$p\le2$$라 실질적 부담은 크지 않다.[1]
- 유한 스텝 크기에서는 자연 그래디언트의 불변성(invariance)이 일부 손실될 수 있다.  

## 5. 일반화 성능 향상 관련 고찰  
자연 그래디언트는 파라미터 공간이 아닌 분포 공간의 곡률(curvature)에 최적화된 방향으로 학습을 유도하여, 다양한 조건부 평균·분산 환경에서도 균형 잡힌 업데이트를 가능케 한다. 이는 “행운적인” 샘플에 과적합(overfitting)되는 것을 방지하고, 분포 매개변수들이 균등하게 수렴하도록 하여 모델의 **일반화 성능**을 향상시킨다.[1]

## 6. 향후 연구 영향 및 고려사항  
NGBoost는 분류·생존 예측·다변량 예측 등에도 즉시 확장 가능하며,  
- 유한 스텝에서의 불변성 보완(고차 미분방정식 활용)  
- 더 강력한 트리 학습기 및 정규화 기법 도입  
- 이론적 일관성(샘플 수 증가 시 파라미터 수렴 조건) 및 모델 오염(misspecification) 시 일관성 연구  
등이 미래 연구 과제로 제시된다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/13558929-d55e-4dab-98fc-8d8dca8a36bd/1910.03225v4.pdf)
