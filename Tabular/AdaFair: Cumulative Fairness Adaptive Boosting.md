# AdaFair: Cumulative Fairness Adaptive Boosting

**핵심 주장 및 주요 기여 요약**  
AdaFair는 **AdaBoost** 기반의 페어니스(fairness) 인식 분류기로, 각 부스팅 라운드에서 **누적 공정성(cumulative fairness)** 개념을 도입하여 인스턴스 가중치를 동적으로 조정하며, 동시에 **클래스 불균형** 문제를 명시적으로 해결한다. 주요 기여는 다음과 같다:[1]
- **누적 공정성 정의**: 현재까지 생성된 앙상블 모델 전체의 **Equalized Odds** 지표를 평가하여, 차별 그룹에 추가 가중치(페어니스 비용)를 부여하는 스키마 제안.
- **신뢰도 기반 가중치**: 약한 학습기의 개별 예측 신뢰도(마진)를 가중치 업데이트에 통합, 경계에서 멀리 떨어진 어려운 사례를 더욱 강조.
- **불균형 최적화**: 최종 앙상블의 크기를 **Balanced Error Rate (BER)** 기준으로 최적화하여 소수 클래스 성능 저하를 방지.
- **경험적 성능 향상**: 기존 공정성 기법 대비 최대 25%까지 BER 개선 및 TPR·TNR 균형 유지.

***

## 1. 해결하려는 문제  
머신러닝 기반 의사결정 시스템이 성별·인종 등 민감 속성에 따라 예측 성능 차이를 보이며 차별을 유발할 수 있다.  
기존 공정성 알고리즘은 전체 분류 정확도(Overall Accuracy)를 유지하면서 집단 간 예측률 차이를 줄이는 데 집중하나,  
- 다수 클래스 중심의 정확도 지표는 **클래스 불균형** 상황에서 소수 클래스 성능 저하(낮은 TPR)로 이어진다.  
- Equalized Odds(ΔFPR+ΔFNR) 기준으로 차별을 완화해도, 소수 클래스의 예측 성능을 희생할 수 있음.[1]

***

## 2. 제안하는 방법  
### 2.1 문제 정의  
- 데이터: $$D = \{(x_i,y_i)\}$$, 이진 분류 $$(y\in\{+,-\})$$, 단일 이진 민감 속성 $$S\in\{s,\bar s\}$$.  
- 공정성 지표:  

$$
    \delta_{\mathrm{FPR}} = P(\hat y\neq y\mid \bar s,-) - P(\hat y\neq y\mid s,-),\quad
    \delta_{\mathrm{FNR}} = P(\hat y\neq y\mid \bar s,+) - P(\hat y\neq y\mid s,+)
  $$

$$
    \mathrm{Eq.Odds} = |\delta_{\mathrm{FPR}}| + |\delta_{\mathrm{FNR}}|
  $$

- 성능 지표: **Balanced Error Rate**  

$$
    \mathrm{BER} = 1 - \tfrac12(\mathrm{TPR} + \mathrm{TNR})
  $$

### 2.2 알고리즘 구조  
1. 초기화: 모든 인스턴스 가중치 $$w_i=1/N$$, 페어니스 비용 $$u_i=0$$.  
2. 부스팅 라운드 $$j=1\ldots T$$:  
   a) 약한 학습기 $$h_j$$ 학습(가중치 $$w$$)  
   b) 분류 오류 $$\mathrm{err}_j$$, 부스팅 계수 $$\alpha_j = \frac12\ln\bigl((1-\mathrm{err}_j)/\mathrm{err}_j\bigr)$$ 계산  
   c) 누적 공정성  

$$
       \delta_{\mathrm{FNR}}^{1:j},\;\delta_{\mathrm{FPR}}^{1:j}
       \quad\text{(Equation 5)}
     $$
   
   d) 페어니스 비용 $$u_i$$ 할당:  

$$
       u_i = 
       \begin{cases}
         |\delta|,& \text{해당 그룹·클래스에서 차별 발생 시}\\
         0,&\text{기타}
       \end{cases}
       \quad\text{(Equation 6)}
     $$
   
   e) 가중치 업데이트:  

$$
       w_i \leftarrow \frac{1}{Z_j} \, w_i \,\exp\bigl(\alpha_j\hat h_j(x_i)\,I[y_i\neq h_j(x_i)]\bigr)\,(1+u_i)
     $$
     
  여기서 $$\hat h_j(x)$$는 예측 신뢰도(마진)  
3. 최적 앙상블 크기 $$\theta$$ 선택:  

$$
     \arg\min_{\theta\le T}\bigl(c\cdot\mathrm{BER}_\theta + (1-c)\cdot\mathrm{ER}_\theta + \mathrm{Eq.Odds}_\theta\bigr)
     \quad\text{(c=1 추천)}.
   $$  
   
   Balanced Error 최적화 시 TPR 향상, TNR 소폭 저하로 소수 클래스 성능 보존.[1]

***

## 3. 성능 향상 및 한계  
### 성능 향상  
- **클래스 불균형 강건성**: BER 최적화 및 불균형 보정으로 소수 클래스 TPR↗, 전체 BER↓(최대 25%).[1]
- **공정성 유지**: 누적 Eq.Odds≈0에 도달하며, TPR·TNR 그룹 간 차이 최소화.  
- **신뢰도 반영**: 마진 기반 가중치로 어려운 사례 집중, 예측 마진 분포 전체적 우위 관찰.[1]

### 한계  
- **이론적 수렴 분석 부재**: 실험적 우수성에도 불구하고 수렴 보장에 대한 이론적 근거 미비.  
- **하이퍼파라미터 T 조정**: 라운드 수 $$T$$와 비용 허용치 $$\epsilon$$ 설정에 민감.  
- **이진·단일 속성 한계**: 다중 민감 속성, 다중 클래스 확장 필요.

***

## 4. 일반화 성능 향상 관점  
AdaFair는 **누적 공정성**과 **BER 기반 최적화** 덕분에, **학습 과정**에서 지속적으로 그룹 간 성능 격차를 보정하며, **모델의 마진 분포**를 통해 소수 클래스 경계 사례에 대한 강인성을 확보한다. 이러한 특성은 새로운 분포나 도메인으로의 전이 시에도 **일관된 공정성-성능 균형**을 제공할 가능성을 높인다.

***

## 5. 향후 연구 영향 및 고려 사항  
- **영향**: AdaFair는 페어니스-불균형 동시 고려를 제안함으로써, 실제 사회적 민감 도메인에서 **공정성 제약**과 **클래스 불균형** 문제를 동시에 해결하는 연구 방향을 제시한다.  
- **고려 사항**:  
  - **온라인·실시간 학습**에서 θ 최적화 및 비용 적응 메커니즘 설계  
  - **다중 속성·다중 클래스** 환경 확장  
  - **수렴 이론** 및 **일반화 보증** 연구  
  - **딥러닝 앙상블**으로의 응용: 대규모 표현 학습 모델에 누적 공정성 개념 통합  

이로써 AdaFair는 공정성 연구에 있어 **누적적·불균형 대응** 관점을 확립하며, 후속 연구에서 **이론적 고찰**과 **다양한 확대 적용**을 위한 기반을 마련한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/bb1a3338-ffbd-46f0-96bf-d5584b815c11/1909.08982v1.pdf)
