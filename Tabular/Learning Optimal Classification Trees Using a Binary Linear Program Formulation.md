# Learning Optimal Classification Trees Using a Binary Linear Program Formulation

# 핵심 주장 및 주요 기여 요약

**학습 최적 분류 트리를 이진 선형 계획(Binary Linear Program, BLP)으로 모델링**  
“BinOCT”라 명명된 이 논문은, 깊이가 고정된 최적 분류 트리를 학습 문제로 정의하고, 이를 기존 ILP(Integer Linear Program) 대비  
  -  변수 수를 데이터 행 수와 독립적으로, 고유 특성값 수의 로그에 비례하도록 줄이고  
  -  제약식 수도 크게 감소시키는  
새로운 이진 선형 계획(Binary LP) 포뮬레이션을 제안한다.[1]

실험 결과, 대소 규모 데이터셋 모두에서 이전 MO 기반 기법(OCT, DTIPs)보다  
  -  학습 및 추론 시간 단축  
  -  동일한 또는 더 높은 정확도  
를 달성함을 보였다.[1]

# 문제 정의 및 해결 방법 상세 설명

## 1. 해결하고자 하는 문제  
깊이 $$K$$로 제한된 결정 트리에서, 전체 학습 데이터에 대한 **분류 오류 합을 최소화**하는 최적 트리를 찾는 문제는 NP-hard이다.[1]
기존 CART, ID3 등의 탐욕적 휴리스틱은 전역 최적값을 보장하지 않으며, 기존 ILP 포뮬레이션(OCT, DTIPs)은  
  -  모든 데이터 행마다 변수와 제약식을 생성하여  
  -  데이터 행 수에 따라 모델 크기 및 해 시간 급증  
하는 한계를 가졌다.[1]

## 2. 제안하는 이진 LP 포뮬레이션 (BinOCT)  
### 2.1 변수 정의  
- $$f_{n,f}\in\{0,1\}$$: 내부 노드 $$n$$에서 특징 $$f$$를 선택 여부  
- $$t_{n,t}\in\{0,1\}$$: 노드 $$n$$의 이진 인코딩 단계 $$t$$ 결정 변수  
- $$p_{l,c}\in\{0,1\}$$: 잎 노드 $$l$$가 클래스 $$c$$를 예측 여부  
- $$l_{r,l}\in$$: 행 $$r$$이 잎 $$l$$에 도달하면 1, 아니면 0 (연속 변수)[1]
- $$e_{l,c}\ge0$$: 잎 $$l$$에서 클래스 $$c$$ 예측 오류 합  

### 2.2 트리 구조 인코딩  
#### (1) 이진 탐색 방식 임계값 결정  
특성 $$f$$의 가능한 임계값 수 $$T_f$$에 대해 $$\lceil\log_2 T_f\rceil$$개의 이진 변수 $$t_{n,1},…,t_{n,\lceil\log_2T_f\rceil}$$로  
임계값 선택을 이진 탐색 트리 구조처럼 인코딩하여, 변수 수를 $$\mathcal O(\log T_f)$$로 줄인다.[1]

#### (2) 빅-엠(Big-M) 제약식 결합  
각 임계값 분할 제약을 모든 해당 행에 별도 적용하는 대신, 해당 행 개수 $$M$$를 활용한 빅-엠 제약식으로  

$$
    \sum_{r\in\text{lower}(b)}\sum_{i\inLL(n)}l_{r,i}+M\,t_{n,t}\le M,
  $$

$$
    \sum_{r\in\text{upper}(b)}\sum_{i\inRL(n)}l_{r,i}-M\,t_{n,t}\le0
  $$

처럼 각 분할 범위 $$b$$마다 하나의 제약식으로 묶어 작성함으로써 제약식 수도 크게 줄였다.[1]

#### (3) 특징 선택  
각 내부 노드마다 정확히 하나의 특징을 쓰도록

$$\sum_{f=1}^F f_{n,f}=1$$ 제약을 추가하고, 빅-엠 곱셈으로 위 분할 제약이  

해당 $$f$$ 선택 시에만 활성화되게 설계했다.[1]

### 2.3 목적 함수  
전체 오류 합 $$\min\sum_{l,c} e_{l,c}$$를 최소화하며,  
잎 $$l$$의 클래스 $$c$$ 예측 여부 $$p_{l,c}$$와 행-잎 도달 변수 $$l_{r,l}$$을 결합한 빅-엠 제약으로 오류를 정의한다.[1]

## 3. 모형 복잡도  
- **이진 변수 수** $$\le2^K(F + C + \log T_{\max})$$ → 데이터 행 수 독립  
- **제약식 수** $$O(R + 2^K(F\cdot T + C))$$ → 기존 ILP 대비 크게 감소[1]

## 4. 성능 향상 및 한계  
- **실험 결과**: UCI 벤치마크 16개 데이터셋(행 124–4601, 특성 4–57)에서  
  -  학습 정확도 및 해 시간에서 OCT, DTIPs 상회  
  -  테스트 정확도에서도 깊이 2–3의 경우 CART·OCT 대비 우수  
  -  깊이 4에서는 순수 학습 오류 최적화가 과적합을 유발해 일반화 성능이 다소 저하됨[1]
- **한계**:  
  -  목적이 순수 학습 오류 최소화이므로 과적합 위험  
  -  제약식 수는 여전히 깊이·특성·임계값 수에 따라 늘어날 수 있음

# 일반화 성능 향상 가능성

BinOCT의 **이진 탐색 기반 임계값 인코딩**은 노드별 불필요한 변수·제약을 억제하므로,  
트리 깊이가 깊어져도 학습 오버헤드는 완만하게 증가한다. 이는 다음과 같은 일반화 향상 여지를 제공한다:

-  **정규화 및 복잡도 페널티** 추가: 객관식에 학습 오류와 함께 $$\alpha\cdot\text{노드수}$$ 또는 $$\beta\cdot\sum|\mathbf{t}|$$ 항을 도입하여 과적합 방지 가능  
-  **제약식 샘플링**: 모든 행·임계값에 대해 제약을 생성하지 않고, 고빈도 행 또는 중요한 임계값만 선택해 근사적 최적화  
-  **앙상블 적용**: BinOCT를 배깅/부스팅 프레임워크 내 약분류기(weak learner)로 사용해 일반화 성능 강화

# 향후 연구 영향 및 고려사항

BinOCT는 **MO 기반 의사결정모델** 연구에 새로운 방향을 제시하며, 후속 연구에서 다음을 고려할 수 있다:

-  **다양한 목적 함수**: 공정성, 비용 민감도, 불확실성 제약 등 복합 목표 최적화  
-  **모델 복잡도 제어**: 정보 이득, 지니 불순도 등 휴리스틱 요소와 BLP 결합  
-  **확장성 향상**: 임계값/데이터 샘플링, 열 생성(column generation) 기법을 접목해 초대형 데이터셋 지원  
-  **다중 출력·다중 작업**: 회귀·다중 레이블 분류 트리로 일반화  
-  **하이브리드 기법**: 신경망·그래프모델과 결합한 혼합 의사결정 시스템 개발

이상으로 BinOCT는 최적 분류 트리 학습에 있어 **변수·제약 효율화**를 통해 **속도**와 **정확도**를 동시에 개선한 혁신적 포뮬레이션을 제안하였다. 앞으로의 연구에서는 과적합 제어 및 다양한 목적 반영을 통해 실용적 범용성을 더욱 확대할 수 있을 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/29b98783-34bb-4b60-8f64-203ab6168b08/3978-Article-Text-7037-1-10-20190703.pdf)
