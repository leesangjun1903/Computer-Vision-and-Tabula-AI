# SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption

**핵심 주장**  
SCARF(Self-Supervised Contrastive Learning using Random Feature Corruption)은 Tabular 데이터에 적용 가능한 **간단하면서도 범용적인** 자기지도 대조학습 기법으로, 입력의 무작위 특성 일부를 **경험적 주변분포(empirical marginal)** 에서 샘플링한 값으로 교체하여 두 뷰(view)를 생성하고, InfoNCE 손실로 긍정 쌍의 유사도는 최대화하고 부정 쌍의 유사도는 최소화함으로써 견고한 표현을 학습한다.[1]
이 방식은  
- 완전 지도 학습: 분류 정확도 향상  
- 반(半)지도 학습 (라벨 일부만 사용): 유의미한 성능 개선  
- 라벨 노이즈 환경: 강건성 증가  
를 동시에 달성하며, 기존의 오토인코더 기반 사전학습 및 다른 노이즈 기법보다 우수함을 입증한다.[1]

# 문제 정의 및 제안 방법

## 해결하고자 하는 문제  
일반적인 탭형 데이터 분야에서는 라벨이 부족하거나 라벨에 노이즈가 존재하는 경우가 흔하나, 컴퓨터 비전이나 NLP처럼 **도메인 특화된** 자기지도 왜곡(augmentations) 기법이 부재하다. SCARF는 탭형 데이터에 **범용적으로 적용 가능한** 뷰 생성 방법을 제시한다.[1]

## 제안 방법  
1. 입력 $$x \in \mathbb{R}^M$$에서 **특정 비율 $$c$$** 의 특성 인덱스 $$I$$를 무작위 샘플링  
2. 해당 인덱스의 값을 그 특성의 **경험적 주변분포 $$\mathcal{X}_j$$** 에서 무작위 샘플 $$v\sim\mathcal{X}_j$$로 교체하여 교란된 뷰 $$\tilde{x}$$ 생성  
3. **인코더 $$f$$**와 **프리트레인 헤드 $$g$$**를 통해 임베딩 $$z = g(f(x))$$, $$\tilde{z} = g(f(\tilde{x}))$$ 획득  
4. **InfoNCE 손실**  

$$
     \mathcal{L}_{\mathrm{cont}}
     = -\frac{1}{N}\sum_{i=1}^N \log \frac{\exp(s_{i,i}/\tau)}{\sum_{k=1}^N \exp(s_{i,k}/\tau)},
   $$

$$
     s_{i,j} = \frac{z^{(i)\top}\tilde{z}^{(j)}}{\|z^{(i)}\|_2\,\|\tilde{z}^{(j)}\|_2},
   $$
   
   를 최소화하여 긍정 쌍(i,i)의 유사도를 높이고 부정 쌍(i,k, k≠i)을 낮춤.[1]

5. 사전학습 완료 후 프리트레인 헤드 $$g$$를 제거하고, **분류 헤드 $$h$$**를 추가하여 $$f$$와 $$h$$를 **교차 엔트로피 손실**로 미세조정(fine-tuning)

## 모델 구조  
- **인코더 $$f$$**: 4-layer ReLU 네트워크, 히든 차원 256  
- **프리트레인 헤드 $$g$$** 및 **분류 헤드 $$h$$**: 각각 2-layer ReLU, 히든 차원 256  
- **하이퍼파라미터**: 배치 크기 128, 부패율 $$c=0.6$$, 온도 매개변수 $$\tau=1$$, Adam 옵티마이저 LR=0.001, 사전학습 최대 1000 epoch, 미세조정 최대 200 epoch, early stopping 적용.[1]

# 성능 향상 및 한계

## 성능 개선  
- **완전 지도 학습**: 69개 OpenML-CC18 탭 데이터셋에서 기준 모델 대비 평균 **1–2%** 상대적 정확도 향상.[1]
- **라벨 노이즈(30%)**: 기존 노이즈 대처 기법 대비 **2–3%** 추가 개선.[1]
- **반지도 학습(라벨 25%)**: Mixup, Self-/Tri-training 대비 **2–4%** 향상.[1]
- **기존 기법과의 조합**: Mixup, Label smoothing, Dropout 등과 결합 시에도 상호 보완적 효과 확인.[1]

## 한계 및 민감도  
- 배치 크기(>128), 온도($$\tau$$) 및 부패율($$c\in[0.5,0.8]$$)에 비교적 **강건**하나, **특성 스케일링**에는 민감할 수 있으며, 주변분포 샘플링 방식은 z-score, min-max, mean 스케일링에 대해 **불변성**을 보인다.[1]
- **대안 손실 함수**(Barlow Twins, Alignment-Uniform 등)는 InfoNCE만큼 효과적이지 않았다.[1]
- 너무 높은 부패율이나 부정확한 뷰 생성 시 표현 학습 난이도가 증가하여 성능 저하 우려.

# 일반화 성능 향상 관점

SCARF의 **주요 이점**은 **탭형 데이터 전반**에 걸친 표현 학습의 **범용성**과 **강건성**이다. 무작위 특성 교란 기법은 데이터셋별로 조정이 불필요하며, **다양한 도메인**에서 동일한 메커니즘으로 **일관된** 일반화 성능 향상을 보인다. 특히  
- **라벨 노이즈**나 **반지도 상황**에서도 표현의 **일관성**과 **판별력**이 유지되어—모델이 적은 라벨만으로도 신뢰도 높은 예측 수행  
- **하이퍼파라미터 튜닝** 부담이 적어 실무 적용 시 유연성 제공.[1]

# 향후 연구 영향 및 고려 사항

SCARF는 탭형 데이터 자기지도 학습의 **표준**으로 자리잡을 잠재력이 있다.  
- **응용**: 금융, 의료, 산업 IoT 등 라벨 수집이 어려운 분야에서 사전학습 기반 **제로(0) 또는 소수 샷** 분류 적용 가능  
- **확장**: 특성 간 상관관계를 보존하는 교란, 구조화된 임베딩, 도메인 적응(adaptation)과의 통합 연구  
- **윤리·편향**: 입력 데이터의 **편향**이 표현으로 강화될 위험이 있으므로, **공정성(fairness)** 보장을 위한 교란 방안·정규화 필요  
- **기타 손실**: 자기지도 학습 손실의 **다변화** 및 **메타러닝** 접근으로 최적 교란 방식 자동 학습 탐색.

위와 같은 고려를 통해 SCARF 기반 자기지도 학습은 탭형 데이터 영역에서 **강력한 일반화**와 **실용성**을 동시에 확보하는 방향으로 발전할 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/c8dc7a45-9887-4b2c-8671-fffbefd06b81/2106.15147v2.pdf)
