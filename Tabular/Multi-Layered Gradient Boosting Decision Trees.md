# Multi-Layered Gradient Boosting Decision Trees

## 핵심 주장 및 주요 기여  
이 논문은 기존의 비미분 가능 알고리즘인 **Gradient Boosting Decision Trees(GBDT)**를 다층 구조로 확장하여, 비미분 시스템에서도 계층적 표현 학습이 가능함을 최초로 증명한 연구이다. 이를 위해 GBDT를 여러 층으로 쌓은 모델(mGBDT)을 제안하고, 각 층 간의 정보를 주고받기 위한 **변형된 타깃 프로퍼게이션** 방식을 도입하였다. 이로써 역전파 없이도 계층적·분산 표현 학습을 달성하고, 탭형 데이터에서 신경망에 버금가는 성능을 보임을 실험적으로 입증하였다.[1]

## 해결하고자 하는 문제  
딥러닝의 핵심은 다층 구조를 통한 표현 학습이지만, GBDT 같은 비미분 기법은 역전파를 적용할 수 없어 계층적 표현 학습이 불가능했다.  
- **문제 정의**: 비미분 모듈만으로 다층 네트워크를 구성하고, 역전파 없이 각 층의 출력을 분산 표현으로 학습할 수 있는가?[1]

## 제안 방법  
1. **모델 구조**  
   - 입력층 $$o_0$$과 $$M-1$$개의 은닉층, 출력층 $$o_M$$으로 구성.  
   - 각 은닉층 $$F_i: \mathbb{R}^{d_{i-1}}\to\mathbb{R}^{d_i}$$은 회귀 GBDT로 수행.

2. **타깃 프로퍼게이션 변형**  
   - 각 순방향 매핑 $$F_i$$에 대응하는 **의사 역함수** $$G_i$$를 학습해, 상위층의 가짜 라벨 $$z_i$$를 하위층으로 전파.  
   - 역함수 학습:  

$$
       G_i^{(t)} = \arg\min_{G_i} \mathbb{E}_{x}\big[L_{\text{inv}}(G_i(F_i^{(t-1)}(o_{i-1})),o_{i-1})\big]
     $$
     
  여기서 $$L_{\text{inv}}$$는 잡음 주입 기반 재구성 오차이다.[1]
   
   - 순방향 매핑 업데이트:

$$
       F_i^{(t)} \leftarrow F_i^{(t-1)} + \text{GBDT}\big(o_{i-1},\, z_i^{(t)} - F_i^{(t-1)}(o_{i-1})\big)
     $$
   
   - 최상위층 은닉층의 pseudo-label은 실제 정답 $$y$$로부터 정의한다.

3. **학습 절차**  
   - 초기화: 각 은닉층 출력에 작은 GBDT를 적용해 무작위 초기 표현 생성.  
   - 반복적으로 역함수 $$G_i$$ 학습 → pseudo-label 계산 → 순방향 매핑 $$F_i$$ 업데이트를 수행.[1]

## 성능 향상 및 한계  
- **성능 비교**: 소득 예측, 단백질 분류 등 탭형 데이터 실험에서  
  - mGBDT는 동일 구조의 DNN(BackProp, TargetProp)보다 높은 정확도를 달성.  
  - 단일/스태킹 GBDT 대비 우수하며, 수렴 속도도 빠름.[1]
- **일반화 성능**:  
  - 층 수가 증가해도 mGBDT는 안정적으로 표현 품질이 향상되는 반면, DNN(TargetProp)은 과적합으로 성능 저하 발생.[1]
- **한계**:  
  - 복잡한 구조(X, CNN, RNN) 적용 시 계산 비용이 높을 수 있음.  
  - 역함수 학습의 안정성과 스케일링에 대한 추가 연구 필요.

## 일반화 성능 향상 가능성  
mGBDT는 노이즈 주입 기반 재구성 손실과 pseudo-label 전파를 통해 각 층이 더욱 **구조적이고 분산적인 표현**을 학습하도록 유도한다. 특히  
- **잡음 주입**: 역함수 학습 시 주변 영역에서도 올바른 매핑을 학습하게 해 일반화 성능 강화.  
- **pseudo-label 전파**: 상위층의 타깃 정보를 하위층으로 전파해 계층 간 연대감 형성으로 과적합 완화.  
실험 결과, 층 깊이에 따른 표현 품질 향상과 과적합 억제를 동시에 달성함을 확인했다.[1]

## 향후 연구 영향 및 고려사항  
향후 연구에서 다음 사항을 고려할 때 mGBDT 프레임워크가 유용할 것이다.  
- **Deep Forest 통합**: mGBDT를 Deep Forest 구조에 임베딩해 모델 복잡도를 자동 조정.  
- **하이브리드 아키텍처**: mGBDT 층을 DNN에 삽입해 이질적 데이터 처리 및 보안(비미분 구조의 공격 내성) 강화.  
- **순환/합성곱 구조**: RNN/CNN과 유사한 구조적 편향을 가진 mGBDT 계층 설계로 시계열·영상 탭형 데이터 확장.  
- **역함수 학습 최적화**: 역함수의 안정성·효율성 향상을 위한 정규화 및 근사 기법 연구.

이 논문은 **비미분 기반으로도 심층 표현 학습이 가능**함을 제시함으로써, 신경망 중심의 패러다임을 확장하고 다양한 탭형 데이터 응용에서 새로운 연구 방향을 제시한다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/20b6a651-7233-488a-8081-935a101064d2/1806.00007v1.pdf)
