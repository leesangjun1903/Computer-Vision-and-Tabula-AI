# Gradient Boosted Decision Trees for High Dimensional Sparse Output

## 1. 핵심 주장과 주요 기여

이 논문은 고차원 희소 출력 공간(High Dimensional Sparse Output)을 다루는 새로운 GBDT 변형 모델인 **GBDT-SPARSE**를 제안합니다. 주요 기여는 다음과 같습니다:[1]

- 기존 GBDT가 고차원 다중라벨 분류에서 메모리 부족과 긴 실행시간 문제를 해결
- **L0 정규화**를 통해 희소성을 강제하여 모델 크기와 예측 시간을 대폭 개선
- 기존 방법 대비 **10배 이상**의 모델 크기 감소와 예측 시간 단축을 달성하면서 유사한 성능 유지

## 2. 해결하고자 하는 문제와 제안 방법

### 문제 정의

기존 GBDT를 고차원 희소 출력에 적용할 때 발생하는 문제들:[1]

1. **밀집 기울기 문제**: 첫 번째 반복 후 기울기 벡터 $$g_i$$가 L차원 밀집 벡터로 변화하여 O(NL)의 시간과 메모리 복잡도 발생
2. **모델 크기**: 각 트리의 잎 노드에서 L차원 밀집 벡터로 인한 O(TML) 모델 크기
3. **예측 시간**: 테스트 시점에서 O(T𝑙̄ + TL)의 예측 시간 필요
4. **불균형 트리**: 희소 입력으로 인한 깊고 불균형한 결정 트리 생성

### 제안 방법: GBDT-SPARSE

#### 희소성 제약 도입

각 잎 노드의 예측 벡터에 L0 제약을 추가하여 k개 이하의 비영 원소만 유지:[1]

$$
\min_{f_m,w_j^m} \sum_{i=1}^N \|g_i - f_m(x_i)\|_2^2 + \lambda \sum_{j=1}^{M_m} \|w_j^m\|_2^2
$$

제약조건: $$\|w_j^m\|_0 \leq k, \forall j$$

#### 폐형식 해

L0 제약이 있는 최적화 문제의 폐형식 해:[1]

$$
(h_l^*)_q = \begin{cases}
p_q^l / (|V_l| + \lambda) & \text{if } \pi(q) \leq k \\
0 & \text{otherwise}
\end{cases}
$$

여기서 $$p_q^l = \sum_{i \in V_l} (g_i)_q$$이고, π는 절댓값 기준 내림차순 정렬 순서입니다.

## 3. 모델 구조

### 핵심 알고리즘 구조

**Algorithm 1: 노드 분할 알고리즘**[1]
- 각 특성에 대해 정렬된 리스트 유지
- 좌우 자식 노드의 상위 k개 특성 값을 우선순위 큐로 관리
- O(D‖G‖₀ log(k)) 시간 복잡도

### 희소 특성 처리

1. **Random Projection**: $$\bar{x}_i = \bar{G}x_i$$ (고정 가우시안 행렬 사용)
2. **PCA**: 특이값 분해를 통한 주성분 분석
3. **LEML**: 지도학습 기반 임베딩 방법[1]

### 빠른 예측 알고리즘

k-희소 벡터 T개를 효율적으로 합산하는 min-heap 기반 병합 알고리즘:
- 시간 복잡도: O(Tk log k)
- Top-B 라벨만 필요한 경우: O(Tk log B)

## 4. 성능 향상 및 한계

### 성능 향상

**Wiki10-31K 데이터셋 기준**:[1]
- **예측 시간**: 1.3초 (FASTXML 10초 대비 87% 감소)
- **모델 크기**: 85.8MB (FASTXML 853.5MB 대비 90% 감소)  
- **정확도**: 84.34% P@1 (FASTXML 82.71% 대비 향상)

**시간 복잡도**:[1]
- **훈련**: O(DTh‖X‖₀ log(k))
- **예측**: O(Tk log k) (준선형)
- **모델 크기**: O(kT2ʰ)

### 한계점

1. **하이퍼파라미터 민감성**: k, λ 등 매개변수 튜닝 필요
2. **희소성 가정 의존**: 출력이 희소하지 않은 경우 효과 제한
3. **특성 투영 오버헤드**: 희소 입력 처리를 위한 전처리 시간 필요
4. **근사해**: L0 정규화로 인한 최적해로부터의 편차 가능성

## 5. 일반화 성능 향상 가능성

### 정규화 효과

L0 제약은 **자연스러운 정규화 역할**을 수행하여 과적합 방지에 기여합니다. 각 잎 노드에서 소수의 라벨만 활성화되어 모델 복잡도가 감소합니다.[1]

### 구조적 희소성 활용

실제 데이터의 희소성 구조를 모델에 명시적으로 반영하여 **귀납적 편향(inductive bias)**을 제공합니다. 이는 특히 극단적 다중라벨 문제에서 일반화 성능 향상에 기여할 수 있습니다.

### 앙상블 효과

GBDT의 본질적인 **부스팅 메커니즘**과 희소성 제약이 결합되어 각 트리가 상호 보완적인 희소 패턴을 학습하게 됩니다.

## 6. 연구에 미치는 영향과 향후 고려사항

### 연구 영향

1. **확장성 해결**: 수백만 라벨을 다루는 대규모 다중라벨 문제의 실용적 해결책 제시
2. **효율성과 성능의 균형**: 모델 압축과 성능 유지의 새로운 패러다임 제시
3. **희소성 정규화**: L0 정규화의 결정 트리 적용 가능성 입증

### 향후 연구 고려사항

**기술적 확장**:
- 동적 희소성 조정 메커니즘 개발
- 다른 손실 함수에 대한 확장 연구
- 깊은 트리 구조에서의 안정성 분석

**응용 영역 확장**:
- 추천 시스템, 자연어처리 등 다양한 도메인 적용
- 온라인 학습 및 스트리밍 데이터 처리로의 확장

**이론적 기초**:
- 희소성 제약하에서의 수렴성 분석
- 일반화 오차 상한 연구
- 최적 희소성 수준 결정 방법론 개발

이 연구는 대규모 다중라벨 학습 분야에서 **실용성과 성능을 동시에 달성**한 중요한 기여로, 향후 희소 구조를 활용한 효율적인 기계학습 알고리즘 개발의 기반이 될 것으로 기대됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/447a3000-803a-4773-b69d-76fc9f4150bf/si17a.pdf)
