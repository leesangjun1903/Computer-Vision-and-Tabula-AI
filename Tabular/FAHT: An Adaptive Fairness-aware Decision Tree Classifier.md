# FAHT: An Adaptive Fairness-aware Decision Tree Classifier

**핵심 주장 및 주요 기여**  
FAHT는 **온라인 스트림 환경**에서 차별(discrimination)을 동적으로 완화하면서도 합리적인 예측 성능을 유지하는 최초의 **공정성 인지(​fairness-aware) 결정 트리**이다.[1]
- **문제 제기**: 기존의 공정성 기법은 정적 데이터에만 적용되며, 스트림 데이터의 변화(concept drift)를 고려하지 않는다.  
- **주요 기여**:  
  1. **공정 정보 이득(fair information gain, FIG)**라는 새로운 분할 기준을 제안하여, 정보 이득(accuracy)과 공정 이득(fairness)을 결합함.[1]
  2. **FAHT(Fairness-Aware Hoeffding Tree)** 프레임워크를 설계·구현하여, 스트림 분류 시 차별 완화와 모델 적응성을 동시에 달성함.[1]
  3. 스트림 실험에서 HT 대비 차별을 크게 줄이면서 예측 정확도 감소를 최소화했음을 입증.[1]

***

## 1. 해결 과제  
- **스트리밍 데이터의 개념 변화**: 데이터 분포가 시간에 따라 변화(​concept drift)하며, 고정된 정적 모델은 적응 불가능.  
- **역사적 데이터의 내재적 편향**: 민감 속성(sensitive attribute)별 분류 결과에 불균형이 있어, **통계적 동등성(statistical parity)**을 위반할 수 있음.[1]

## 2. 제안 방법  
### 2.1 공정성 및 정보 이득 정의  
1. **통계적 차별 점수**  

$$
   \mathrm{Disc}(D) = \frac{|\mathrm{FG}|}{|\mathrm{FG}|+|\mathrm{FR}|} - \frac{|\mathrm{DG}|}{|\mathrm{DG}|+|\mathrm{DR}|}
   $$  
   
   - FG, FR, DG, DR: 민감·클래스 조합 커뮤니티 크기.[1]
2. **공정 이득(FG)**  
  
$$
   \mathrm{FG}(D,A) = |\mathrm{Disc}(D)| - \sum_{v\in\mathrm{dom}(A)}\frac{|D_v|}{|D|}\,|\mathrm{Disc}(D_v)|
   $$  

3. **정보 이득(IG)**: 기존 엔트로피 기반 정보 이득.  
4. **공정 정보 이득(FIG)**  

$$
   \mathrm{FIG}(D,A)=
   \begin{cases}
     \mathrm{IG}(D,A), & \text{if }\mathrm{FG}(D,A)=0,\\
     \mathrm{IG}(D,A)\times\mathrm{FG}(D,A), & \text{otherwise.}
   \end{cases}
   $$  
   
   - 곱셈 연산으로 스케일 차이를 완화하고, 차별 완화 효과가 있는 분할에 우선순위를 부여.[1]

### 2.2 모델 구조 및 알고리즘  
- **기반 모델**: Hoeffding Tree(HT)–단일 패스 스트림 분류기.[1]
- **확장 사항**:  
  1. **분할 기준**을 IG→FIG로 교체하여, 분할 시 예측 성능과 공정성을 동시 최적화.  
  2. **각 노드별 충분 통계량** 확장(클래스별, 민감도별 카운팅)으로 FG 계산 지원.  
  3. **사전 가지치기(pre-pruning)**: FIG 기준으로 ‘분할하지 않음(null attribute)’ 옵션과 비교.  
  4. **메모리 오버헤드**: 기존 O(d·v·c) → O((d+2)·v·c)로, 속성 수(d)≫2일 때 무시 가능 수준.[1]

***

## 3. 성능 향상 및 한계  
- **성능**:  
  - **Adult 데이터**: HT 대비 차별 27.9% 감소, 정확도 2.5% 감소  
  - **Census 데이터**: 차별 53.2% 감소, 정확도 8.2% 감소.[1]
- **일반화 성능**:  
  - **스트림 전 구간(prequential) 평가**에서, 개념 변화에도 차별을 안정적으로 억제하면서 예측 성능 유지.  
  - **슬라이딩 윈도우 앙상블**에서도 FAHT 기반 모델이 지속적으로 낮은 차별을 보임.[1]
- **한계**:  
  1. 정확도와 차별 완화 간 **트레이드오프** 존재: 정확도 일부 포기  
  2. **단일 민감 속성**(binary)만 처리하며, 다중 민감 속성 적용 미검증  
  3. **후처리(post-processing)** 없는 구조: 추가 후처리 기법과의 조합 가능성 탐색 필요  

***

## 4. 일반화 성능 향상 관점  
- **적응적 학습**: 새로 도입된 FIG 기준이 스트림 변화에 반응하여, 중요 속성 선택에서 **민감 속성 독립성**을 유지함으로써 과적합 방지 및 **일반화 능력** 향상.  
- **모델 보수성**: FIG 분할 기준이 더 보수적(split 제한)으로, 나무 깊이가 얕아져 과적합 리스크 저감.[1]
- **구조적 유연성**: 충분 통계량으로 드리프트 감지 또는 윈도우 기반 유지보수 기법과 결합하여, 온·오프라인 모두 일반화 가능.  

***

## 5. 향후 연구 방향 및 고려 사항  
- **앙상블 확장**: 다중 윈도우 모델 및 learner 복합도 조절로 **안정성**과 **공정성** 동시 강화.  
- **다중 민감 속성**: 복합 그룹 간 상호작용 고려, 다차원 분할 기준 개발.  
- **후처리 기법 통합**: 리프 재라벨링·재가중치화 등 추가적 차별 완화 모듈과의 **협업 효과** 분석.  
- **세멘틱 유사성**: 트리 구조 간 의미 유사성을 활용하여, 개념 변화 패턴별 분류기 전환 전략 수립.[1]

**결론**: FAHT는 실시간 스트림 분류 환경에서 **공정성과 예측 성능** 간 균형을 최초로 달성했으며, 향후 앙상블·다중 민감 속성·후처리 연계 연구를 통해 더 폭넓은 적용이 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/29e90688-e3f4-4674-a255-66307c734275/1907.07237v1.pdf)
